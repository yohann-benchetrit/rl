


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>TensorDictModule &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="TorchRL envs" href="torch_envs.html" />
    <link rel="prev" title="TensorDict" href="tensordict_tutorial.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  main (None )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torchrl_demo.html">Introduction to TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensordict_tutorial.html">TensorDict</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">TensorDictModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="coding_ddpg.html">Coding DDPG using TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="coding_dqn.html">Coding a pixel-based DQN using TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>TensorDictModule</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/tensordict_module.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">tutorials/tensordict_module</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-tutorials-tensordict-module-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="tensordictmodule">
<span id="sphx-glr-tutorials-tensordict-module-py"></span><h1>TensorDictModule<a class="headerlink" href="#tensordictmodule" title="Permalink to this heading">¶</a></h1>
<p>We recommand reading the TensorDict tutorial before going through this one.</p>
<p>For a convenient usage of the <code class="docutils literal notranslate"><span class="pre">TensorDict</span></code> class with <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensordict</span></code> provides an interface between the two named <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code>.
The <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> class is an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> that takes a
<code class="docutils literal notranslate"><span class="pre">TensorDict</span></code> as input when called.
It is up to the user to define the keys to be read as input and output.</p>
<section id="tensordictmodule-by-examples">
<h2>TensorDictModule by examples<a class="headerlink" href="#tensordictmodule-by-examples" title="Permalink to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">tensordict</span> <span class="kn">import</span> <span class="n">TensorDict</span>
<span class="kn">from</span> <span class="nn">tensordict.nn</span> <span class="kn">import</span> <span class="n">TensorDictModule</span><span class="p">,</span> <span class="n">TensorDictSequential</span>
</pre></div>
</div>
<section id="example-1-simple-usage">
<h3>Example 1: Simple usage<a class="headerlink" href="#example-1-simple-usage" title="Permalink to this heading">¶</a></h3>
<p>We have a <code class="docutils literal notranslate"><span class="pre">TensorDict</span></code> with 2 entries <code class="docutils literal notranslate"><span class="pre">&quot;a&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;b&quot;</span></code> but only the
value associated with <code class="docutils literal notranslate"><span class="pre">&quot;a&quot;</span></code> has to be read by the network.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensordict</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)},</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">],</span> <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a_out&quot;</span><span class="p">])</span>
<span class="n">linear</span><span class="p">(</span><span class="n">tensordict</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensordict</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TensorDict(
    fields={
        a: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),
        a_out: Tensor(shape=torch.Size([5, 10]), device=cpu, dtype=torch.float32, is_shared=False),
        b: Tensor(shape=torch.Size([5, 4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([5]),
    device=None,
    is_shared=False)
</pre></div>
</div>
</section>
<section id="example-2-multiple-inputs">
<h3>Example 2: Multiple inputs<a class="headerlink" href="#example-2-multiple-inputs" title="Permalink to this heading">¶</a></h3>
<p>Suppose we have a slightly more complex network that takes 2 entries and
averages them into a single output tensor. To make a <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code>
instance read multiple input values, one must register them in the
<code class="docutils literal notranslate"><span class="pre">in_keys</span></code> keyword argument of the constructor.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MergeLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_1</span><span class="p">,</span> <span class="n">in_2</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_1</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_2</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x_2</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensordict</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="p">},</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">mergelinear</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span>
    <span class="n">MergeLinear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">mergelinear</span><span class="p">(</span><span class="n">tensordict</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TensorDict(
    fields={
        a: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),
        b: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        output: Tensor(shape=torch.Size([5, 10]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([5]),
    device=None,
    is_shared=False)
</pre></div>
</div>
</section>
<section id="example-3-multiple-outputs">
<h3>Example 3: Multiple outputs<a class="headerlink" href="#example-3-multiple-outputs" title="Permalink to this heading">¶</a></h3>
<p>Similarly, <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> not only supports multiple inputs but also
multiple outputs. To make a <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> instance write to multiple
output values, one must register them in the <code class="docutils literal notranslate"><span class="pre">out_keys</span></code> keyword argument
of the constructor.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_1</span><span class="p">,</span> <span class="n">out_1</span><span class="p">,</span> <span class="n">out_2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_1</span><span class="p">,</span> <span class="n">out_1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_1</span><span class="p">,</span> <span class="n">out_2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensordict</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)},</span> <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>

<span class="n">splitlinear</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span>
    <span class="n">MultiHeadLinear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output_1&quot;</span><span class="p">,</span> <span class="s2">&quot;output_2&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">splitlinear</span><span class="p">(</span><span class="n">tensordict</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TensorDict(
    fields={
        a: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),
        output_1: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        output_2: Tensor(shape=torch.Size([5, 10]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([5]),
    device=None,
    is_shared=False)
</pre></div>
</div>
<p>When having multiple input keys and output keys, make sure they match the
order in the module.</p>
<p><code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> can work with <code class="docutils literal notranslate"><span class="pre">TensorDict</span></code> instances that contain
more tensors than what the <code class="docutils literal notranslate"><span class="pre">in_keys</span></code> attribute indicates.</p>
<p>Unless a <code class="docutils literal notranslate"><span class="pre">vmap</span></code> operator is used, the <code class="docutils literal notranslate"><span class="pre">TensorDict</span></code> is modified in-place.</p>
<p><strong>Ignoring some outputs</strong></p>
<p>Note that it is possible to avoid writing some of the tensors to the
<code class="docutils literal notranslate"><span class="pre">TensorDict</span></code> output, using <code class="docutils literal notranslate"><span class="pre">&quot;_&quot;</span></code> in <code class="docutils literal notranslate"><span class="pre">out_keys</span></code>.</p>
</section>
<section id="example-4-combining-multiple-tensordictmodule-with-tensordictsequential">
<h3>Example 4: Combining multiple <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> with <code class="docutils literal notranslate"><span class="pre">TensorDictSequential</span></code><a class="headerlink" href="#example-4-combining-multiple-tensordictmodule-with-tensordictsequential" title="Permalink to this heading">¶</a></h3>
<p>To combine multiple <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> instances, we can use
<code class="docutils literal notranslate"><span class="pre">TensorDictSequential</span></code>. We create a list where each <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> must
be executed sequentially. <code class="docutils literal notranslate"><span class="pre">TensorDictSequential</span></code> will read and write keys to the
tensordict following the sequence of modules provided.</p>
<p>We can also gather the inputs needed by <code class="docutils literal notranslate"><span class="pre">TensorDictSequential</span></code> with the
<code class="docutils literal notranslate"><span class="pre">in_keys</span></code> property, and the outputs keys are found at the <code class="docutils literal notranslate"><span class="pre">out_keys</span></code> attribute.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensordict</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)},</span> <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>

<span class="n">splitlinear</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span>
    <span class="n">MultiHeadLinear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output_1&quot;</span><span class="p">,</span> <span class="s2">&quot;output_2&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">mergelinear</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span>
    <span class="n">MergeLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">13</span><span class="p">),</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output_1&quot;</span><span class="p">,</span> <span class="s2">&quot;output_2&quot;</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">split_and_merge_linear</span> <span class="o">=</span> <span class="n">TensorDictSequential</span><span class="p">(</span><span class="n">splitlinear</span><span class="p">,</span> <span class="n">mergelinear</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">split_and_merge_linear</span><span class="p">(</span><span class="n">tensordict</span><span class="p">)[</span><span class="s2">&quot;output&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">13</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="example-5-compatibility-with-functorch">
<h3>Example 5: Compatibility with functorch<a class="headerlink" href="#example-5-compatibility-with-functorch" title="Permalink to this heading">¶</a></h3>
<p>tensordict.nn is compatible with functorch. It also comes with its own functional
utilities. Let us have a look:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">functorch</span>

<span class="n">tensordict</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)},</span> <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>

<span class="n">splitlinear</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span>
    <span class="n">MultiHeadLinear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output_1&quot;</span><span class="p">,</span> <span class="s2">&quot;output_2&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">func</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">buffers</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">make_functional_with_buffers</span><span class="p">(</span><span class="n">splitlinear</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">buffers</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TensorDict(
    fields={
        a: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),
        output_1: Tensor(shape=torch.Size([5, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        output_2: Tensor(shape=torch.Size([5, 10]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([5]),
    device=None,
    is_shared=False)
</pre></div>
</div>
<p>This can be used with the vmap operator. For example, we use 3 replicas of the
params and buffers and execute a vectorized map over these for a single batch
of data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params_expand</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">*</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>
<span class="n">buffers_expand</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">*</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">buffers</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">params_expand</span><span class="p">,</span> <span class="n">buffers_expand</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TensorDict(
    fields={
        a: Tensor(shape=torch.Size([3, 5, 3]), device=cpu, dtype=torch.float32, is_shared=False),
        output_1: Tensor(shape=torch.Size([3, 5, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        output_2: Tensor(shape=torch.Size([3, 5, 10]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([3, 5]),
    device=None,
    is_shared=False)
</pre></div>
</div>
<p>We can also use the native <code class="xref py py-obj docutils literal notranslate"><span class="pre">get_functional()</span></code> function from tensordict.nn,
which modifies the module to make it accept the parameters as regular inputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensordict.nn</span> <span class="kn">import</span> <span class="n">make_functional</span>

<span class="n">tensordict</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)},</span> <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
<span class="n">num_models</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">],</span> <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">])</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">make_functional</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># we stack two groups of parameters to show the vmap usage:</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">params</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))],</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">result_td</span> <span class="o">=</span> <span class="n">functorch</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))(</span><span class="n">tensordict</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;the output tensordict shape is: &quot;</span><span class="p">,</span> <span class="n">result_td</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">tensordict.nn</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ProbabilisticTensorDictModule</span><span class="p">,</span>
    <span class="n">ProbabilisticTensorDictSequential</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>the output tensordict shape is:  torch.Size([2, 5])
</pre></div>
</div>
</section>
</section>
<section id="do-s-and-don-t-with-tensordictmodule">
<h2>Do’s and don’t with TensorDictModule<a class="headerlink" href="#do-s-and-don-t-with-tensordictmodule" title="Permalink to this heading">¶</a></h2>
<p>Don’t use <code class="docutils literal notranslate"><span class="pre">nn.Sequence</span></code>, similar to <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>, it would break features
such as <code class="docutils literal notranslate"><span class="pre">functorch</span></code> compatibility. Do use <code class="docutils literal notranslate"><span class="pre">TensorDictSequential</span></code> instead.</p>
<p>Don’t assign the output tensordict to a new variable, as the output
tensordict is just the input modified in-place:</p>
<blockquote>
<div><p>tensordict = module(tensordict)  # ok!</p>
<p>tensordict_out = module(tensordict)  # don’t!</p>
</div></blockquote>
<section id="probabilistictensordictmodule">
<h3><code class="docutils literal notranslate"><span class="pre">ProbabilisticTensorDictModule</span></code><a class="headerlink" href="#probabilistictensordictmodule" title="Permalink to this heading">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">ProbabilisticTensorDictModule</span></code> is a non-parametric module representing a
probability distribution. Distribution parameters are read from tensordict
input, and the output is written to an output tensordict. The output is
sampled given some rule, specified by the input <code class="docutils literal notranslate"><span class="pre">default_interaction_mode</span></code>
argument and the <code class="docutils literal notranslate"><span class="pre">exploration_mode()</span></code> global function. If they conflict,
the context manager precedes.</p>
<p>It can be wired together with a <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> that returns
a tensordict updated with the distribution parameters using
<code class="docutils literal notranslate"><span class="pre">ProbabilisticTensorDictSequential</span></code>. This is a special case of
<code class="docutils literal notranslate"><span class="pre">TensorDictSequential</span></code> that terminates in a
<code class="docutils literal notranslate"><span class="pre">ProbabilisticTensorDictModule</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">ProbabilisticTensorDictModule</span></code> is responsible for constructing the
distribution (through the <code class="docutils literal notranslate"><span class="pre">get_dist()</span></code> method) and/or sampling from this
distribution (through a regular <code class="docutils literal notranslate"><span class="pre">__call__()</span></code> to the module). The same
<code class="docutils literal notranslate"><span class="pre">get_dist()</span></code> method is exposed on <a href="#id1"><span class="problematic" id="id2">``</span></a>ProbabilisticTensorDictSequential.</p>
<p>One can find the parameters in the output tensordict as well as the log
probability if needed.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchrl.modules</span> <span class="kn">import</span> <span class="n">NormalParamWrapper</span><span class="p">,</span> <span class="n">TanhNormal</span>

<span class="n">td</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">({</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="s2">&quot;hidden&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)},</span> <span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">NormalParamWrapper</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">module</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="s2">&quot;hidden&quot;</span><span class="p">],</span> <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">])</span>
<span class="n">td_module</span> <span class="o">=</span> <span class="n">ProbabilisticTensorDictSequential</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">ProbabilisticTensorDictModule</span><span class="p">(</span>
        <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">],</span>
        <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
        <span class="n">distribution_class</span><span class="o">=</span><span class="n">TanhNormal</span><span class="p">,</span>
        <span class="n">return_log_prob</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TensorDict before going through module: </span><span class="si">{</span><span class="n">td</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">td_module</span><span class="p">(</span><span class="n">td</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TensorDict after going through module now as keys action, loc and scale: </span><span class="si">{</span><span class="n">td</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TensorDict before going through module: TensorDict(
    fields={
        hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
        input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([3]),
    device=None,
    is_shared=False)
TensorDict after going through module now as keys action, loc and scale: TensorDict(
    fields={
        action: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
        input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        sample_log_prob: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
        scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([3]),
    device=None,
    is_shared=False)
</pre></div>
</div>
</section>
<section id="actor">
<h3><code class="docutils literal notranslate"><span class="pre">Actor</span></code><a class="headerlink" href="#actor" title="Permalink to this heading">¶</a></h3>
<p>Actor inherits from <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> and comes with a default value
for <code class="docutils literal notranslate"><span class="pre">out_keys</span></code> of <code class="docutils literal notranslate"><span class="pre">[&quot;action&quot;]</span></code>.</p>
</section>
<section id="probabilisticactor">
<h3><code class="docutils literal notranslate"><span class="pre">ProbabilisticActor</span></code><a class="headerlink" href="#probabilisticactor" title="Permalink to this heading">¶</a></h3>
<p>General class for probabilistic actors in RL that inherits from
<code class="docutils literal notranslate"><span class="pre">ProbabilisticTensorDictModule</span></code>. Similarly to <code class="docutils literal notranslate"><span class="pre">Actor</span></code>, it comes with
default values for the <code class="docutils literal notranslate"><span class="pre">out_keys</span></code> (<code class="docutils literal notranslate"><span class="pre">[&quot;action&quot;]</span></code>).</p>
</section>
<section id="actorcriticoperator">
<h3><code class="docutils literal notranslate"><span class="pre">ActorCriticOperator</span></code><a class="headerlink" href="#actorcriticoperator" title="Permalink to this heading">¶</a></h3>
<p>Similarly, <code class="docutils literal notranslate"><span class="pre">ActorCriticOperator</span></code> inherits from <code class="docutils literal notranslate"><span class="pre">TensorDictSequentialand</span></code>
wraps both an actor network and a value Network.</p>
<p><code class="docutils literal notranslate"><span class="pre">ActorCriticOperator</span></code> will first compute the action from the actor and
then the value according to this action.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchrl.modules</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ActorCriticOperator</span><span class="p">,</span>
    <span class="n">MLP</span><span class="p">,</span>
    <span class="n">NormalParamWrapper</span><span class="p">,</span>
    <span class="n">TanhNormal</span><span class="p">,</span>
    <span class="n">ValueOperator</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torchrl.modules.tensordict_module</span> <span class="kn">import</span> <span class="n">ProbabilisticActor</span>

<span class="n">module_hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">td_module_hidden</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">module_hidden</span><span class="p">,</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;hidden&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">module_action</span> <span class="o">=</span> <span class="n">NormalParamWrapper</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">module_action</span> <span class="o">=</span> <span class="n">TensorDictModule</span><span class="p">(</span>
    <span class="n">module_action</span><span class="p">,</span> <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;hidden&quot;</span><span class="p">],</span> <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">td_module_action</span> <span class="o">=</span> <span class="n">ProbabilisticActor</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">module_action</span><span class="p">,</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">distribution_class</span><span class="o">=</span><span class="n">TanhNormal</span><span class="p">,</span>
    <span class="n">return_log_prob</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">module_value</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_cells</span><span class="o">=</span><span class="p">[])</span>
<span class="n">td_module_value</span> <span class="o">=</span> <span class="n">ValueOperator</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">module_value</span><span class="p">,</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;hidden&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;state_action_value&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">td_module</span> <span class="o">=</span> <span class="n">ActorCriticOperator</span><span class="p">(</span><span class="n">td_module_hidden</span><span class="p">,</span> <span class="n">td_module_action</span><span class="p">,</span> <span class="n">td_module_value</span><span class="p">)</span>
<span class="n">td</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">({</span><span class="s2">&quot;observation&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)},</span> <span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">td</span><span class="p">)</span>
<span class="n">td_clone</span> <span class="o">=</span> <span class="n">td_module</span><span class="p">(</span><span class="n">td</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">td_clone</span><span class="p">)</span>
<span class="n">td_clone</span> <span class="o">=</span> <span class="n">td_module</span><span class="o">.</span><span class="n">get_policy_operator</span><span class="p">()(</span><span class="n">td</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Policy: </span><span class="si">{</span><span class="n">td_clone</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># no value</span>
<span class="n">td_clone</span> <span class="o">=</span> <span class="n">td_module</span><span class="o">.</span><span class="n">get_critic_operator</span><span class="p">()(</span><span class="n">td</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Critic: </span><span class="si">{</span><span class="n">td_clone</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># no action</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TensorDict(
    fields={
        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([3]),
    device=None,
    is_shared=False)
TensorDict(
    fields={
        action: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        hidden: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        sample_log_prob: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
        scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        state_action_value: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([3]),
    device=None,
    is_shared=False)
Policy: TensorDict(
    fields={
        action: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        hidden: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        sample_log_prob: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
        scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([3]),
    device=None,
    is_shared=False)
Critic: TensorDict(
    fields={
        action: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        hidden: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        sample_log_prob: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
        scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
        state_action_value: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([3]),
    device=None,
    is_shared=False)
</pre></div>
</div>
<p>Other blocks exist such as:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">ValueOperator</span></code> which is a general class for value functions in RL.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">ActorCriticWrapper</span></code> which wraps together an actor and a value model
that do not share a common observation embedding network.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">ActorValueOperator</span></code> which wraps together an actor and a value model
that share a common observation embedding network.</p></li>
</ul>
</section>
</section>
<section id="showcase-implementing-a-transformer-using-tensordictmodule">
<h2>Showcase: Implementing a transformer using TensorDictModule<a class="headerlink" href="#showcase-implementing-a-transformer-using-tensordictmodule" title="Permalink to this heading">¶</a></h2>
<p>To demonstrate the flexibility of <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code>, we are going to
create a transformer that reads <code class="docutils literal notranslate"><span class="pre">TensorDict</span></code> objects using <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code>.</p>
<p>The following figure shows the classical transformer architecture
(Vaswani et al, 2017).</p>
<img alt="The transformer png" src="../_images/transformer.png" />
<p>We have let the positional encoders aside for simplicity.</p>
<p>Let’s re-write the classical transformers blocks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TokensToQKV</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">to_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_to</span><span class="p">,</span> <span class="n">X_from</span><span class="p">):</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">X_to</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">(</span><span class="n">X_from</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">X_from</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span>


<span class="k">class</span> <span class="nc">SplitHeads</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">to_num</span><span class="p">,</span> <span class="n">latent_dim</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">from_num</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">d_tensor</span> <span class="o">=</span> <span class="n">latent_dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">to_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">from_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">from_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">d_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span>


<span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">to_num</span><span class="p">,</span> <span class="n">d_in</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_in</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">V</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">to_num</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="n">d_in</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">attn</span>


<span class="k">class</span> <span class="nc">SkipLayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">((</span><span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x_0</span> <span class="o">+</span> <span class="n">x_1</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FFN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">to_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokens_to_qkv</span> <span class="o">=</span> <span class="n">TokensToQKV</span><span class="p">(</span><span class="n">to_dim</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span> <span class="o">=</span> <span class="n">SplitHeads</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skip</span> <span class="o">=</span> <span class="n">SkipLayerNorm</span><span class="p">(</span><span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_to</span><span class="p">,</span> <span class="n">X_from</span><span class="p">):</span>
        <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokens_to_qkv</span><span class="p">(</span><span class="n">X_to</span><span class="p">,</span> <span class="n">X_from</span><span class="p">)</span>
        <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="n">X_to</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">EncoderTransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_block</span> <span class="o">=</span> <span class="n">AttentionBlock</span><span class="p">(</span>
            <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">to_dim</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">to_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skip</span> <span class="o">=</span> <span class="n">SkipLayerNorm</span><span class="p">(</span><span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_to</span><span class="p">):</span>
        <span class="n">X_to</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_block</span><span class="p">(</span><span class="n">X_to</span><span class="p">,</span> <span class="n">X_to</span><span class="p">)</span>
        <span class="n">X_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="p">(</span><span class="n">X_to</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="n">X_out</span><span class="p">,</span> <span class="n">X_to</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DecoderTransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_block</span> <span class="o">=</span> <span class="n">AttentionBlock</span><span class="p">(</span>
            <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_block</span> <span class="o">=</span> <span class="n">EncoderTransformerBlock</span><span class="p">(</span>
            <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_to</span><span class="p">,</span> <span class="n">X_from</span><span class="p">):</span>
        <span class="n">X_to</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_block</span><span class="p">(</span><span class="n">X_to</span><span class="p">,</span> <span class="n">X_from</span><span class="p">)</span>
        <span class="n">X_to</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_block</span><span class="p">(</span><span class="n">X_to</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X_to</span>


<span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">EncoderTransformerBlock</span><span class="p">(</span><span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_to</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">)):</span>
            <span class="n">X_to</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">X_to</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X_to</span>


<span class="k">class</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">DecoderTransformerBlock</span><span class="p">(</span><span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_to</span><span class="p">,</span> <span class="n">X_from</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">)):</span>
            <span class="n">X_to</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">X_to</span><span class="p">,</span> <span class="n">X_from</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X_to</span>


<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">from_len</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span>
            <span class="n">num_blocks</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span>
            <span class="n">num_blocks</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">from_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_to</span><span class="p">,</span> <span class="n">X_from</span><span class="p">):</span>
        <span class="n">X_to</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">X_to</span><span class="p">)</span>
        <span class="n">X_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">X_from</span><span class="p">,</span> <span class="n">X_to</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X_out</span>
</pre></div>
</div>
<p>We first create the <code class="docutils literal notranslate"><span class="pre">AttentionBlockTensorDict</span></code>, the attention block using
<code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> and <code class="docutils literal notranslate"><span class="pre">TensorDictSequential</span></code>.</p>
<p>The wiring operation that connects the modules to each other requires us
to indicate which key each of them must read and write. Unlike
<code class="docutils literal notranslate"><span class="pre">nn.Sequence</span></code>, a <code class="docutils literal notranslate"><span class="pre">TensorDictSequential</span></code> can read/write more than one
input/output. Moreover, its components inputs need not be identical to the
previous layers outputs, allowing us to code complicated neural architecture.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AttentionBlockTensorDict</span><span class="p">(</span><span class="n">TensorDictSequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">to_name</span><span class="p">,</span>
        <span class="n">from_name</span><span class="p">,</span>
        <span class="n">to_dim</span><span class="p">,</span>
        <span class="n">to_len</span><span class="p">,</span>
        <span class="n">from_dim</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">TensorDictModule</span><span class="p">(</span>
                <span class="n">TokensToQKV</span><span class="p">(</span><span class="n">to_dim</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">),</span>
                <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="n">to_name</span><span class="p">,</span> <span class="n">from_name</span><span class="p">],</span>
                <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;K&quot;</span><span class="p">,</span> <span class="s2">&quot;V&quot;</span><span class="p">],</span>
            <span class="p">),</span>
            <span class="n">TensorDictModule</span><span class="p">(</span>
                <span class="n">SplitHeads</span><span class="p">(</span><span class="n">num_heads</span><span class="p">),</span>
                <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;K&quot;</span><span class="p">,</span> <span class="s2">&quot;V&quot;</span><span class="p">],</span>
                <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;K&quot;</span><span class="p">,</span> <span class="s2">&quot;V&quot;</span><span class="p">],</span>
            <span class="p">),</span>
            <span class="n">TensorDictModule</span><span class="p">(</span>
                <span class="n">Attention</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">),</span>
                <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Q&quot;</span><span class="p">,</span> <span class="s2">&quot;K&quot;</span><span class="p">,</span> <span class="s2">&quot;V&quot;</span><span class="p">],</span>
                <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X_out&quot;</span><span class="p">,</span> <span class="s2">&quot;Attn&quot;</span><span class="p">],</span>
            <span class="p">),</span>
            <span class="n">TensorDictModule</span><span class="p">(</span>
                <span class="n">SkipLayerNorm</span><span class="p">(</span><span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">),</span>
                <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="n">to_name</span><span class="p">,</span> <span class="s2">&quot;X_out&quot;</span><span class="p">],</span>
                <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="n">to_name</span><span class="p">],</span>
            <span class="p">),</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>We build the encoder and decoder blocks that will be part of the transformer
thanks to <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformerBlockEncoderTensorDict</span><span class="p">(</span><span class="n">TensorDictSequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">to_name</span><span class="p">,</span>
        <span class="n">from_name</span><span class="p">,</span>
        <span class="n">to_dim</span><span class="p">,</span>
        <span class="n">to_len</span><span class="p">,</span>
        <span class="n">from_dim</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">AttentionBlockTensorDict</span><span class="p">(</span>
                <span class="n">to_name</span><span class="p">,</span>
                <span class="n">from_name</span><span class="p">,</span>
                <span class="n">to_dim</span><span class="p">,</span>
                <span class="n">to_len</span><span class="p">,</span>
                <span class="n">from_dim</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">TensorDictModule</span><span class="p">(</span>
                <span class="n">FFN</span><span class="p">(</span><span class="n">to_dim</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">to_dim</span><span class="p">),</span>
                <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="n">to_name</span><span class="p">],</span>
                <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X_out&quot;</span><span class="p">],</span>
            <span class="p">),</span>
            <span class="n">TensorDictModule</span><span class="p">(</span>
                <span class="n">SkipLayerNorm</span><span class="p">(</span><span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">),</span>
                <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="n">to_name</span><span class="p">,</span> <span class="s2">&quot;X_out&quot;</span><span class="p">],</span>
                <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="n">to_name</span><span class="p">],</span>
            <span class="p">),</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">TransformerBlockDecoderTensorDict</span><span class="p">(</span><span class="n">TensorDictSequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">to_name</span><span class="p">,</span>
        <span class="n">from_name</span><span class="p">,</span>
        <span class="n">to_dim</span><span class="p">,</span>
        <span class="n">to_len</span><span class="p">,</span>
        <span class="n">from_dim</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">AttentionBlockTensorDict</span><span class="p">(</span>
                <span class="n">to_name</span><span class="p">,</span>
                <span class="n">to_name</span><span class="p">,</span>
                <span class="n">to_dim</span><span class="p">,</span>
                <span class="n">to_len</span><span class="p">,</span>
                <span class="n">to_dim</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">TransformerBlockEncoderTensorDict</span><span class="p">(</span>
                <span class="n">to_name</span><span class="p">,</span>
                <span class="n">from_name</span><span class="p">,</span>
                <span class="n">to_dim</span><span class="p">,</span>
                <span class="n">to_len</span><span class="p">,</span>
                <span class="n">from_dim</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>We create the transformer encoder and decoder.</p>
<p>For an encoder, we just need to take the same tokens for both queries,
keys and values.</p>
<p>For a decoder, we now can extract info from <code class="docutils literal notranslate"><span class="pre">X_from</span></code> into <code class="docutils literal notranslate"><span class="pre">X_to</span></code>.
<code class="docutils literal notranslate"><span class="pre">X_from</span></code> will map to queries whereas <code class="docutils literal notranslate"><span class="pre">X_from</span></code> will map to keys and values.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformerEncoderTensorDict</span><span class="p">(</span><span class="n">TensorDictSequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_blocks</span><span class="p">,</span>
        <span class="n">to_name</span><span class="p">,</span>
        <span class="n">from_name</span><span class="p">,</span>
        <span class="n">to_dim</span><span class="p">,</span>
        <span class="n">to_len</span><span class="p">,</span>
        <span class="n">from_dim</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span>
                <span class="n">TransformerBlockEncoderTensorDict</span><span class="p">(</span>
                    <span class="n">to_name</span><span class="p">,</span>
                    <span class="n">from_name</span><span class="p">,</span>
                    <span class="n">to_dim</span><span class="p">,</span>
                    <span class="n">to_len</span><span class="p">,</span>
                    <span class="n">from_dim</span><span class="p">,</span>
                    <span class="n">latent_dim</span><span class="p">,</span>
                    <span class="n">num_heads</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">TransformerDecoderTensorDict</span><span class="p">(</span><span class="n">TensorDictSequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_blocks</span><span class="p">,</span>
        <span class="n">to_name</span><span class="p">,</span>
        <span class="n">from_name</span><span class="p">,</span>
        <span class="n">to_dim</span><span class="p">,</span>
        <span class="n">to_len</span><span class="p">,</span>
        <span class="n">from_dim</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span>
                <span class="n">TransformerBlockDecoderTensorDict</span><span class="p">(</span>
                    <span class="n">to_name</span><span class="p">,</span>
                    <span class="n">from_name</span><span class="p">,</span>
                    <span class="n">to_dim</span><span class="p">,</span>
                    <span class="n">to_len</span><span class="p">,</span>
                    <span class="n">from_dim</span><span class="p">,</span>
                    <span class="n">latent_dim</span><span class="p">,</span>
                    <span class="n">num_heads</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">TransformerTensorDict</span><span class="p">(</span><span class="n">TensorDictSequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_blocks</span><span class="p">,</span>
        <span class="n">to_name</span><span class="p">,</span>
        <span class="n">from_name</span><span class="p">,</span>
        <span class="n">to_dim</span><span class="p">,</span>
        <span class="n">to_len</span><span class="p">,</span>
        <span class="n">from_dim</span><span class="p">,</span>
        <span class="n">from_len</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">TransformerEncoderTensorDict</span><span class="p">(</span>
                <span class="n">num_blocks</span><span class="p">,</span>
                <span class="n">to_name</span><span class="p">,</span>
                <span class="n">to_name</span><span class="p">,</span>
                <span class="n">to_dim</span><span class="p">,</span>
                <span class="n">to_len</span><span class="p">,</span>
                <span class="n">to_dim</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">TransformerDecoderTensorDict</span><span class="p">(</span>
                <span class="n">num_blocks</span><span class="p">,</span>
                <span class="n">from_name</span><span class="p">,</span>
                <span class="n">to_name</span><span class="p">,</span>
                <span class="n">from_dim</span><span class="p">,</span>
                <span class="n">from_len</span><span class="p">,</span>
                <span class="n">to_dim</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>We now test our new <code class="docutils literal notranslate"><span class="pre">TransformerTensorDict</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">to_dim</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">from_dim</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">to_len</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">from_len</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_blocks</span> <span class="o">=</span> <span class="mi">6</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;X_encode&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">),</span>
        <span class="s2">&quot;X_decode&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">from_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">),</span>
    <span class="p">},</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">transformer</span> <span class="o">=</span> <span class="n">TransformerTensorDict</span><span class="p">(</span>
    <span class="n">num_blocks</span><span class="p">,</span>
    <span class="s2">&quot;X_encode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;X_decode&quot;</span><span class="p">,</span>
    <span class="n">to_dim</span><span class="p">,</span>
    <span class="n">to_len</span><span class="p">,</span>
    <span class="n">from_dim</span><span class="p">,</span>
    <span class="n">from_len</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">transformer</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="n">tokens</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TensorDict(
    fields={
        Attn: Tensor(shape=torch.Size([8, 2, 10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
        K: Tensor(shape=torch.Size([8, 2, 3, 5]), device=cpu, dtype=torch.float32, is_shared=False),
        Q: Tensor(shape=torch.Size([8, 2, 10, 5]), device=cpu, dtype=torch.float32, is_shared=False),
        V: Tensor(shape=torch.Size([8, 2, 3, 5]), device=cpu, dtype=torch.float32, is_shared=False),
        X_decode: Tensor(shape=torch.Size([8, 10, 6]), device=cpu, dtype=torch.float32, is_shared=False),
        X_encode: Tensor(shape=torch.Size([8, 3, 5]), device=cpu, dtype=torch.float32, is_shared=False),
        X_out: Tensor(shape=torch.Size([8, 10, 6]), device=cpu, dtype=torch.float32, is_shared=False)},
    batch_size=torch.Size([8]),
    device=None,
    is_shared=False)
</pre></div>
</div>
<p>We’ve achieved to create a transformer with <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code>. This
shows that <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> is a flexible module that can implement
complex operarations.</p>
<section id="benchmarking">
<h3>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">to_dim</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">from_dim</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">to_len</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">from_len</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_blocks</span> <span class="o">=</span> <span class="mi">6</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">td_tokens</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;X_encode&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">),</span>
        <span class="s2">&quot;X_decode&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">from_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">),</span>
    <span class="p">},</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_encode</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">)</span>
<span class="n">X_decode</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">from_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tdtransformer</span> <span class="o">=</span> <span class="n">TransformerTensorDict</span><span class="p">(</span>
    <span class="n">num_blocks</span><span class="p">,</span>
    <span class="s2">&quot;X_encode&quot;</span><span class="p">,</span>
    <span class="s2">&quot;X_decode&quot;</span><span class="p">,</span>
    <span class="n">to_dim</span><span class="p">,</span>
    <span class="n">to_len</span><span class="p">,</span>
    <span class="n">from_dim</span><span class="p">,</span>
    <span class="n">from_len</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
    <span class="n">num_blocks</span><span class="p">,</span> <span class="n">to_dim</span><span class="p">,</span> <span class="n">to_len</span><span class="p">,</span> <span class="n">from_dim</span><span class="p">,</span> <span class="n">from_len</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Inference Time</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">tdtransformer</span><span class="p">(</span><span class="n">td_tokens</span><span class="p">)</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Execution time:&quot;</span><span class="p">,</span> <span class="n">t2</span> <span class="o">-</span> <span class="n">t1</span><span class="p">,</span> <span class="s2">&quot;seconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Execution time: 0.009760379791259766 seconds
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t3</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">X_out</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">X_encode</span><span class="p">,</span> <span class="n">X_decode</span><span class="p">)</span>
<span class="n">t4</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Execution time:&quot;</span><span class="p">,</span> <span class="n">t4</span> <span class="o">-</span> <span class="n">t3</span><span class="p">,</span> <span class="s2">&quot;seconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Execution time: 0.006848812103271484 seconds
</pre></div>
</div>
<p>We can see on this minimal example that the overhead introduced by
<code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> is marginal.</p>
<p>Have fun with TensorDictModule!</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.104 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-tensordict-module-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/3e61c81f224cb8cecc06413ef124320a/tensordict_module.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tensordict_module.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/ba42555713e16dc342a5f3e550dec192/tensordict_module.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tensordict_module.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="torch_envs.html" class="btn btn-neutral float-right" title="TorchRL envs" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="tensordict_tutorial.html" class="btn btn-neutral" title="TensorDict" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">TensorDictModule</a><ul>
<li><a class="reference internal" href="#tensordictmodule-by-examples">TensorDictModule by examples</a><ul>
<li><a class="reference internal" href="#example-1-simple-usage">Example 1: Simple usage</a></li>
<li><a class="reference internal" href="#example-2-multiple-inputs">Example 2: Multiple inputs</a></li>
<li><a class="reference internal" href="#example-3-multiple-outputs">Example 3: Multiple outputs</a></li>
<li><a class="reference internal" href="#example-4-combining-multiple-tensordictmodule-with-tensordictsequential">Example 4: Combining multiple <code class="docutils literal notranslate"><span class="pre">TensorDictModule</span></code> with <code class="docutils literal notranslate"><span class="pre">TensorDictSequential</span></code></a></li>
<li><a class="reference internal" href="#example-5-compatibility-with-functorch">Example 5: Compatibility with functorch</a></li>
</ul>
</li>
<li><a class="reference internal" href="#do-s-and-don-t-with-tensordictmodule">Do’s and don’t with TensorDictModule</a><ul>
<li><a class="reference internal" href="#probabilistictensordictmodule"><code class="docutils literal notranslate"><span class="pre">ProbabilisticTensorDictModule</span></code></a></li>
<li><a class="reference internal" href="#actor"><code class="docutils literal notranslate"><span class="pre">Actor</span></code></a></li>
<li><a class="reference internal" href="#probabilisticactor"><code class="docutils literal notranslate"><span class="pre">ProbabilisticActor</span></code></a></li>
<li><a class="reference internal" href="#actorcriticoperator"><code class="docutils literal notranslate"><span class="pre">ActorCriticOperator</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#showcase-implementing-a-transformer-using-tensordictmodule">Showcase: Implementing a transformer using TensorDictModule</a><ul>
<li><a class="reference internal" href="#benchmarking">Benchmarking</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/sphinx_highlight.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>

        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>