


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchrl.trainers.trainers &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  main (None )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/tensordict_tutorial.html">TensorDict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/tensordict_module.html">TensorDictModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/torch_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/coding_ddpg.html">Coding DDPG using TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/coding_dqn.html">Coding a pixel-based DQN using TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchrl.trainers.trainers</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">_modules/torchrl/trainers/trainers</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../../../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../../../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../../../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchrl.trainers.trainers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the MIT license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">abc</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">textwrap</span> <span class="kn">import</span> <span class="n">indent</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch.nn</span>
<span class="kn">from</span> <span class="nn">tensordict.tensordict</span> <span class="kn">import</span> <span class="n">pad</span><span class="p">,</span> <span class="n">TensorDictBase</span>
<span class="kn">from</span> <span class="nn">tensordict.utils</span> <span class="kn">import</span> <span class="n">expand_right</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>

<span class="kn">from</span> <span class="nn">torchrl._utils</span> <span class="kn">import</span> <span class="n">_CKPT_BACKEND</span><span class="p">,</span> <span class="n">KeyDependentDefaultDict</span>
<span class="kn">from</span> <span class="nn">torchrl.collectors.collectors</span> <span class="kn">import</span> <span class="n">_DataCollector</span>
<span class="kn">from</span> <span class="nn">torchrl.data</span> <span class="kn">import</span> <span class="n">TensorDictPrioritizedReplayBuffer</span><span class="p">,</span> <span class="n">TensorDictReplayBuffer</span>
<span class="kn">from</span> <span class="nn">torchrl.data.utils</span> <span class="kn">import</span> <span class="n">DEVICE_TYPING</span>
<span class="kn">from</span> <span class="nn">torchrl.envs.common</span> <span class="kn">import</span> <span class="n">EnvBase</span>
<span class="kn">from</span> <span class="nn">torchrl.envs.utils</span> <span class="kn">import</span> <span class="n">set_exploration_mode</span>
<span class="kn">from</span> <span class="nn">torchrl.modules</span> <span class="kn">import</span> <span class="n">SafeModule</span>
<span class="kn">from</span> <span class="nn">torchrl.objectives.common</span> <span class="kn">import</span> <span class="n">LossModule</span>
<span class="kn">from</span> <span class="nn">torchrl.trainers.loggers</span> <span class="kn">import</span> <span class="n">Logger</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

    <span class="n">_has_tqdm</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_has_tqdm</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchsnapshot</span> <span class="kn">import</span> <span class="n">Snapshot</span><span class="p">,</span> <span class="n">StateDict</span>

    <span class="n">_has_ts</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_has_ts</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">REPLAY_BUFFER_CLASS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;prioritized&quot;</span><span class="p">:</span> <span class="n">TensorDictPrioritizedReplayBuffer</span><span class="p">,</span>
    <span class="s2">&quot;circular&quot;</span><span class="p">:</span> <span class="n">TensorDictReplayBuffer</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">LOGGER_METHODS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;grad_norm&quot;</span><span class="p">:</span> <span class="s2">&quot;log_scalar&quot;</span><span class="p">,</span>
    <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="s2">&quot;log_scalar&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">TYPE_DESCR</span> <span class="o">=</span> <span class="p">{</span><span class="nb">float</span><span class="p">:</span> <span class="s2">&quot;4.4f&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">TrainerHookBase</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An abstract hooking class for torchrl Trainer class.&quot;&quot;&quot;</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>


<div class="viewcode-block" id="Trainer"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.Trainer.html#torchrl.trainers.Trainer">[docs]</a><span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A generic Trainer class.</span>

<span class="sd">    A trainer is responsible for collecting data and training the model.</span>
<span class="sd">    To keep the class as versatile as possible, Trainer does not construct any</span>
<span class="sd">    of its specific operations: they all must be hooked at specific points in</span>
<span class="sd">    the training loop.</span>

<span class="sd">    To build a Trainer, one needs an iterable data source (a :obj:`collector`), a</span>
<span class="sd">    loss module and an optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        collector (Sequence[TensorDictBase]): An iterable returning batches of</span>
<span class="sd">            data in a TensorDict form of shape [batch x time steps].</span>
<span class="sd">        total_frames (int): Total number of frames to be collected during</span>
<span class="sd">            training.</span>
<span class="sd">        loss_module (LossModule): A module that reads TensorDict batches</span>
<span class="sd">            (possibly sampled from a replay buffer) and return a loss</span>
<span class="sd">            TensorDict where every key points to a different loss component.</span>
<span class="sd">        optimizer (optim.Optimizer): An optimizer that trains the parameters</span>
<span class="sd">            of the model.</span>
<span class="sd">        logger (Logger, optional): a Logger that will handle the logging.</span>
<span class="sd">        optim_steps_per_batch (int, optional): number of optimization steps</span>
<span class="sd">            per collection of data. An trainer works as follows: a main loop</span>
<span class="sd">            collects batches of data (epoch loop), and a sub-loop (training</span>
<span class="sd">            loop) performs model updates in between two collections of data.</span>
<span class="sd">            Default is 500</span>
<span class="sd">        clip_grad_norm (bool, optional): If True, the gradients will be clipped</span>
<span class="sd">            based on the total norm of the model parameters. If False,</span>
<span class="sd">            all the partial derivatives will be clamped to</span>
<span class="sd">            (-clip_norm, clip_norm). Default is :obj:`True`.</span>
<span class="sd">        clip_norm (Number, optional): value to be used for clipping gradients.</span>
<span class="sd">            Default is 100.0.</span>
<span class="sd">        progress_bar (bool, optional): If True, a progress bar will be</span>
<span class="sd">            displayed using tqdm. If tqdm is not installed, this option</span>
<span class="sd">            won&#39;t have any effect. Default is :obj:`True`</span>
<span class="sd">        seed (int, optional): Seed to be used for the collector, pytorch and</span>
<span class="sd">            numpy. Default is 42.</span>
<span class="sd">        save_trainer_interval (int, optional): How often the trainer should be</span>
<span class="sd">            saved to disk. Default is 10000.</span>
<span class="sd">        save_trainer_file (path, optional): path where to save the trainer.</span>
<span class="sd">            Default is None (no saving)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># trackers</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_optim_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_collected_frames</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_last_log</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_last_save</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_log_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_app_state</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">collector</span><span class="p">:</span> <span class="n">_DataCollector</span><span class="p">,</span>
        <span class="n">total_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">frame_skip</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">loss_module</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">LossModule</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">TensorDictBase</span><span class="p">],</span> <span class="n">TensorDictBase</span><span class="p">]],</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logger</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Logger</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim_steps_per_batch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
        <span class="n">clip_grad_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">clip_norm</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">100.0</span><span class="p">,</span>
        <span class="n">progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span>
        <span class="n">save_trainer_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">save_trainer_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="c1"># objects</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span> <span class="o">=</span> <span class="n">frame_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span> <span class="o">=</span> <span class="n">collector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span> <span class="o">=</span> <span class="n">loss_module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logger</span>

        <span class="c1"># seeding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_seed</span><span class="p">()</span>

        <span class="c1"># constants</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optim_steps_per_batch</span> <span class="o">=</span> <span class="n">optim_steps_per_batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span> <span class="o">=</span> <span class="n">total_frames</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_norm</span> <span class="o">=</span> <span class="n">clip_grad_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_norm</span> <span class="o">=</span> <span class="n">clip_norm</span>
        <span class="k">if</span> <span class="n">progress_bar</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_has_tqdm</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;tqdm library not found. &quot;</span>
                <span class="s2">&quot;Consider installing tqdm to use the Trainer progress bar.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span> <span class="o">=</span> <span class="n">progress_bar</span> <span class="ow">and</span> <span class="n">_has_tqdm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_interval</span> <span class="o">=</span> <span class="n">save_trainer_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span> <span class="o">=</span> <span class="n">save_trainer_file</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_log_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">[])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_process_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_log_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_steps_log_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_log_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_optim_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_loss_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_process_optim_batch_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_ops</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optimizer_hook</span> <span class="o">=</span> <span class="n">OptimizerHook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="n">optimizer_hook</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">register_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2"> is already registered, choose a different name.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>

    <span class="k">def</span> <span class="nf">_get_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torchsnapshot&quot;</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">StateDict</span><span class="p">(</span>
                <span class="n">collected_frames</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span><span class="p">,</span>
                <span class="n">_last_log</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span><span class="p">,</span>
                <span class="n">_last_save</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span><span class="p">,</span>
                <span class="n">_optim_count</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span>
                <span class="n">collected_frames</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span><span class="p">,</span>
                <span class="n">_last_log</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span><span class="p">,</span>
                <span class="n">_last_save</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span><span class="p">,</span>
                <span class="n">_optim_count</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">app_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;state&quot;</span><span class="p">:</span> <span class="n">StateDict</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_state</span><span class="p">()),</span>
            <span class="s2">&quot;collector&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="p">,</span>
            <span class="s2">&quot;loss_module&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="p">,</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">item</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_state</span><span class="p">()</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span>
            <span class="n">collector</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="n">loss_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model_state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;loss_module&quot;</span><span class="p">]</span>
        <span class="n">collector_state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;collector&quot;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">collector_state_dict</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">item</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="s2">&quot;collected_frames&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="s2">&quot;_last_log&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="s2">&quot;_last_save&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="s2">&quot;_optim_count&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_save_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torchsnapshot&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_ts</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                    <span class="s2">&quot;torchsnapshot not found. Consider installing torchsnapshot or &quot;</span>
                    <span class="s2">&quot;using the torch checkpointing backend (`CKPT_BACKEND=torch`)&quot;</span>
                <span class="p">)</span>
            <span class="n">Snapshot</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">app_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">app_state</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;CKPT_BACKEND should be one of </span><span class="si">{</span><span class="n">_CKPT_BACKEND</span><span class="o">.</span><span class="n">backends</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">_CKPT_BACKEND</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">save_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">force_save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_save</span> <span class="o">=</span> <span class="n">force_save</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_interval</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span>
                <span class="n">_save</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">_save</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_trainer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">load_from_file</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Trainer</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torchsnapshot&quot;</span><span class="p">:</span>
            <span class="n">snapshot</span> <span class="o">=</span> <span class="n">Snapshot</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>
            <span class="n">snapshot</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">app_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">app_state</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
            <span class="n">loaded_dict</span><span class="p">:</span> <span class="n">OrderedDict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">loaded_dict</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">static_seed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">collector</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_DataCollector</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collector</span>

    <span class="nd">@collector</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">collector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">collector</span><span class="p">:</span> <span class="n">_DataCollector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collector</span> <span class="o">=</span> <span class="n">collector</span>

    <span class="k">def</span> <span class="nf">register_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dest</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;batch_process&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">TensorDictBase</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_batch_process_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;pre_optim_steps&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_optim_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;process_optim_batch&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">TensorDictBase</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_process_optim_batch_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_loss&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">TensorDictBase</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_loss_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="n">TensorDictBase</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_steps&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_optim&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;pre_steps_log&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_steps_log_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_steps_log&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_log_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_optim_log&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_log_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The hook collection </span><span class="si">{</span><span class="n">dest</span><span class="si">}</span><span class="s2"> is not recognised. Choose from:&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(batch_process, pre_steps, pre_step, post_loss, post_steps, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;post_steps_log, post_optim_log)&quot;</span>
            <span class="p">)</span>

    <span class="c1"># Process batch</span>
    <span class="k">def</span> <span class="nf">_process_batch_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_process_ops</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">_post_steps_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_ops</span><span class="p">:</span>
            <span class="n">op</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_post_optim_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_log_ops</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span><span class="o">**</span><span class="n">result</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_pre_optim_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_optim_ops</span><span class="p">:</span>
            <span class="n">op</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_optim_batch_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_optim_batch_ops</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">_post_loss_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_loss_ops</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">_optimizer_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_ops</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_norm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_norm</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">batch</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_post_optim_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_ops</span><span class="p">:</span>
            <span class="n">op</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_pre_steps_log_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_steps_log_ops</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span><span class="o">**</span><span class="n">result</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_post_steps_log_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_log_ops</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span><span class="o">**</span><span class="n">result</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_batch_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_steps_log_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">current_frames</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">numel</span><span class="p">()))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">+=</span> <span class="n">current_frames</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">init_random_frames</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optim_steps</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_hook</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_log_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">current_frames</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_pbar_description</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer</span><span class="p">(</span><span class="n">force_save</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">shutdown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;shutting down collector&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">optim_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">average_losses</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_optim_hook</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim_steps_per_batch</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">sub_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_optim_batch_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">losses_td</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_loss_hook</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span>

            <span class="n">losses_detached</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_hook</span><span class="p">(</span><span class="n">losses_td</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_hook</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_log</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">average_losses</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">average_losses</span><span class="p">:</span> <span class="n">TensorDictBase</span> <span class="o">=</span> <span class="n">losses_detached</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">losses_detached</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">val</span> <span class="o">=</span> <span class="n">average_losses</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
                    <span class="n">average_losses</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="o">*</span> <span class="n">j</span> <span class="o">/</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">item</span> <span class="o">/</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">del</span> <span class="n">sub_batch</span><span class="p">,</span> <span class="n">losses_td</span><span class="p">,</span> <span class="n">losses_detached</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim_steps_per_batch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span>
                <span class="n">optim_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span><span class="p">,</span>
                <span class="o">**</span><span class="n">average_losses</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_pbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">collected_frames</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span><span class="n">collected_frames</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_interval</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">collected_frames</span>
                <span class="n">_log</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_log</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">LOGGER_METHODS</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="s2">&quot;log_scalar&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">_log</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">,</span> <span class="n">method</span><span class="p">)(</span><span class="n">key</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">collected_frames</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;log_scalar&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span> <span class="ow">and</span> <span class="n">log_pbar</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span>

    <span class="k">def</span> <span class="nf">_pbar_description</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
                <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="w"> </span><span class="si">:{</span><span class="n">TYPE_DESCR</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span><span class="p">[</span><span class="n">key</span><span class="p">]),</span><span class="w"> </span><span class="s1">&#39;4.4f&#39;</span><span class="p">)</span><span class="si">}}</span><span class="s2">&quot;</span>
                        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">loss_str</span> <span class="o">=</span> <span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loss=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">collector_str</span> <span class="o">=</span> <span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;collector=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">optimizer_str</span> <span class="o">=</span> <span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;optimizer=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;logger=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>

        <span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">loss_str</span><span class="p">,</span>
                <span class="n">collector_str</span><span class="p">,</span>
                <span class="n">optimizer_str</span><span class="p">,</span>
                <span class="n">logger</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">string</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Trainer(</span><span class="se">\n</span><span class="si">{</span><span class="n">string</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="k">return</span> <span class="n">string</span></div>


<span class="k">def</span> <span class="nf">_get_list_state_dict</span><span class="p">(</span><span class="n">hook_list</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="n">hook_list</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s2">&quot;state_dict&quot;</span><span class="p">):</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">item</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">kwargs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_load_list_state_dict</span><span class="p">(</span><span class="n">list_state_dict</span><span class="p">,</span> <span class="n">hook_list</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">state_dict_item</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">),</span> <span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">_</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span><span class="n">list_state_dict</span><span class="p">,</span> <span class="n">hook_list</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">state_dict_item</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">item</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict_item</span><span class="p">)</span>
            <span class="n">hook_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>


<div class="viewcode-block" id="SelectKeys"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.SelectKeys.html#torchrl.trainers.SelectKeys">[docs]</a><span class="k">class</span> <span class="nc">SelectKeys</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Selects keys in a TensorDict batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        keys (iterable of strings): keys to be selected in the tensordict.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; trainer = make_trainer()</span>
<span class="sd">        &gt;&gt;&gt; key1 = &quot;first key&quot;</span>
<span class="sd">        &gt;&gt;&gt; key2 = &quot;second key&quot;</span>
<span class="sd">        &gt;&gt;&gt; td = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         key1: torch.randn(3),</span>
<span class="sd">        ...         key2: torch.randn(3),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     [],</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;batch_process&quot;, SelectKeys([key1]))</span>
<span class="sd">        &gt;&gt;&gt; td_out = trainer._process_batch_hook(td)</span>
<span class="sd">        &gt;&gt;&gt; assert key1 in td_out.keys()</span>
<span class="sd">        &gt;&gt;&gt; assert key2 not in td_out.keys()</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keys</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Expected keys to be an iterable of str, got str instead&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;select_keys&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;batch_process&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ReplayBufferTrainer</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Replay buffer hook provider.</span>

<span class="sd">    Args:</span>
<span class="sd">        replay_buffer (TensorDictReplayBuffer): replay buffer to be used.</span>
<span class="sd">        batch_size (int): batch size when sampling data from the</span>
<span class="sd">            latest collection or from the replay buffer.</span>
<span class="sd">        memmap (bool, optional): if True, a memmap tensordict is created.</span>
<span class="sd">            Default is False.</span>
<span class="sd">        device (device, optional): device where the samples must be placed.</span>
<span class="sd">            Default is cpu.</span>
<span class="sd">        flatten_tensordicts (bool, optional): if True, the tensordicts will be</span>
<span class="sd">            flattened (or equivalently masked with the valid mask obtained from</span>
<span class="sd">            the collector) before being passed to the replay buffer. Otherwise,</span>
<span class="sd">            no transform will be achieved other than padding (see :obj:`max_dims` arg below).</span>
<span class="sd">            Defaults to True</span>
<span class="sd">        max_dims (sequence of int, optional): if :obj:`flatten_tensordicts` is set to False,</span>
<span class="sd">            this will be a list of the length of the batch_size of the provided</span>
<span class="sd">            tensordicts that represent the maximum size of each. If provided,</span>
<span class="sd">            this list of sizes will be used to pad the tensordict and make their shape</span>
<span class="sd">            match before they are passed to the replay buffer. If there is no</span>
<span class="sd">            maximum value, a -1 value should be provided.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; rb_trainer = ReplayBufferTrainer(replay_buffer=replay_buffer, batch_size=N)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;batch_process&quot;, rb_trainer.extend)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;process_optim_batch&quot;, rb_trainer.sample)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;post_loss&quot;, rb_trainer.update_priority)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">replay_buffer</span><span class="p">:</span> <span class="n">TensorDictReplayBuffer</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">memmap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">DEVICE_TYPING</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">flatten_tensordicts</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">max_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">replay_buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memmap</span> <span class="o">=</span> <span class="n">memmap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten_tensordicts</span> <span class="o">=</span> <span class="n">flatten_tensordicts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_dims</span> <span class="o">=</span> <span class="n">max_dims</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten_tensordicts</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;mask&quot;</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">)]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">pads</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()):</span>
                    <span class="n">pad_value</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="mi">0</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_dims</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span>
                        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_dims</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">batch</span><span class="o">.</span><span class="n">batch_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="n">pads</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_value</span><span class="p">]</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">pads</span><span class="p">)</span>
        <span class="c1"># reward_training = batch.get(&quot;reward&quot;).mean().item()</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">memmap</span><span class="p">:</span>
            <span class="c1"># We can already place the tensords on the device if they&#39;re memmap,</span>
            <span class="c1"># as this is a lazy op</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">memmap_</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sample</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">update_tensordict_priority</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;replay_buffer&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;replay_buffer&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;replay_buffer&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;batch_process&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">extend</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;process_optim_batch&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;post_loss&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_priority</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>


<div class="viewcode-block" id="OptimizerHook"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.OptimizerHook.html#torchrl.trainers.OptimizerHook">[docs]</a><span class="k">class</span> <span class="nc">OptimizerHook</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add an optimizer for one or more loss components.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (optim.Optimizer): An optimizer to apply to the loss_components.</span>
<span class="sd">        loss_components (Sequence[str], optional): The keys in the loss TensorDict</span>
<span class="sd">            for which the optimizer should be appled to the respective values.</span>
<span class="sd">            If omitted, the optimizer is applied to all components with the</span>
<span class="sd">            names starting with `loss_`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; optimizer_hook = OptimizerHook(optimizer, [&quot;loss_actor&quot;])</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;optimizer&quot;, optimizer_hook)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">loss_components</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">loss_components</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">loss_components</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;loss_components list cannot be empty. &quot;</span>
                <span class="s2">&quot;Set to None to act on all components of the loss.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span> <span class="o">=</span> <span class="n">loss_components</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_grad_clip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clip_grad_norm</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">params</span> <span class="o">+=</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">clip_grad_norm</span><span class="p">:</span>
            <span class="n">gn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gn</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_value_</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">gn</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">losses_td</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span>
        <span class="n">clip_grad_norm</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">clip_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="n">loss_components</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">losses_td</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">losses_td</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss_components</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">grad_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_clip</span><span class="p">(</span><span class="n">clip_grad_norm</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">)</span>
        <span class="n">losses_td</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;grad_norm_</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">grad_norm</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">losses_td</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;optimizer&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="ClearCudaCache"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.ClearCudaCache.html#torchrl.trainers.ClearCudaCache">[docs]</a><span class="k">class</span> <span class="nc">ClearCudaCache</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Clears cuda cache at a given interval.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; clear_cuda = ClearCudaCache(100)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;pre_optim_steps&quot;, clear_cuda)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interval</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">interval</span> <span class="o">=</span> <span class="n">interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span></div>


<div class="viewcode-block" id="LogReward"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.LogReward.html#torchrl.trainers.LogReward">[docs]</a><span class="k">class</span> <span class="nc">LogReward</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reward logger hook.</span>

<span class="sd">    Args:</span>
<span class="sd">        logname (str, optional): name of the rewards to be logged. Default is :obj:`&quot;r_training&quot;`.</span>
<span class="sd">        log_pbar (bool, optional): if True, the reward value will be logged on</span>
<span class="sd">            the progression bar. Default is :obj:`False`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; log_reward = LogReward(&quot;reward&quot;)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;pre_steps_log&quot;, log_reward)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logname</span><span class="o">=</span><span class="s2">&quot;r_training&quot;</span><span class="p">,</span> <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logname</span> <span class="o">=</span> <span class="n">logname</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span> <span class="o">=</span> <span class="n">log_pbar</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;mask&quot;</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logname</span><span class="p">:</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">)[</span><span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">)]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s2">&quot;log_pbar&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logname</span><span class="p">:</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s2">&quot;log_pbar&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;log_reward&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;pre_steps_log&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="RewardNormalizer"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.RewardNormalizer.html#torchrl.trainers.RewardNormalizer">[docs]</a><span class="k">class</span> <span class="nc">RewardNormalizer</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reward normalizer hook.</span>

<span class="sd">    Args:</span>
<span class="sd">        decay (float, optional): exponential moving average decay parameter.</span>
<span class="sd">            Default is 0.999</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; reward_normalizer = RewardNormalizer()</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;batch_process&quot;, reward_normalizer.update_reward_stats)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;process_optim_batch&quot;, reward_normalizer.normalize_reward)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_has_been_called</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_has_been_called</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;decay&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">update_reward_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;mask&quot;</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span><span class="p">[</span><span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">)]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_has_been_called</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_has_been_called</span><span class="p">:</span>
            <span class="c1"># We&#39;d like to check that rewards are normalized. Problem is that the trainer can collect data without calling steps...</span>
            <span class="c1"># raise RuntimeError(</span>
            <span class="c1">#     &quot;There have been two consecutive calls to update_reward_stats without a call to normalize_reward. &quot;</span>
            <span class="c1">#     &quot;Check that normalize_reward has been registered in the trainer.&quot;</span>
            <span class="c1"># )</span>
            <span class="k">pass</span>
        <span class="n">decay</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decay&quot;</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>
        <span class="nb">sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">reward</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">ssq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;ssq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ssq&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">reward</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;count&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">reward</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span> <span class="o">/</span> <span class="n">count</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;var&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">ssq</span> <span class="o">-</span> <span class="nb">sum</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;var&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">sum</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;std&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_has_been_called</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">normalize_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="n">tensordict</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">to_tensordict</span><span class="p">()</span>  <span class="c1"># make sure it is not a SubTensorDict</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">reward</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">reward</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;std&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">reward</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">]</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;std&quot;</span><span class="p">]</span>

        <span class="n">tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">reward</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_has_been_called</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">tensordict</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;_reward_stats&quot;</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">),</span>
            <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
            <span class="s2">&quot;_normalize_has_been_called&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_has_been_called</span><span class="p">,</span>
            <span class="s2">&quot;_update_has_been_called&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_has_been_called</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reward_normalizer&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;batch_process&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_reward_stats</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;process_optim_batch&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_reward</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">mask_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Batch masking hook.</span>

<span class="sd">    If a tensordict contained padded trajectories but only single events are</span>
<span class="sd">    needed, this hook can be used to select the valid events from the original</span>
<span class="sd">    tensordict.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch:</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; trainer = mocking_trainer()</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;batch_process&quot;, mask_batch)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;mask&quot;</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">batch</span>


<div class="viewcode-block" id="BatchSubSampler"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.BatchSubSampler.html#torchrl.trainers.BatchSubSampler">[docs]</a><span class="k">class</span> <span class="nc">BatchSubSampler</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Data subsampler for online RL algorithms.</span>

<span class="sd">    This class subsamples a part of a whole batch of data just collected from the</span>
<span class="sd">    environment.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size (int): sub-batch size to collect. The provided batch size</span>
<span class="sd">            must be equal to the total number of items in the output tensordict,</span>
<span class="sd">            which will have size [batch_size // sub_traj_len, sub_traj_len].</span>
<span class="sd">        sub_traj_len (int, optional): length of the trajectories that</span>
<span class="sd">            sub-samples must have in online settings. Default is -1 (i.e.</span>
<span class="sd">            takes the full length of the trajectory)</span>
<span class="sd">        min_sub_traj_len (int, optional): minimum value of :obj:`sub_traj_len`, in</span>
<span class="sd">            case some elements of the batch contain few steps.</span>
<span class="sd">            Default is -1 (i.e. no minimum value)</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; td = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         key1: torch.stack([torch.arange(0, 10), torch.arange(10, 20)], 0),</span>
<span class="sd">        ...         key2: torch.stack([torch.arange(0, 10), torch.arange(10, 20)], 0),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     [2, 10],</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(</span>
<span class="sd">        ...     &quot;process_optim_batch&quot;,</span>
<span class="sd">        ...     BatchSubSampler(batch_size=batch_size, sub_traj_len=sub_traj_len),</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; td_out = trainer._process_optim_batch_hook(td)</span>
<span class="sd">        &gt;&gt;&gt; assert td_out.shape == torch.Size([batch_size // sub_traj_len, sub_traj_len])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">min_sub_traj_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub_traj_len</span> <span class="o">=</span> <span class="n">sub_traj_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_sub_traj_len</span> <span class="o">=</span> <span class="n">min_sub_traj_len</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sub-sampled part of a batch randomly.</span>

<span class="sd">        If the batch has one dimension, a random subsample of length</span>
<span class="sd">        self.bach_size will be returned. If the batch has two or more</span>
<span class="sd">        dimensions, it is assumed that the first dimension represents the</span>
<span class="sd">        batch, and the second the time. If so, the resulting subsample will</span>
<span class="sd">        contain consecutive samples across time.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">batch</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">batch</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">]]</span>

        <span class="n">sub_traj_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_traj_len</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_traj_len</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="s2">&quot;mask&quot;</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="c1"># if a valid mask is present, it&#39;s important to sample only</span>
            <span class="c1"># valid steps</span>
            <span class="n">traj_len</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">sub_traj_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">min_sub_traj_len</span><span class="p">,</span>
                <span class="nb">min</span><span class="p">(</span><span class="n">sub_traj_len</span><span class="p">,</span> <span class="n">traj_len</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">traj_len</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
                <span class="o">*</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="n">len_mask</span> <span class="o">=</span> <span class="n">traj_len</span> <span class="o">&gt;=</span> <span class="n">sub_traj_len</span>
        <span class="n">valid_trajectories</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="n">len_mask</span><span class="p">]</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">sub_traj_len</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Resulting batch size is zero. The batch size given to &quot;</span>
                <span class="s2">&quot;BatchSubSampler must be equal to the total number of elements &quot;</span>
                <span class="s2">&quot;that will result in a batch provided to the loss function.&quot;</span>
            <span class="p">)</span>
        <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">valid_trajectories</span><span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span>
                <span class="n">valid_trajectories</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="n">sub_traj_len</span> <span class="o">&lt;</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">_traj_len</span> <span class="o">=</span> <span class="n">traj_len</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span>
            <span class="n">seq_idx</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">_traj_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
                <span class="o">*</span> <span class="p">(</span><span class="n">_traj_len</span> <span class="o">-</span> <span class="n">sub_traj_len</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
            <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">seq_idx</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">sub_traj_len</span> <span class="o">==</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;sub_traj_len=</span><span class="si">{</span><span class="n">sub_traj_len</span><span class="si">}</span><span class="s2"> is not allowed. Accepted values &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;are in the range [1, </span><span class="si">{</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">].&quot;</span>
            <span class="p">)</span>

        <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">seq_idx</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sub_traj_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">seq_idx</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">td</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">td</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">index</span><span class="o">=</span><span class="n">expand_right</span><span class="p">(</span><span class="n">seq_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">,</span> <span class="o">*</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])),</span>
            <span class="p">),</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;mask&quot;</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">td</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Sampled invalid steps&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">td</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batch_subsampler&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span>
            <span class="s2">&quot;process_optim_batch&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="Recorder"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder">[docs]</a><span class="k">class</span> <span class="nc">Recorder</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Recorder hook for Trainer.</span>

<span class="sd">    Args:</span>
<span class="sd">        record_interval (int): total number of optimisation steps</span>
<span class="sd">            between two calls to the recorder for testing.</span>
<span class="sd">        record_frames (int): number of frames to be recorded during</span>
<span class="sd">            testing.</span>
<span class="sd">        frame_skip (int): frame_skip used in the environment. It is</span>
<span class="sd">            important to let the trainer know the number of frames skipped at</span>
<span class="sd">            each iteration, otherwise the frame count can be underestimated.</span>
<span class="sd">            For logging, this parameter is important to normalize the reward.</span>
<span class="sd">            Finally, to compare different runs with different frame_skip,</span>
<span class="sd">            one must normalize the frame count and rewards. Default is 1.</span>
<span class="sd">        policy_exploration (ProbabilisticTDModule): a policy</span>
<span class="sd">            instance used for</span>

<span class="sd">            (1) updating the exploration noise schedule;</span>

<span class="sd">            (2) testing the policy on the recorder.</span>

<span class="sd">            Given that this instance is supposed to both explore and render</span>
<span class="sd">            the performance of the policy, it should be possible to turn off</span>
<span class="sd">            the explorative behaviour by calling the</span>
<span class="sd">            `set_exploration_mode(&#39;mode&#39;)` context manager.</span>
<span class="sd">        recorder (EnvBase): An environment instance to be used</span>
<span class="sd">            for testing.</span>
<span class="sd">        exploration_mode (str, optional): exploration mode to use for the</span>
<span class="sd">            policy. By default, no exploration is used and the value used is</span>
<span class="sd">            &quot;mode&quot;. Set to &quot;random&quot; to enable exploration</span>
<span class="sd">        out_key (str, optional): reward key to set to the logger. Default is</span>
<span class="sd">            `&quot;reward_evaluation&quot;`.</span>
<span class="sd">        suffix (str, optional): suffix of the video to be recorded.</span>
<span class="sd">        log_pbar (bool, optional): if True, the reward value will be logged on</span>
<span class="sd">            the progression bar. Default is `False`.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">record_interval</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">record_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">frame_skip</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">policy_exploration</span><span class="p">:</span> <span class="n">SafeModule</span><span class="p">,</span>
        <span class="n">recorder</span><span class="p">:</span> <span class="n">EnvBase</span><span class="p">,</span>
        <span class="n">exploration_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span><span class="p">,</span>
        <span class="n">log_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">out_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">suffix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span> <span class="o">=</span> <span class="n">policy_exploration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recorder</span> <span class="o">=</span> <span class="n">recorder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_frames</span> <span class="o">=</span> <span class="n">record_frames</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span> <span class="o">=</span> <span class="n">frame_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_interval</span> <span class="o">=</span> <span class="n">record_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exploration_mode</span> <span class="o">=</span> <span class="n">exploration_mode</span>
        <span class="k">if</span> <span class="n">log_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">log_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">out_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">out_keys</span> <span class="o">=</span> <span class="n">KeyDependentDefaultDict</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
            <span class="n">out_keys</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;r_evaluation&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_keys</span> <span class="o">=</span> <span class="n">log_keys</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span> <span class="o">=</span> <span class="n">out_keys</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">suffix</span> <span class="o">=</span> <span class="n">suffix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span> <span class="o">=</span> <span class="n">log_pbar</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">set_exploration_mode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">exploration_mode</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="n">td_record</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">rollout</span><span class="p">(</span>
                    <span class="n">policy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="p">,</span>
                    <span class="n">max_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">record_frames</span><span class="p">,</span>
                    <span class="n">auto_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">auto_cast_to_device</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">suffix</span><span class="p">)</span>

                <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_keys</span><span class="p">:</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">td_record</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span>
                        <span class="n">mean_value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span>
                        <span class="n">total_value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                        <span class="n">out</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span> <span class="o">=</span> <span class="n">mean_value</span>
                        <span class="n">out</span><span class="p">[</span><span class="s2">&quot;total_&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span> <span class="o">=</span> <span class="n">total_value</span>
                        <span class="k">continue</span>
                    <span class="n">out</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">out</span><span class="p">[</span><span class="s2">&quot;log_pbar&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;_count&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span><span class="p">,</span>
            <span class="s2">&quot;recorder_state_dict&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_count&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;recorder_state_dict&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;recorder&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span>
            <span class="s2">&quot;post_steps_log&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="UpdateWeights"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.UpdateWeights.html#torchrl.trainers.UpdateWeights">[docs]</a><span class="k">class</span> <span class="nc">UpdateWeights</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A collector weights update hook class.</span>

<span class="sd">    This hook must be used whenever the collector policy weights sit on a</span>
<span class="sd">    different device than the policy weights being trained by the Trainer.</span>
<span class="sd">    In that case, those weights must be synced across devices at regular</span>
<span class="sd">    intervals. If the devices match, this will result in a no-op.</span>

<span class="sd">    Args:</span>
<span class="sd">        collector (_DataCollector): A data collector where the policy weights</span>
<span class="sd">            must be synced.</span>
<span class="sd">        update_weights_interval (int): Interval (in terms of number of batches</span>
<span class="sd">            collected) where the sync must take place.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; update_weights = UpdateWeights(trainer.collector, T)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;post_steps&quot;, update_weights)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">collector</span><span class="p">:</span> <span class="n">_DataCollector</span><span class="p">,</span> <span class="n">update_weights_interval</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span> <span class="o">=</span> <span class="n">collector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_weights_interval</span> <span class="o">=</span> <span class="n">update_weights_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_weights_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">update_policy_weights_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;update_weights&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span>
            <span class="s2">&quot;post_steps&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span></div>


<div class="viewcode-block" id="CountFramesLog"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.CountFramesLog.html#torchrl.trainers.CountFramesLog">[docs]</a><span class="k">class</span> <span class="nc">CountFramesLog</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A frame counter hook.</span>

<span class="sd">    Args:</span>
<span class="sd">        frame_skip (int): frame skip of the environment. This argument is</span>
<span class="sd">            important to keep track of the total number of frames, not the</span>
<span class="sd">            apparent one.</span>
<span class="sd">        log_pbar (bool, optional): if True, the reward value will be logged on</span>
<span class="sd">            the progression bar. Default is `False`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; count_frames = CountFramesLog(frame_skip=frame_skip)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;pre_steps_log&quot;, count_frames)</span>


<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">frame_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_skip</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span> <span class="o">=</span> <span class="n">frame_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span> <span class="o">=</span> <span class="n">log_pbar</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;mask&quot;</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">current_frames</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mask&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_frames</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_count</span> <span class="o">+=</span> <span class="n">current_frames</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;n_frames&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_count</span><span class="p">,</span> <span class="s2">&quot;log_pbar&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;count_frames_log&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span>
            <span class="s2">&quot;pre_steps_log&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;frame_count&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_count</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_count</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;frame_count&quot;</span><span class="p">]</span></div>


<span class="k">def</span> <span class="nf">_check_input_output_typehint</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Type</span> <span class="o">|</span> <span class="n">List</span><span class="p">[</span><span class="n">Type</span><span class="p">],</span> <span class="n">output</span><span class="p">:</span> <span class="n">Type</span>
<span class="p">):</span>
    <span class="c1"># Placeholder for a function that checks the types input / output against expectations</span>
    <span class="k">return</span>


<span class="k">def</span> <span class="nf">flatten_dict</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Flattens a dictionary with sub-dictionaries accessed through point-separated (:obj:`&quot;var1.var2&quot;`) fields.&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_key</span><span class="p">,</span> <span class="n">_item</span> <span class="ow">in</span> <span class="n">item</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">out</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">key</span><span class="p">,</span> <span class="n">_key</span><span class="p">])]</span> <span class="o">=</span> <span class="n">_item</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>

        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>