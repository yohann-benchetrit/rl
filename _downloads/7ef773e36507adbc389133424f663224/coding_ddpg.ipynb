{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Coding DDPG using TorchRL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial will guide you through the steps to code DDPG from scratch.\nDDPG ([Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971))\nis a simple continuous control algorithm. It essentially consists in\nlearning a parametric value function for an action-observation pair, and\nthen learning a policy that outputs actions that maximise this value\nfunction given a certain observation.\n\nIn this tutorial, you will learn:\n\n- how to build an environment in TorchRL, including transforms\n  (e.g. data normalization) and parallel execution;\n- how to design a policy and value network;\n- how to collect data from your environment efficiently and store them\n  in a replay buffer;\n- how to store trajectories (and not transitions) in your replay buffer);\n- and finally how to evaluate your model.\n\nThis tutorial assumes the reader is familiar with some of TorchRL primitives,\nsuch as ``TensorDict`` and ``TensorDictModules``, although it should be\nsufficiently transparent to be understood without a deep understanding of\nthese classes.\n\nWe do not aim at giving a SOTA implementation of the algorithm, but rather\nto provide a high-level illustration of TorchRL features in the context of\nthis algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Make all the necessary imports for training\n\n\nfrom copy import deepcopy\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.cuda\nimport tqdm\nfrom matplotlib import pyplot as plt\nfrom tensordict.nn import TensorDictModule\nfrom torch import nn, optim\nfrom torchrl.collectors import MultiaSyncDataCollector\nfrom torchrl.data import CompositeSpec, TensorDictReplayBuffer\nfrom torchrl.data.postprocs import MultiStep\nfrom torchrl.data.replay_buffers.samplers import PrioritizedSampler, RandomSampler\nfrom torchrl.data.replay_buffers.storages import LazyMemmapStorage\nfrom torchrl.envs import (\n    CatTensors,\n    DoubleToFloat,\n    EnvCreator,\n    ObservationNorm,\n    ParallelEnv,\n)\nfrom torchrl.envs.libs.dm_control import DMControlEnv\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import RewardScaling, TransformedEnv\nfrom torchrl.envs.utils import set_exploration_mode, step_mdp\nfrom torchrl.modules import (\n    MLP,\n    OrnsteinUhlenbeckProcessWrapper,\n    ProbabilisticActor,\n    ValueOperator,\n)\nfrom torchrl.modules.distributions.continuous import TanhDelta\nfrom torchrl.objectives.utils import hold_out_net\nfrom torchrl.trainers import Recorder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment\nLet us start by building the environment.\n\nFor this example, we will be using the cheetah task. The goal is to make\na half-cheetah run as fast as possible.\n\nIn TorchRL, one can create such a task by relying on dm_control or gym:\n\n  env = GymEnv(\"HalfCheetah-v4\")\n\nor\n\n  env = DMControlEnv(\"cheetah\", \"run\")\n\nWe only consider the state-based environment, but if one wishes to use a\npixel-based environment, this can be done via the keyword argument\n``from_pixels=True`` which is passed when calling ``GymEnv`` or\n``DMControlEnv``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_env():\n    \"\"\"\n    Create a base env\n    \"\"\"\n    global env_library\n    global env_name\n\n    if backend == \"dm_control\":\n        env_name = \"cheetah\"\n        env_task = \"run\"\n        env_args = (env_name, env_task)\n        env_library = DMControlEnv\n    elif backend == \"gym\":\n        env_name = \"HalfCheetah-v4\"\n        env_args = (env_name,)\n        env_library = GymEnv\n    else:\n        raise NotImplementedError\n\n    env_kwargs = {\n        \"device\": device,\n        \"frame_skip\": frame_skip,\n        \"from_pixels\": from_pixels,\n        \"pixels_only\": from_pixels,\n    }\n    env = env_library(*env_args, **env_kwargs)\n    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transforms\nNow that we have a base environment, we may want to modify its representation\nto make it more policy-friendly.\n\nIt is common in DDPG to rescale the reward using some heuristic value. We\nwill multiply the reward by 5 in this example.\n\nIf we are using dm_control, it is important also to transform the actions\nto double precision numbers as this is the dtype expected by the library.\n\nWe also leave the possibility to normalize the states: we will take care of\ncomputing the normalizing constants later on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_transformed_env(\n    env,\n    stats=None,\n):\n    \"\"\"\n    Apply transforms to the env (such as reward scaling and state normalization)\n    \"\"\"\n\n    env = TransformedEnv(env)\n\n    # we append transforms one by one, although we might as well create the transformed environment using the `env = TransformedEnv(base_env, transforms)` syntax.\n    env.append_transform(RewardScaling(loc=0.0, scale=reward_scaling))\n\n    double_to_float_list = []\n    double_to_float_inv_list = []\n    if env_library is DMControlEnv:\n        # DMControl requires double-precision\n        double_to_float_list += [\n            \"reward\",\n            \"action\",\n        ]\n        double_to_float_inv_list += [\"action\"]\n\n    # We concatenate all states into a single \"observation_vector\"\n    # even if there is a single tensor, it'll be renamed in \"observation_vector\".\n    # This facilitates the downstream operations as we know the name of the output tensor.\n    # In some environments (not half-cheetah), there may be more than one observation vector: in this case this code snippet will concatenate them all.\n    selected_keys = list(env.observation_spec.keys())\n    out_key = \"observation_vector\"\n    env.append_transform(CatTensors(in_keys=selected_keys, out_key=out_key))\n\n    #  we normalize the states\n    if stats is None:\n        _stats = {\"loc\": 0.0, \"scale\": 1.0}\n    else:\n        _stats = stats\n    env.append_transform(\n        ObservationNorm(**_stats, in_keys=[out_key], standard_normal=True)\n    )\n\n    double_to_float_list.append(out_key)\n    env.append_transform(\n        DoubleToFloat(\n            in_keys=double_to_float_list, in_keys_inv=double_to_float_inv_list\n        )\n    )\n\n    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parallel execution\nThe following helper function allows us to run environments in parallel.\nOne can choose between running each base env in a separate process and\nexecute the transform in the main process, or execute the transforms in\nparallel. To leverage the vectorization capabilities of PyTorch, we adopt\nthe first method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def parallel_env_constructor(\n    stats,\n    **env_kwargs,\n):\n    if env_per_collector == 1:\n        env_creator = EnvCreator(\n            lambda: make_transformed_env(make_env(), stats, **env_kwargs)\n        )\n        return env_creator\n\n    parallel_env = ParallelEnv(\n        num_workers=env_per_collector,\n        create_env_fn=EnvCreator(lambda: make_env()),\n        create_env_kwargs=None,\n        pin_memory=False,\n    )\n    env = make_transformed_env(parallel_env, stats, **env_kwargs)\n    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalization of the observations\nTo compute the normalizing statistics, we run an arbitrary number of random\nsteps in the environment and compute the mean and standard deviation of the\ncollected observations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_stats_random_rollout(proof_environment, key: Optional[str] = None):\n    print(\"computing state stats\")\n    n = 0\n    td_stats = []\n    while n < init_env_steps:\n        _td_stats = proof_environment.rollout(max_steps=init_env_steps)\n        n += _td_stats.numel()\n        _td_stats_select = _td_stats.to_tensordict().select(key).cpu()\n        if not len(list(_td_stats_select.keys())):\n            raise RuntimeError(\n                f\"key {key} not found in tensordict with keys {list(_td_stats.keys())}\"\n            )\n        td_stats.append(_td_stats_select)\n        del _td_stats, _td_stats_select\n    td_stats = torch.cat(td_stats, 0)\n\n    m = td_stats.get(key).mean(dim=0)\n    s = td_stats.get(key).std(dim=0)\n    m[s == 0] = 0.0\n    s[s == 0] = 1.0\n\n    print(\n        f\"stats computed for {td_stats.numel()} steps. Got: \\n\"\n        f\"loc = {m}, \\n\"\n        f\"scale: {s}\"\n    )\n    if not torch.isfinite(m).all():\n        raise RuntimeError(\"non-finite values found in mean\")\n    if not torch.isfinite(s).all():\n        raise RuntimeError(\"non-finite values found in sd\")\n    stats = {\"loc\": m, \"scale\": s}\n    return stats\n\n\ndef get_env_stats():\n    \"\"\"\n    Gets the stats of an environment\n    \"\"\"\n    proof_env = make_transformed_env(make_env(), None)\n    proof_env.set_seed(seed)\n    stats = get_stats_random_rollout(\n        proof_env,\n        key=\"observation_vector\",\n    )\n    # make sure proof_env is closed\n    proof_env.close()\n    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the model\nLet us now build the DDPG actor and QValue network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_ddpg_actor(\n    stats,\n    device=\"cpu\",\n):\n    proof_environment = make_transformed_env(make_env(), stats)\n\n    env_specs = proof_environment.specs\n    out_features = env_specs[\"action_spec\"].shape[0]\n\n    actor_net = MLP(\n        num_cells=[num_cells] * num_layers,\n        activation_class=nn.Tanh,\n        out_features=out_features,\n    )\n    in_keys = [\"observation_vector\"]\n    out_keys = [\"param\"]\n\n    actor_module = TensorDictModule(actor_net, in_keys=in_keys, out_keys=out_keys)\n\n    # We use a ProbabilisticActor to make sure that we map the network output\n    # to the right space using a TanhDelta distribution.\n    actor = ProbabilisticActor(\n        module=actor_module,\n        in_keys=[\"param\"],\n        spec=CompositeSpec(action=env_specs[\"action_spec\"]),\n        safe=True,\n        distribution_class=TanhDelta,\n        distribution_kwargs={\n            \"min\": env_specs[\"action_spec\"].space.minimum,\n            \"max\": env_specs[\"action_spec\"].space.maximum,\n        },\n    ).to(device)\n\n    q_net = MLP(\n        num_cells=[num_cells] * num_layers,\n        activation_class=nn.Tanh,\n        out_features=1,\n    )\n\n    in_keys = in_keys + [\"action\"]\n    qnet = ValueOperator(\n        in_keys=in_keys,\n        module=q_net,\n    ).to(device)\n\n    # init: since we have lazy layers, we should run the network\n    # once to initialize them\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        td = proof_environment.rollout(max_steps=4)\n        td = td.to(device)\n        actor(td)\n        qnet(td)\n\n    return actor, qnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator: building your recorder object\nAs the training data is obtained using some exploration strategy, the true\nperformance of our algorithm needs to be assessed in deterministic mode. We\ndo this using a dedicated class, ``Recorder``, which executes the policy in\nthe environment at a given frequency and returns some statistics obtained\nfrom these simulations. The following helper function builds this object:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_recorder(actor_model_explore, stats):\n    base_env = make_env()\n    recorder = make_transformed_env(base_env, stats)\n\n    recorder_obj = Recorder(\n        record_frames=1000,\n        frame_skip=frame_skip,\n        policy_exploration=actor_model_explore,\n        recorder=recorder,\n        exploration_mode=\"mean\",\n        record_interval=record_interval,\n    )\n    return recorder_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replay buffer\nReplay buffers come in two flavours: prioritized (where some error signal\nis used to give a higher likelihood of sampling to some items than others)\nand regular, circular experience replay.\n\nWe also provide a special storage, names LazyMemmapStorage, that will\nstore tensors on physical memory using a memory-mapped array. The following\nfunction takes care of creating the replay buffer with the desired\nhyperparameters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_replay_buffer(make_replay_buffer=3):\n    if prb:\n        sampler = PrioritizedSampler(\n            max_capacity=buffer_size,\n            alpha=0.7,\n            beta=0.5,\n        )\n    else:\n        sampler = RandomSampler()\n    replay_buffer = TensorDictReplayBuffer(\n        storage=LazyMemmapStorage(\n            buffer_size,\n            scratch_dir=buffer_scratch_dir,\n            device=device,\n        ),\n        sampler=sampler,\n        pin_memory=False,\n        prefetch=make_replay_buffer,\n    )\n    return replay_buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\nAfter having written all our helper functions, it is now time to set the\nexperiment hyperparameters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backend = \"gym\"  # or \"dm_control\"\nframe_skip = 2  # if this value is changed, the number of frames collected etc. need to be adjusted\nfrom_pixels = False\nreward_scaling = 5.0\n\n# execute on cuda if available\ndevice = (\n    torch.device(\"cpu\") if torch.cuda.device_count() == 0 else torch.device(\"cuda:0\")\n)\n\ninit_env_steps = 1000  # number of random steps used as for stats computation\nenv_per_collector = 2  # number of environments in each data collector\n\nenv_library = None  # overwritten because global in env maker\nenv_name = None  # overwritten because global in env maker\n\nexp_name = \"cheetah\"\nannealing_frames = (\n    1000000 // frame_skip\n)  # Number of frames before OU noise becomes null\nlr = 5e-4\nweight_decay = 0.0\ntotal_frames = 5000 // frame_skip\ninit_random_frames = 0\n# init_random_frames = 5000 // frame_skip   # Number of random frames used as warm-up\noptim_steps_per_batch = 32  # Number of iterations of the inner loop\nbatch_size = 128\nframes_per_batch = (\n    1000 // frame_skip\n)  # Number of frames returned by the collector at each iteration of the outer loop\ngamma = 0.99\ntau = 0.005  # Decay factor for the target network\nprb = True  # If True, a Prioritized replay buffer will be used\nbuffer_size = min(\n    total_frames, 1000000 // frame_skip\n)  # Number of frames stored in the buffer\nbuffer_scratch_dir = \"/tmp/\"\nn_steps_forward = 3\n\nrecord_interval = 10  # record every 10 batch collected\n\n# Network specs\nnum_cells = 64\nnum_layers = 2\n\nseed = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: for fast rendering of the tutorial ``total_frames`` hyperparameter\nwas set to a very low number. To get a reasonable performance, use a greater\nvalue e.g. 1000000.\n\n## Initialization\nTo initialize the experiment, we first acquire the observation statistics,\nthen build the networks, wrap them in an exploration wrapper (following the\nseminal DDPG paper, we used an Ornstein-Uhlenbeck process to add noise to the\nsampled actions).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nnp.random.seed(0)\n\n# get stats for normalization\nstats = get_env_stats()\n\n# Actor and qnet instantiation\nactor, qnet = make_ddpg_actor(\n    stats=stats,\n    device=device,\n)\nif device == torch.device(\"cpu\"):\n    actor.share_memory()\n# Target network\nqnet_target = deepcopy(qnet).requires_grad_(False)\n\n# Exploration wrappers:\nactor_model_explore = OrnsteinUhlenbeckProcessWrapper(\n    actor,\n    annealing_num_steps=annealing_frames,\n).to(device)\nif device == torch.device(\"cpu\"):\n    actor_model_explore.share_memory()\n\n# Environment setting:\ncreate_env_fn = parallel_env_constructor(\n    stats=stats,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data collector\nCreating the data collector is a crucial step in an RL experiment. TorchRL\nprovides a couple of classes to collect data in parallel. Here we will use\n``MultiaSyncDataCollector``, a data collector that will be executed in an\nasync manner (i.e. data will be collected while the policy is being optimized).\n\nThe parameters to specify are:\n\n- the list of environment creation functions,\n- the policy,\n- the total number of frames before the collector is considered empty,\n- the maximum number of frames per trajectory (useful for non-terminating\n  environments, like dm_control ones).\n\nOne should also pass:\n\n- the number of frames in each batch collected,\n- the number of random steps executed independently from the policy,\n- the devices used for policy execution, and\n- data transmission.\n\nThe ``MultiStep`` object passed as postproc makes it so that the rewards\nof the n upcoming steps are added (with some discount factor) and the next\nobservation is changed to be the n-step forward observation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Batch collector:\ncollector = MultiaSyncDataCollector(\n    create_env_fn=[create_env_fn, create_env_fn],\n    policy=actor_model_explore,\n    total_frames=total_frames,\n    max_frames_per_traj=1000,\n    frames_per_batch=frames_per_batch,\n    init_random_frames=init_random_frames,\n    reset_at_each_iter=False,\n    postproc=MultiStep(n_steps_max=n_steps_forward, gamma=gamma)\n    if n_steps_forward > 0\n    else None,\n    split_trajs=True,\n    devices=[device, device],  # device for execution\n    passing_devices=[device, device],  # device where data will be stored and passed\n    seed=None,\n    pin_memory=False,\n    update_at_each_batch=False,\n    exploration_mode=\"random\",\n)\ncollector.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now create the replay buffer as part of the initialization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Replay buffer:\nreplay_buffer = make_replay_buffer()\n\n# Trajectory recorder\nrecorder = make_recorder(actor_model_explore, stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we will use the Adam optimizer for the policy and value network,\nwith the same learning rate for both.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Optimizers\noptimizer_actor = optim.Adam(actor.parameters(), lr=lr, weight_decay=weight_decay)\noptimizer_qnet = optim.Adam(qnet.parameters(), lr=lr, weight_decay=weight_decay)\ntotal_collection_steps = total_frames // frames_per_batch\n\nscheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_actor, T_max=total_collection_steps\n)\nscheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_qnet, T_max=total_collection_steps\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time to train the policy!\nSome notes about the following cell:\n\n- ``hold_out_net`` is a TorchRL context manager that temporarily sets\n  ``requires_grad`` to False for a set of network parameters. This is used to\n  prevent ``backward`` to write gradients on parameters that need not to be\n  differentiated given the loss at hand.\n- The value network is designed using the ``ValueOperator`` TensorDictModule\n  subclass. This class will write a ``\"state_action_value\"`` if one of its\n  ``in_keys`` is named \"action\", otherwise it will assume that only the\n  state-value is returned and the output key will simply be ``\"state_value\"``.\n  In the case of DDPG, the value if of the state-action pair,\n  hence the first name is used.\n- The ``step_mdp`` helper function returns a new TensorDict that essentially\n  does the ``obs = next_obs`` step. In other words, it will return a new\n  tensordict where the values that are related to the next state (next\n  observations of various type) are selected and written as if they were\n  current. This makes it possible to pass this new tensordict to the policy or\n  value network.\n- When using prioritized replay buffer, a priority key is added to the\n  sampled tensordict (named ``\"td_error\"`` by default). Then, this\n  TensorDict will be fed back to the replay buffer using the ``update_priority``\n  method. Under the hood, this method will read the index present in the\n  TensorDict as well as the priority value, and update its list of priorities\n  at these indices.\n- TorchRL provides optimized versions of the loss functions (such as this one)\n  where one only needs to pass a sampled tensordict and obtains a dictionary\n  of losses and metadata in return (see ``torchrl.objectives`` for more\n  context). Here we write the full loss function in the optimization loop\n  for transparency. Similarly, the target network updates are written\n  explicitely but TorchRL provides a couple of dedicated classes for this\n  (see ``torchrl.objectives.SoftUpdate`` and ``torchrl.objectives.HardUpdate``).\n- After each collection of data, we call ``collector.update_policy_weights_()``,\n  which will update the policy network weights on the data collector. If the\n  code is executed on cpu or with a single cuda device, this part can be\n  ommited. If the collector is executed on another device, then its weights\n  must be synced with those on the main, training process and this method\n  should be incorporated in the training loop (ideally early in the loop in\n  async settings, and at the end of it in sync settings).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rewards = []\nrewards_eval = []\n\n# Main loop\nnorm_factor_training = (\n    sum(gamma**i for i in range(n_steps_forward)) if n_steps_forward else 1\n)\n\ncollected_frames = 0\npbar = tqdm.tqdm(total=total_frames)\nr0 = None\nfor i, tensordict in enumerate(collector):\n\n    # update weights of the inference policy\n    collector.update_policy_weights_()\n\n    if r0 is None:\n        r0 = tensordict[\"reward\"].mean().item()\n    pbar.update(tensordict.numel())\n\n    # extend the replay buffer with the new data\n    if \"mask\" in tensordict.keys():\n        # if multi-step, a mask is present to help filter padded values\n        current_frames = tensordict[\"mask\"].sum()\n        tensordict = tensordict[tensordict.get(\"mask\")]\n    else:\n        tensordict = tensordict.view(-1)\n        current_frames = tensordict.numel()\n    collected_frames += current_frames\n    replay_buffer.extend(tensordict.cpu())\n\n    # optimization steps\n    if collected_frames >= init_random_frames:\n        for _ in range(optim_steps_per_batch):\n            # sample from replay buffer\n            sampled_tensordict = replay_buffer.sample(batch_size).clone()\n\n            # compute loss for qnet and backprop\n            with hold_out_net(actor):\n                # get next state value\n                next_tensordict = step_mdp(sampled_tensordict)\n                qnet_target(actor(next_tensordict))\n                next_value = next_tensordict[\"state_action_value\"]\n                assert not next_value.requires_grad\n            value_est = (\n                sampled_tensordict[\"reward\"]\n                + gamma * (1 - sampled_tensordict[\"done\"].float()) * next_value\n            )\n            value = qnet(sampled_tensordict)[\"state_action_value\"]\n            value_loss = (value - value_est).pow(2).mean()\n            # we write the td_error in the sampled_tensordict for priority update\n            # because the indices of the samples is tracked in sampled_tensordict\n            # and the replay buffer will know which priorities to update.\n            sampled_tensordict[\"td_error\"] = (value - value_est).pow(2).detach()\n            value_loss.backward()\n\n            optimizer_qnet.step()\n            optimizer_qnet.zero_grad()\n\n            # compute loss for actor and backprop: the actor must maximise the state-action value, hence the loss is the neg value of this.\n            sampled_tensordict_actor = sampled_tensordict.select(*actor.in_keys)\n            with hold_out_net(qnet):\n                qnet(actor(sampled_tensordict_actor))\n            actor_loss = -sampled_tensordict_actor[\"state_action_value\"]\n            actor_loss.mean().backward()\n\n            optimizer_actor.step()\n            optimizer_actor.zero_grad()\n\n            # update qnet_target params\n            for (p_in, p_dest) in zip(qnet.parameters(), qnet_target.parameters()):\n                p_dest.data.copy_(tau * p_in.data + (1 - tau) * p_dest.data)\n            for (b_in, b_dest) in zip(qnet.buffers(), qnet_target.buffers()):\n                b_dest.data.copy_(tau * b_in.data + (1 - tau) * b_dest.data)\n\n            # update priority\n            if prb:\n                replay_buffer.update_tensordict_priority(sampled_tensordict)\n\n    rewards.append(\n        (i, tensordict[\"reward\"].mean().item() / norm_factor_training / frame_skip)\n    )\n    td_record = recorder(None)\n    if td_record is not None:\n        rewards_eval.append((i, td_record[\"r_evaluation\"]))\n    if len(rewards_eval):\n        pbar.set_description(\n            f\"reward: {rewards[-1][1]: 4.4f} (r0 = {r0: 4.4f}), reward eval: reward: {rewards_eval[-1][1]: 4.4f}\"\n        )\n\n    # update the exploration strategy\n    actor_model_explore.step(current_frames)\n    if collected_frames >= init_random_frames:\n        scheduler1.step()\n        scheduler2.step()\n\ncollector.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment results\nWe make a simple plot of the average rewards during training. We can observe\nthat our policy learned quite well to solve the task.\n\n**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 1000000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.plot(*zip(*rewards), label=\"training\")\nplt.plot(*zip(*rewards_eval), label=\"eval\")\nplt.legend()\nplt.xlabel(\"iter\")\nplt.ylabel(\"reward\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling trajectories and using TD(lambda)\nTD(lambda) is known to be less biased than the regular TD-error we used in\nthe previous example. To use it, however, we need to sample trajectories and\nnot single transitions.\n\nWe modify the previous example to make this possible.\n\nThe first modification consists in building a replay buffer that stores\ntrajectories (and not transitions). We'll collect trajectories of (at most)\n250 steps (note that the total trajectory length is actually 1000, but we\ncollect batches of 500 transitions obtained over 2 environments running in\nparallel, hence only 250 steps per trajectory are collected at any given\ntime). Hence, we'll devide our replay buffer size by 250:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "buffer_size = 100000 // frame_skip // 250\nprint(\"the new buffer size is\", buffer_size)\nbatch_size_traj = max(4, batch_size // 250)\nprint(\"the new batch size for trajectories is\", batch_size_traj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_steps_forward = 0  # disable multi-step for simplicity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code is identical to the initialization we made earlier:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nnp.random.seed(0)\n\n# get stats for normalization\nstats = get_env_stats()\n\n# Actor and qnet instantiation\nactor, qnet = make_ddpg_actor(\n    stats=stats,\n    device=device,\n)\nif device == torch.device(\"cpu\"):\n    actor.share_memory()\n# Target network\nqnet_target = deepcopy(qnet).requires_grad_(False)\n\n# Exploration wrappers:\nactor_model_explore = OrnsteinUhlenbeckProcessWrapper(\n    actor,\n    annealing_num_steps=annealing_frames,\n).to(device)\nif device == torch.device(\"cpu\"):\n    actor_model_explore.share_memory()\n\n# Environment setting:\ncreate_env_fn = parallel_env_constructor(\n    stats=stats,\n)\n# Batch collector:\ncollector = MultiaSyncDataCollector(\n    create_env_fn=[create_env_fn, create_env_fn],\n    policy=actor_model_explore,\n    total_frames=total_frames,\n    max_frames_per_traj=1000,\n    frames_per_batch=frames_per_batch,\n    init_random_frames=init_random_frames,\n    reset_at_each_iter=False,\n    postproc=MultiStep(n_steps_max=n_steps_forward, gamma=gamma)\n    if n_steps_forward > 0\n    else None,\n    split_trajs=True,\n    devices=[device, device],  # device for execution\n    passing_devices=[device, device],  # device where data will be stored and passed\n    seed=None,\n    pin_memory=False,\n    update_at_each_batch=False,\n    exploration_mode=\"random\",\n)\ncollector.set_seed(seed)\n\n# Replay buffer:\nreplay_buffer = make_replay_buffer(0)\n\n# trajectory recorder\nrecorder = make_recorder(actor_model_explore, stats)\n\n# Optimizers\noptimizer_actor = optim.Adam(actor.parameters(), lr=lr, weight_decay=weight_decay)\noptimizer_qnet = optim.Adam(qnet.parameters(), lr=lr, weight_decay=weight_decay)\ntotal_collection_steps = total_frames // frames_per_batch\n\nscheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_actor, T_max=total_collection_steps\n)\nscheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_qnet, T_max=total_collection_steps\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop needs to be modified. First, whereas before extending the\nreplay buffer we used to flatten the collected data, this won't be the case\nanymore. To understand why, let's check the output shape of the data collector:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for data in collector:\n    print(data.shape)\n    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that our data has shape ``[2, 250]`` as expected: 2 envs, each\nreturning 250 frames.\n\nLet's import the td_lambda function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.objectives.value.functional import vec_td_lambda_advantage_estimate\n\nlmbda = 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop is roughly the same as before, with the exception that we\ndon't flatten the collected data. Also, the sampling from the replay buffer\nis slightly different: We will collect at minimum four trajectories, compute\nthe returns (TD(lambda)), then sample from these the values we'll be using\nto compute gradients. This ensures that do not have batches that are\n'too big' but still compute an accurate return.\n\nNote that when storing tensordicts the replay buffer, we must change their\nbatch size: indeed, we will be storing an \"index\" (and possibly an\npriority) key in the stored tensordicts that will not have a time dimension.\nBecause of this, when sampling from the replay buffer, we remove the keys\nthat do not have a time dimension, change the batch size to\n``torch.Size([batch, time])``, compute our loss and then revert the\nbatch size to ``torch.Size([batch])``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rewards = []\nrewards_eval = []\n\n# Main loop\nnorm_factor_training = (\n    sum(gamma**i for i in range(n_steps_forward)) if n_steps_forward else 1\n)\n\ncollected_frames = 0\n# # if tqdm is to be used\n# pbar = tqdm.tqdm(total=total_frames)\nr0 = None\nfor i, tensordict in enumerate(collector):\n\n    # update weights of the inference policy\n    collector.update_policy_weights_()\n\n    if r0 is None:\n        r0 = tensordict[\"reward\"].mean().item()\n    #     pbar.update(tensordict.numel())\n\n    # extend the replay buffer with the new data\n    tensordict.batch_size = tensordict.batch_size[\n        :1\n    ]  # this is necessary for prioritized replay buffers: we will assign one priority value to each element, hence the batch_size must comply with the number of priority values\n    current_frames = tensordict.numel()\n    collected_frames += tensordict[\"mask\"].sum()\n    replay_buffer.extend(tensordict.cpu())\n\n    # optimization steps\n    if collected_frames >= init_random_frames:\n        for _ in range(optim_steps_per_batch):\n            # sample from replay buffer\n            sampled_tensordict = replay_buffer.sample(batch_size_traj)\n            # reset the batch size temporarily, and exclude index whose shape is incompatible with the new size\n            index = sampled_tensordict.get(\"index\")\n            sampled_tensordict.exclude(\"index\", inplace=True)\n            sampled_tensordict.batch_size = [batch_size_traj, 250]\n\n            # compute loss for qnet and backprop\n            with hold_out_net(actor):\n                # get next state value\n                next_tensordict = step_mdp(sampled_tensordict)\n                qnet_target(actor(next_tensordict.view(-1))).view(\n                    sampled_tensordict.shape\n                )\n                next_value = next_tensordict[\"state_action_value\"]\n                assert not next_value.requires_grad\n\n            # This is the crucial bit: we'll compute the TD(lambda) instead of a simple single step estimate\n            done = sampled_tensordict[\"done\"]\n            reward = sampled_tensordict[\"reward\"]\n            value = qnet(sampled_tensordict.view(-1)).view(sampled_tensordict.shape)[\n                \"state_action_value\"\n            ]\n            advantage = vec_td_lambda_advantage_estimate(\n                gamma, lmbda, value, next_value, reward, done\n            )\n            # we sample from the values we have computed\n            rand_idx = torch.randint(0, advantage.numel(), (batch_size,))\n            value_loss = advantage.view(-1)[rand_idx].pow(2).mean()\n\n            # we write the td_error in the sampled_tensordict for priority update\n            # because the indices of the samples is tracked in sampled_tensordict\n            # and the replay buffer will know which priorities to update.\n            value_loss.backward()\n\n            optimizer_qnet.step()\n            optimizer_qnet.zero_grad()\n\n            # compute loss for actor and backprop: the actor must maximise the state-action value, hence the loss is the neg value of this.\n            sampled_tensordict_actor = sampled_tensordict.select(*actor.in_keys)\n            with hold_out_net(qnet):\n                qnet(actor(sampled_tensordict_actor.view(-1))).view(\n                    sampled_tensordict.shape\n                )\n            actor_loss = -sampled_tensordict_actor[\"state_action_value\"]\n            actor_loss.view(-1)[rand_idx].mean().backward()\n\n            optimizer_actor.step()\n            optimizer_actor.zero_grad()\n\n            # update qnet_target params\n            for (p_in, p_dest) in zip(qnet.parameters(), qnet_target.parameters()):\n                p_dest.data.copy_(tau * p_in.data + (1 - tau) * p_dest.data)\n            for (b_in, b_dest) in zip(qnet.buffers(), qnet_target.buffers()):\n                b_dest.data.copy_(tau * b_in.data + (1 - tau) * b_dest.data)\n\n            # update priority\n            sampled_tensordict.batch_size = [batch_size_traj]\n            sampled_tensordict[\"td_error\"] = advantage.detach().pow(2).mean(1)\n            sampled_tensordict[\"index\"] = index\n            if prb:\n                replay_buffer.update_tensordict_priority(sampled_tensordict)\n\n    rewards.append(\n        (i, tensordict[\"reward\"].mean().item() / norm_factor_training / frame_skip)\n    )\n    td_record = recorder(None)\n    if td_record is not None:\n        rewards_eval.append((i, td_record[\"r_evaluation\"]))\n    #     if len(rewards_eval):\n    #         pbar.set_description(f\"reward: {rewards[-1][1]: 4.4f} (r0 = {r0: 4.4f}), reward eval: reward: {rewards_eval[-1][1]: 4.4f}\")\n\n    # update the exploration strategy\n    actor_model_explore.step(current_frames)\n    if collected_frames >= init_random_frames:\n        scheduler1.step()\n        scheduler2.step()\n\ncollector.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe that using TD(lambda) made our results considerably more\nstable for a similar training speed:\n\n**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 1000000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.plot(*zip(*rewards), label=\"training\")\nplt.plot(*zip(*rewards_eval), label=\"eval\")\nplt.legend()\nplt.xlabel(\"iter\")\nplt.ylabel(\"reward\")\nplt.tight_layout()\nplt.title(\"TD-labmda DDPG results\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}