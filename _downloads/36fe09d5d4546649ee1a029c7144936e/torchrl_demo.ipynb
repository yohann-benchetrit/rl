{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Introduction to TorchRL\nThis demo was presented at ICML 2022 on the industry demo day.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It gives a good overview of TorchRL functionalities. Feel free to reach out\nto vmoens@fb.com or submit issues if you have questions or comments about\nit.\n\nTorchRL is an open-source Reinforcement Learning (RL) library for PyTorch.\n\nhttps://github.com/pytorch/rl\n\nThe PyTorch ecosystem team (Meta) has decided to invest in that library to\nprovide a leading platform to develop RL solutions in research settings.\n\nIt provides pytorch and **python-first**, low and high level\n**abstractions** # for RL that are intended to be efficient, documented and\nproperly tested.\nThe code is aimed at supporting research in RL. Most of it is written in\npython in a highly modular way, such that researchers can easily swap\ncomponents, transform them or write new ones with little effort.\n\nThis repo attempts to align with the existing pytorch ecosystem libraries\nin that it has a dataset pillar (torchrl/envs), transforms, models, data\nutilities (e.g. collectors and containers), etc. TorchRL aims at having as\nfew dependencies as possible (python standard library, numpy and pytorch).\nCommon environment libraries (e.g. OpenAI gym) are only optional.\n\n**Content**:\n   .. aafig::\n\n     \"torchrl\"\n     \u2502\n     \u251c\u2500\u2500 \"collectors\"\n     \u2502   \u2514\u2500\u2500 \"collectors.py\"\n     \u251c\u2500\u2500 \"data\"\n     \u2502   \u251c\u2500\u2500 \"tensor_specs.py\"\n     \u2502   \u251c\u2500\u2500 \"postprocs\"\n     \u2502   \u2502  \u2514\u2500\u2500 \"postprocs.py\"\n     \u2502   \u2514\u2500\u2500 \"replay_buffers\"\n     \u2502      \u251c\u2500\u2500 \"replay_buffers.py\"\n     \u2502      \u2514\u2500\u2500 \"storages.py\"\n     \u251c\u2500\u2500 \"envs\"\n     \u2502   \u251c\u2500\u2500 \"common.py\"\n     \u2502   \u251c\u2500\u2500 \"env_creator.py\"\n     \u2502   \u251c\u2500\u2500 \"gym_like.py\"\n     \u2502   \u251c\u2500\u2500 \"vec_env.py\"\n     \u2502   \u251c\u2500\u2500 \"libs\"\n     \u2502   \u2502  \u251c\u2500\u2500 \"dm_control.py\"\n     \u2502   \u2502  \u2514\u2500\u2500 \"gym.py\"\n     \u2502   \u2514\u2500\u2500 \"transforms\"\n     \u2502      \u251c\u2500\u2500 \"functional.py\"\n     \u2502      \u2514\u2500\u2500 \"transforms.py\"\n     \u251c\u2500\u2500 \"modules\"\n     \u2502   \u251c\u2500\u2500 \"distributions\"\n     \u2502   \u2502  \u251c\u2500\u2500 \"continuous.py\"\n     \u2502   \u2502  \u2514\u2500\u2500 \"discrete.py\"\n     \u2502   \u251c\u2500\u2500 \"models\"\n     \u2502   \u2502  \u251c\u2500\u2500 \"models.py\"\n     \u2502   \u2502  \u2514\u2500\u2500 \"exploration.py\"\n     \u2502   \u2514\u2500\u2500 \"tensordict_module\"\n     \u2502      \u251c\u2500\u2500 \"actors.py\"\n     \u2502      \u251c\u2500\u2500 \"common.py\"\n     \u2502      \u251c\u2500\u2500 \"exploration.py\"\n     \u2502      \u251c\u2500\u2500 \"probabilistic.py\"\n     \u2502      \u2514\u2500\u2500 \"sequence.py\"\n     \u251c\u2500\u2500 \"objectives\"\n     \u2502   \u251c\u2500\u2500 \"common.py\"\n     \u2502   \u251c\u2500\u2500 \"ddpg.py\"\n     \u2502   \u251c\u2500\u2500 \"dqn.py\"\n     \u2502   \u251c\u2500\u2500 \"functional.py\"\n     \u2502   \u251c\u2500\u2500 \"ppo.py\"\n     \u2502   \u251c\u2500\u2500 \"redq.py\"\n     \u2502   \u251c\u2500\u2500 \"reinforce.py\"\n     \u2502   \u251c\u2500\u2500 \"sac.py\"\n     \u2502   \u251c\u2500\u2500 \"utils.py\"\n     \u2502   \u2514\u2500\u2500 \"value\"\n     \u2502      \u251c\u2500\u2500 \"advantages.py\"\n     \u2502      \u251c\u2500\u2500 \"functional.py\"\n     \u2502      \u251c\u2500\u2500 \"pg.py\"\n     \u2502      \u251c\u2500\u2500 \"utils.py\"\n     \u2502      \u2514\u2500\u2500 \"vtrace.py\"\n     \u251c\u2500\u2500 \"record\"\n     \u2502   \u2514\u2500\u2500 \"recorder.py\"\n     \u2514\u2500\u2500 \"trainers\"\n         \u251c\u2500\u2500 \"loggers\"\n         \u2502  \u251c\u2500\u2500 \"common.py\"\n         \u2502  \u251c\u2500\u2500 \"csv.py\"\n         \u2502  \u251c\u2500\u2500 \"mlflow.py\"\n         \u2502  \u251c\u2500\u2500 \"tensorboard.py\"\n         \u2502  \u2514\u2500\u2500 \"wandb.py\"\n         \u251c\u2500\u2500 \"trainers.py\"\n         \u2514\u2500\u2500 \"helpers\"\n            \u251c\u2500\u2500 \"collectors.py\"\n            \u251c\u2500\u2500 \"envs.py\"\n            \u251c\u2500\u2500 \"loggers.py\"\n            \u251c\u2500\u2500 \"losses.py\"\n            \u251c\u2500\u2500 \"models.py\"\n            \u251c\u2500\u2500 \"replay_buffer.py\"\n            \u2514\u2500\u2500 \"trainers.py\"\n\nUnlike other domains, RL is less about media than *algorithms*. As such, it\nis harder to make truly independent components.\n\nWhat TorchRL is not:\n\n* a collection of algorithms: we do not intend to provide SOTA implementations of RL algorithms,\n  but we provide these algorithms only as examples of how to use the library.\n\n* a research framework: modularity in TorchRL comes in two flavours. First, we try\n  to build re-usable components, such that they can be easily swapped with each other.\n  Second, we make our best such that components can be used independently of the rest\n  of the library.\n\nTorchRL has very few core dependencies, predominantly PyTorch and numpy. All\nother dependencies (gym, torchvision, wandb / tensorboard) are optional.\n\n## Data\n\n### TensorDict\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom tensordict import TensorDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a TensorDict.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 5\ntensordict = TensorDict(\n    source={\n        \"key 1\": torch.zeros(batch_size, 3),\n        \"key 2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool),\n    },\n    batch_size=[batch_size],\n)\nprint(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can index a TensorDict as well as query keys.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tensordict[2])\nprint(tensordict[\"key 1\"] is tensordict.get(\"key 1\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following shows how to stack multiple TensorDicts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict1 = TensorDict(\n    source={\n        \"key 1\": torch.zeros(batch_size, 1),\n        \"key 2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool),\n    },\n    batch_size=[batch_size],\n)\n\ntensordict2 = TensorDict(\n    source={\n        \"key 1\": torch.ones(batch_size, 1),\n        \"key 2\": torch.ones(batch_size, 5, 6, dtype=torch.bool),\n    },\n    batch_size=[batch_size],\n)\n\ntensordict = torch.stack([tensordict1, tensordict2], 0)\ntensordict.batch_size, tensordict[\"key 1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are some other functionalities of TensorDict.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    \"view(-1): \",\n    tensordict.view(-1).batch_size,\n    tensordict.view(-1).get(\"key 1\").shape,\n)\n\nprint(\"to device: \", tensordict.to(\"cpu\"))\n\n# print(\"pin_memory: \", tensordict.pin_memory())\n\nprint(\"share memory: \", tensordict.share_memory_())\n\nprint(\n    \"permute(1, 0): \",\n    tensordict.permute(1, 0).batch_size,\n    tensordict.permute(1, 0).get(\"key 1\").shape,\n)\n\nprint(\n    \"expand: \",\n    tensordict.expand(3, *tensordict.batch_size).batch_size,\n    tensordict.expand(3, *tensordict.batch_size).get(\"key 1\").shape,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can create a **nested TensorDict** as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n    source={\n        \"key 1\": torch.zeros(batch_size, 3),\n        \"key 2\": TensorDict(\n            source={\"sub-key 1\": torch.zeros(batch_size, 2, 1)},\n            batch_size=[batch_size, 2],\n        ),\n    },\n    batch_size=[batch_size],\n)\ntensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replay buffers\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.data import PrioritizedReplayBuffer, ReplayBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb = ReplayBuffer(collate_fn=lambda x: x)\nrb.add(1)\nrb.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb.extend([2, 3])\nrb.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb = PrioritizedReplayBuffer(alpha=0.7, beta=1.1, collate_fn=lambda x: x)\nrb.add(1)\nrb.sample(1)\nrb.update_priority(1, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are examples of using a replaybuffer with tensordicts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "collate_fn = torch.stack\nrb = ReplayBuffer(collate_fn=collate_fn)\nrb.add(TensorDict({\"a\": torch.randn(3)}, batch_size=[]))\nlen(rb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb.extend(TensorDict({\"a\": torch.randn(2, 3)}, batch_size=[2]))\nprint(len(rb))\nprint(rb.sample(10))\nprint(rb.sample(2).contiguous())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nfrom torchrl.data import TensorDictPrioritizedReplayBuffer\n\nrb = TensorDictPrioritizedReplayBuffer(alpha=0.7, beta=1.1, priority_key=\"td_error\")\nrb.extend(TensorDict({\"a\": torch.randn(2, 3)}, batch_size=[2]))\ntensordict_sample = rb.sample(2).contiguous()\ntensordict_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict_sample[\"index\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict_sample[\"td_error\"] = torch.rand(2)\nrb.update_tensordict_priority(tensordict_sample)\n\nfor i, val in enumerate(rb._sampler._sum_tree):\n    print(i, val)\n    if i == len(rb):\n        break\n\nimport gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Envs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.libs.gym import GymEnv, GymWrapper\n\ngym_env = gym.make(\"Pendulum-v1\")\nenv = GymWrapper(gym_env)\nenv = GymEnv(\"Pendulum-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = env.reset()\nenv.rand_step(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Changing environments config\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\nenv.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.close()\ndel env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import (\n    Compose,\n    NoopResetEnv,\n    ObservationNorm,\n    ToTensorImage,\n    TransformedEnv,\n)\n\nbase_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\nenv = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\nenv.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transforms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import (\n    Compose,\n    NoopResetEnv,\n    ObservationNorm,\n    StepCounter,\n    ToTensorImage,\n    TransformedEnv,\n)\n\nbase_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\nenv = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\nenv.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"env: \", env)\nprint(\"last transform parent: \", env.transform[2].parent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectorized Environments\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import ParallelEnv\n\nbase_env = ParallelEnv(\n    4,\n    lambda: GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False),\n)\nenv = TransformedEnv(\n    base_env, Compose(StepCounter(), ToTensorImage())\n)  # applies transforms on batch of envs\nenv.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\nenv.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.action_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modules\n\n### Models\n\nExample of a MLP model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import ConvNet, MLP\nfrom torchrl.modules.models.utils import SquashDims\n\nnet = MLP(num_cells=[32, 64], out_features=4, activation_class=nn.ELU)\nprint(net)\nprint(net(torch.randn(10, 3)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example of a CNN model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn = ConvNet(\n    num_cells=[32, 64],\n    kernel_sizes=[8, 4],\n    strides=[2, 1],\n    aggregator_class=SquashDims,\n)\nprint(cnn)\nprint(cnn(torch.randn(10, 3, 32, 32)).shape)  # last tensor is squashed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorDictModules\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import TensorDictModule\n\ntensordict = TensorDict({\"key 1\": torch.randn(10, 3)}, batch_size=[10])\nmodule = nn.Linear(3, 4)\ntd_module = TensorDictModule(module, in_keys=[\"key 1\"], out_keys=[\"key 2\"])\ntd_module(tensordict)\nprint(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sequences of Modules\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import TensorDictSequential\n\nbackbone_module = nn.Linear(5, 3)\nbackbone = TensorDictModule(\n    backbone_module, in_keys=[\"observation\"], out_keys=[\"hidden\"]\n)\nactor_module = nn.Linear(3, 4)\nactor = TensorDictModule(actor_module, in_keys=[\"hidden\"], out_keys=[\"action\"])\nvalue_module = MLP(out_features=1, num_cells=[4, 5])\nvalue = TensorDictModule(value_module, in_keys=[\"hidden\", \"action\"], out_keys=[\"value\"])\n\nsequence = TensorDictSequential(backbone, actor, value)\nprint(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(sequence.in_keys, sequence.out_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n    {\"observation\": torch.randn(3, 5)},\n    [3],\n)\nbackbone(tensordict)\nactor(tensordict)\nvalue(tensordict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n    {\"observation\": torch.randn(3, 5)},\n    [3],\n)\nsequence(tensordict)\nprint(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functional Programming (Ensembling / Meta-RL)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import make_functional\n\nparams = make_functional(sequence)\nlen(list(sequence.parameters()))  # functional modules have no parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sequence(tensordict, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import functorch\n\nparams_expand = params.expand(4)\ntensordict_exp = functorch.vmap(sequence, (None, 0))(tensordict, params_expand)\nprint(tensordict_exp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specialized Classes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nfrom torchrl.data import BoundedTensorSpec\nfrom torchrl.modules import SafeModule\n\nspec = BoundedTensorSpec(-torch.ones(3), torch.ones(3))\nbase_module = nn.Linear(5, 3)\nmodule = SafeModule(\n    module=base_module, spec=spec, in_keys=[\"obs\"], out_keys=[\"action\"], safe=True\n)\ntensordict = TensorDict({\"obs\": torch.randn(5)}, batch_size=[])\nmodule(tensordict)[\"action\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"obs\": torch.randn(5) * 100}, batch_size=[])\nmodule(tensordict)[\"action\"]  # safe=True projects the result within the set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import Actor\n\nbase_module = nn.Linear(5, 3)\nactor = Actor(base_module, in_keys=[\"obs\"])\ntensordict = TensorDict({\"obs\": torch.randn(5)}, batch_size=[])\nactor(tensordict)  # action is the default value\n\nfrom tensordict.nn import (\n    ProbabilisticTensorDictModule,\n    ProbabilisticTensorDictSequential,\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Probabilistic modules\nfrom torchrl.modules import NormalParamWrapper, TanhNormal\n\ntd = TensorDict({\"input\": torch.randn(3, 5)}, [3])\nnet = NormalParamWrapper(nn.Linear(5, 4))  # splits the output in loc and scale\nmodule = TensorDictModule(net, in_keys=[\"input\"], out_keys=[\"loc\", \"scale\"])\ntd_module = ProbabilisticTensorDictSequential(\n    module,\n    ProbabilisticTensorDictModule(\n        in_keys=[\"loc\", \"scale\"],\n        out_keys=[\"action\"],\n        distribution_class=TanhNormal,\n        return_log_prob=False,\n    ),\n)\ntd_module(td)\nprint(td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# returning the log-probability\ntd = TensorDict({\"input\": torch.randn(3, 5)}, [3])\ntd_module = ProbabilisticTensorDictSequential(\n    module,\n    ProbabilisticTensorDictModule(\n        in_keys=[\"loc\", \"scale\"],\n        out_keys=[\"action\"],\n        distribution_class=TanhNormal,\n        return_log_prob=True,\n    ),\n)\ntd_module(td)\nprint(td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Sampling vs mode / mean\nfrom torchrl.envs.utils import set_exploration_mode\n\ntd = TensorDict({\"input\": torch.randn(3, 5)}, [3])\n\ntorch.manual_seed(0)\nwith set_exploration_mode(\"random\"):\n    td_module(td)\n    print(\"random:\", td[\"action\"])\n\nwith set_exploration_mode(\"mode\"):\n    td_module(td)\n    print(\"mode:\", td[\"action\"])\n\nwith set_exploration_mode(\"mean\"):\n    td_module(td)\n    print(\"mean:\", td[\"action\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Environments and Modules\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.utils import step_mdp\n\nenv = GymEnv(\"Pendulum-v1\")\n\naction_spec = env.action_spec\nactor_module = nn.Linear(3, 1)\nactor = SafeModule(\n    actor_module, spec=action_spec, in_keys=[\"observation\"], out_keys=[\"action\"]\n)\n\ntorch.manual_seed(0)\nenv.set_seed(0)\n\nmax_steps = 100\ntensordict = env.reset()\ntensordicts = TensorDict({}, [max_steps])\nfor i in range(max_steps):\n    actor(tensordict)\n    tensordicts[i] = env.step(tensordict)\n    if tensordict[\"done\"].any():\n        break\n    tensordict = step_mdp(tensordict)  # roughly equivalent to obs = next_obs\n\ntensordicts_prealloc = tensordicts.clone()\nprint(\"total steps:\", i)\nprint(tensordicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# equivalent\ntorch.manual_seed(0)\nenv.set_seed(0)\n\nmax_steps = 100\ntensordict = env.reset()\ntensordicts = []\nfor _ in range(max_steps):\n    actor(tensordict)\n    tensordicts.append(env.step(tensordict))\n    if tensordict[\"done\"].any():\n        break\n    tensordict = step_mdp(tensordict)  # roughly equivalent to obs = next_obs\ntensordicts_stack = torch.stack(tensordicts, 0)\nprint(\"total steps:\", i)\nprint(tensordicts_stack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(tensordicts_stack == tensordicts_prealloc).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# helper\ntorch.manual_seed(0)\nenv.set_seed(0)\ntensordict_rollout = env.rollout(policy=actor, max_steps=max_steps)\ntensordict_rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(tensordict_rollout == tensordicts_prealloc).all()\n\nfrom tensordict.nn import TensorDictModule\n\n# Collectors\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nfrom torchrl.collectors import MultiaSyncDataCollector, MultiSyncDataCollector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import EnvCreator, ParallelEnv\nfrom torchrl.envs.libs.gym import GymEnv\n\n# EnvCreator makes sure that we can send a lambda function from process to process\nparallel_env = ParallelEnv(3, EnvCreator(lambda: GymEnv(\"Pendulum-v1\")))\ncreate_env_fn = [parallel_env, parallel_env]\n\nactor_module = nn.Linear(3, 1)\nactor = TensorDictModule(actor_module, in_keys=[\"observation\"], out_keys=[\"action\"])\n\n# Sync data collector\ndevices = [\"cpu\", \"cpu\"]\n\ncollector = MultiSyncDataCollector(\n    create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv\n    policy=actor,\n    total_frames=240,\n    max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early\n    frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)\n    passing_devices=devices,  # len must match len of env created\n    devices=devices,\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, d in enumerate(collector):\n    if i == 0:\n        print(d)  # trajectories are split automatically in [6 workers x 10 steps]\n    collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices\nprint(i)\ncollector.shutdown()\ndel collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# async data collector: keeps working while you update your model\ncollector = MultiaSyncDataCollector(\n    create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv\n    policy=actor,\n    total_frames=240,\n    max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early\n    frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)\n    passing_devices=devices,  # len must match len of env created\n    devices=devices,\n)\n\nfor i, d in enumerate(collector):\n    if i == 0:\n        print(d)  # trajectories are split automatically in [6 workers x 10 steps]\n    collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices\nprint(i)\ncollector.shutdown()\ndel collector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectives\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# TorchRL delivers meta-RL compatible loss functions\n# Disclaimer: This APi may change in the future\nfrom torchrl.objectives import DDPGLoss\n\nactor_module = nn.Linear(3, 1)\nactor = TensorDictModule(actor_module, in_keys=[\"observation\"], out_keys=[\"action\"])\n\n\nclass ConcatModule(nn.Linear):\n    def forward(self, obs, action):\n        return super().forward(torch.cat([obs, action], -1))\n\n\nvalue_module = ConcatModule(4, 1)\nvalue = TensorDictModule(\n    value_module, in_keys=[\"observation\", \"action\"], out_keys=[\"state_action_value\"]\n)\n\nloss_fn = DDPGLoss(actor, value, gamma=0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n    {\n        \"observation\": torch.randn(10, 3),\n        \"next\": {\"observation\": torch.randn(10, 3)},\n        \"reward\": torch.randn(10, 1),\n        \"action\": torch.randn(10, 1),\n        \"done\": torch.zeros(10, 1, dtype=torch.bool),\n    },\n    batch_size=[10],\n    device=\"cpu\",\n)\nloss_td = loss_fn(tensordict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(loss_td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State of the Library\n\nTorchRL is currently an **alpha-release**: there may be bugs and there is no\nguarantee about BC-breaking changes. We should be able to move to a beta-release\nby the end of the year. Our roadmap to get there comprises:\n\n- Distributed solutions\n- Offline RL\n- Greater support for meta-RL\n- Multi-task and hierarchical RL\n\n## Contributing\n\nWe are actively looking for contributors and early users. If you're working in\nRL (or just curious), try it! Give us feedback: what will make the success of\nTorchRL is how well it covers researchers needs. To do that, we need their input!\nSince the library is nascent, it is a great time for you to shape it the way you want!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing the Library\n\nThe library is on PyPI: *pip install torchrl*\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}