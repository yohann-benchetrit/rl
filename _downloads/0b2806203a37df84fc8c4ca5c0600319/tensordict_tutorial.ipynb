{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# TensorDict\n``TensorDict`` is a new tensor structure introduced in TorchRL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With RL, you need to be able to deal with multiple tensors such as actions,\nobservations and reward. ``TensorDict`` makes it more convenient to deal\nwith multiple tensors at the same time for operations such as casting to\ndevice, reshaping, stacking etc.\n\nFurthermore, different RL algorithms can deal with different input and\noutputs. The ``TensorDict`` class makes it possible to abstract away the\ndifferences between these algorithms.\n\nTensorDict combines the convenience of using ``dicts`` to organize your\ndata with the power of pytorch tensors.\n\n## Improving the modularity of codes\nLet's suppose we have 2 datasets: Dataset A which has images and labels and\nDataset B which has images, segmentation maps and labels.\n\nSuppose we want to train a common algorithm over these two datasets (i.e. an\nalgorithm that would ignore the mask or infer it when needed).\n\nIn classical pytorch we would need to do the following:\n\n**Method A**\n```python\n>>> for i in range(optim_steps):\n...     images, labels = get_data_A()\n...     loss = loss_module(images, labels)\n...     loss.backward()\n...     optim.step()\n...     optim.zero_grad()\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Method B**\n```python\n>>> for i in range(optim_steps):\n...     images, labels = get_data_B()\n...     loss = loss_module(images, labels)\n...     loss.backward()\n...     optim.step()\n...     optim.zero_grad()\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that this limits the reusability of code. A lot of code has to be\nrewriten because of the modality difference between the 2 datasets.\nThe idea of TensorDict is to do the following:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**General Method**\n```python\n>>> for i in range(optim_steps):\n...     images, labels = get_data()\n...     loss = loss_module(images, labels)\n...     loss.backward()\n...     optim.step()\n...     optim.zero_grad()\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now reuse the same training loop across datasets and losses.\n\n### Can't I do this with a python dict?\n\nOne could argue that you could achieve the same results with a dataset\nthat outputs a pytorch dict.\n```python\n>>> class DictDataset(Dataset):\n...     ...\n...     def __getitem__(self, idx)\n...         ...\n...         return {\"images\": image, \"masks\": mask}\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However to achieve this you would need to write a complicated collate\nfunction that make sure that every modality is aggregated properly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def collate_dict_fn(dict_list):\n    final_dict = {}\n    for key in dict_list[0].keys():\n        final_dict[key] = []\n        for single_dict in dict_list:\n            final_dict[key].append(single_dict[key])\n        final_dict[key] = torch.stack(final_dict[key], dim=0)\n    return final_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With TensorDicts this is now much simpler:\n\n**dataloader = Dataloader(DictDataset(), collate_fn = collate_dict_fn)**\n```python\n>>> class DictDataset(Dataset):\n...   ...\n...   def __getitem__(self, idx)\n...       ...\n...       return TensorDict({\"images\": image, \"masks\": mask})\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, the collate function is as simple as:\n\n**collate_tensordict_fn = lambda tds : torch.stack(tds, dim=0)**\n\n**dataloader = Dataloader(DictDataset(), collate_fn = collate_tensordict_fn)**\n\nThis is even more useful when considering nested structures\n(Which ``TensorDict`` supports).\n\nTensorDict inherits multiple properties from ``torch.Tensor`` and ``dict``\nthat we will detail furtherdown.\n\n## TensorDict structure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.tensordict import (\n    _PermutedTensorDict,\n    _UnsqueezedTensorDict,\n    _ViewedTensorDict,\n    TensorDict,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorDict is a Datastructure indexed by either keys or numerical indices.\nThe values can either be tensors, memory-mapped tensors or ``TensorDict``. The\nvalues need to share the same memory location (device or shared memory).\nThey can however have different dtypes.\n\nAnother essential property of TensorDict is the ``batch_size`` (or ``shape``)\nwhich is defined as the n-first dimensions of the tensors. It must be common\nacross values, and it must be set explicitly when instantiating a ``TensorDict``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.zeros(3, 4)\nb = torch.zeros(3, 4, 5)\n\n# works\ntensordict = TensorDict({\"a\": a, \"b\": b}, batch_size=[3, 4])\ntensordict = TensorDict({\"a\": a, \"b\": b}, batch_size=[3])\ntensordict = TensorDict({\"a\": a, \"b\": b}, batch_size=[])\n\n# does not work\ntry:\n    tensordict = TensorDict({\"a\": a, \"b\": b}, batch_size=[3, 4, 5])\nexcept RuntimeError:\n    print(\"caramba!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nested ``TensorDict`` have therefore the following property: the parent\n``TensorDict`` needs to have a batch_size included in the childs\n``TensorDict`` batch size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.zeros(3, 4)\nb = TensorDict(\n    {\n        \"c\": torch.zeros(3, 4, 5, dtype=torch.int32),\n        \"d\": torch.zeros(3, 4, 5, 6, dtype=torch.float32),\n    },\n    batch_size=[3, 4, 5],\n)\ntensordict = TensorDict({\"a\": a, \"b\": b}, batch_size=[3, 4])\nprint(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``TensorDict`` does not support algebraic operations by design.\n\n## TensorDict Dictionary Features\n``TensorDict`` shares a lot of features with python dictionaries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.zeros(3, 4, 5)\nb = torch.zeros(3, 4)\ntensordict = TensorDict({\"a\": a, \"b\": b}, batch_size=[3, 4])\nprint(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``get(key)``\nIf we want to access a certain key, we can index the tensordict\nor alternatively use the ``get`` method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"get and __getitem__ match:\", tensordict[\"a\"] is tensordict.get(\"a\") is a)\nprint(tensordict[\"a\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``get`` method also supports default values:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "out = tensordict.get(\"foo\", torch.ones(3))\nprint(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``set(key, value)``\nThe ``set()`` method can be used to set new values.\nRegular indexing also does the job:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c = torch.zeros((3, 4, 2, 2))\ntensordict.set(\"c\", c)\nprint(f\"td[\\\"c\\\"] is c: {c is tensordict['c']}\")\n\nd = torch.zeros((3, 4, 2, 2))\ntensordict[\"d\"] = d\nprint(f\"td[\\\"d\\\"] is d: {d is tensordict['d']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``keys()``\nWe can access the keys of a tensordict:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict[\"c\"] = torch.zeros(tensordict.shape)\ntensordict.set(\"d\", torch.ones(tensordict.shape))\nassert (tensordict[\"c\"] == 0).all()\nassert (tensordict[\"d\"] == 1).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``values()``\nThe values of a ``TensorDict`` can be retrieved with the ``values()`` function.\nNote that, unlike python ``dicts``, the ``values()`` method returns a\ngenerator and not a list.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for value in tensordict.values():\n    print(value.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``update(tensordict_or_dict)``\nThe ``update`` method can be used to update a TensorDict with another one\n(or with a dict):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict.update({\"a\": torch.ones((3, 4, 5)), \"d\": 2 * torch.ones((3, 4, 2))})\n# Also works with tensordict.update(TensorDict({\"a\":torch.ones((3, 4, 5)),\n# \"c\":torch.ones((3, 4, 2))}, batch_size=[3,4]))\nprint(f\"a is now equal to 1: {(tensordict['a'] == 1).all()}\")\nprint(f\"d is now equal to 2: {(tensordict['d'] == 2).all()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``del``\nTensorDict also support keys deletion with the ``del`` operator:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"before\")\nfor k in tensordict.keys():\n    print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "del tensordict[\"c\"]\nprint(\"after\")\nfor k in tensordict.keys():\n    print(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TensorDict tensor features\nOn many regards, TensorDict is a Tensor-like class: a great deal of tensor\noperation also work on tensordicts, making it easy to cast them across\nmultiple tensors.\n\n### Batch size\n``TensorDict`` has a batch size which is shared across all tensors. The batch\nsize can be [], unidimensional or multidimensional according to your needs,\nbut it must be shared across tensors. Indeed, you cannot have items that don't\nshare the batch size inside the same TensorDict:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n    {\"a\": torch.zeros(3, 4, 5), \"b\": torch.zeros(3, 4)}, batch_size=[3, 4]\n)\nprint(f\"Our TensorDict is of size {tensordict.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The batch size can be changed if needed:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# we cannot add tensors that violate the batch size:\ntry:\n    tensordict.update({\"c\": torch.zeros(4, 3, 1)})\nexcept RuntimeError as err:\n    print(f\"Caramba! We got this error: {err}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "but it must comply with the tensor shapes:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict.batch_size = [3]\nassert tensordict.batch_size == torch.Size([3])\ntensordict.batch_size = [3, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    tensordict.batch_size = [4, 4]\nexcept RuntimeError as err:\n    print(f\"Caramba! We got this error: {err}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also fill the values of a TensorDict sequentially\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({}, [10])\nfor i in range(10):\n    tensordict[i] = TensorDict({\"a\": torch.randn(3, 4)}, [])\nprint(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If all values are not filled, they get the default value of zero.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({}, [10])\nfor i in range(2):\n    tensordict[i] = TensorDict({\"a\": torch.randn(3, 4)}, [])\nassert (tensordict[9][\"a\"] == torch.zeros((3, 4))).all()\ntensordict = TensorDict(\n    {\"a\": torch.zeros(3, 4, 5), \"b\": torch.zeros(3, 4)}, batch_size=[3, 4]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Devices\nTensorDict can be sent to the desired devices like a pytorch tensor with\n``td.cuda()`` or ``td.to(device)`` with ``device`` the desired device.\n\n### Memory sharing via physical memory usage\nWhen on cpu, one can use either ``tensordict.memmap_()`` or\n``tensordict.share_memory_()`` to send a ``tensordict`` to represent it as\na memory-mapped collection of tensors or put it in shared memory resp.\n\n### Tensor operations\nWe can perform tensor operations among the batch dimensions:\n\n**Cloning**\n\nTensorDict supports cloning. Cloning returns the same TensorDict class\nthan the original item.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict_clone = tensordict.clone()\nprint(\n    f\"Content is identical ({(tensordict['a'] == tensordict_clone['a']).all()}) but duplicated ({tensordict['a'] is not tensordict_clone['a']})\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Slicing and Indexing**\n\nSlicing and indexing is supported along the batch dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tensordict[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tensordict[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tensordict[:, 2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Setting Values with Indexing**\n\nIn general, ``tensodict[tuple_index] = new_tensordict`` will work as long as\nthe batch sizes match.\n\nIf one wants to build a tensordict that keeps track of the original tensordict,\nthe ``get_sub_tensordict`` method can be used: in that case, a\n``SubTensorDict`` instance will be returned. This class will store a pointer\nto the original tensordict as well as the desired index such that tensor\nmodifications can be achieved easily.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n    {\"a\": torch.zeros(3, 4, 5), \"b\": torch.zeros(3, 4)}, batch_size=[3, 4]\n)\n# a SubTensorDict keeps track of the original one: it does not create a copy in memory of the original data\nsubtd = tensordict.get_sub_tensordict((slice(None), torch.tensor([1, 3])))\ntensordict.fill_(\"a\", -1)\nassert (subtd[\"a\"] == -1).all(), subtd[\"a\"]  # the \"a\" key-value pair has changed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can set values easily just by indexing the tensordict:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "td2 = TensorDict({\"a\": torch.zeros(2, 4, 5), \"b\": torch.zeros(2, 4)}, batch_size=[2, 4])\ntensordict[:-1] = td2\nprint(tensordict[\"a\"], tensordict[\"b\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Masking**\n\nWe mask ``TensorDict`` as we mask tensors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = torch.BoolTensor([[1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0]])\ntensordict[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Stacking**\n\n``TensorDict`` supports stacking. By default, stacking is done in a lazy\nfashion, returning a ``LazyStackedTensorDict`` item.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Stack\nclonned_tensordict = tensordict.clone()\nstaked_tensordict = torch.stack([tensordict, clonned_tensordict], dim=0)\nprint(staked_tensordict)\n\n# indexing a lazy stack returns the original tensordicts\nif staked_tensordict[0] is tensordict and staked_tensordict[1] is clonned_tensordict:\n    print(\"every tensordict is awesome!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to have a contiguous tensordict, we can call ``.to_tensordict()``\nor ``.contiguous()``. It is recommended to perform this operation before\naccessing the values of the stacked tensordict for efficiency purposes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert isinstance(staked_tensordict.contiguous(), TensorDict)\nassert isinstance(staked_tensordict.to_tensordict(), TensorDict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Unbind**\n\nTensorDict can be unbound along a dim over the tensordict batch size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "list_tensordict = tensordict.unbind(0)\nassert type(list_tensordict) == tuple\nassert len(list_tensordict) == 3\nassert (torch.stack(list_tensordict, dim=0).contiguous() == tensordict).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Cat**\n\nTensorDict supports cat to concatenate among a dim. The dim must be lower\nthan the ``batch_dims`` (i.e. the length of the batch_size).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "list_tensordict = tensordict.unbind(0)\nassert torch.cat(list_tensordict, dim=0).shape[0] == 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**View**\n\nSupport for the view operation returning a ``_ViewedTensorDict``.\nUse ``to_tensordict`` to comeback to retrieve TensorDict.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert type(tensordict.view(-1)) == _ViewedTensorDict\nassert tensordict.view(-1).shape[0] == 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Permute**\n\nWe can permute the dims of ``TensorDict``. Permute is a Lazy operation that\nreturns _PermutedTensorDict. Use ``to_tensordict`` to convert to ``TensorDict``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert type(tensordict.permute(1, 0)) == _PermutedTensorDict\nassert tensordict.permute(1, 0).batch_size == torch.Size([4, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reshape**\n\nReshape allows reshaping the ``TensorDict`` batch size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert tensordict.reshape(-1).batch_size == torch.Size([12])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Squeeze and Unsqueeze**\n\nTensordict also supports squeeze and unsqueeze. Unsqueeze is a lazy operation\nthat returns _UnsqueezedTensorDict. Use ``to_tensordict`` to retrieve a\ntensordict after unsqueeze. Calling ``unsqueeze(dim).squeeze(dim)`` returns\nthe original tensordict.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unsqueezed_tensordict = tensordict.unsqueeze(0)\nassert type(unsqueezed_tensordict) == _UnsqueezedTensorDict\nassert unsqueezed_tensordict.batch_size == torch.Size([1, 3, 4])\n\nassert type(unsqueezed_tensordict.squeeze(0)) == TensorDict\nassert unsqueezed_tensordict.squeeze(0) is tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Have fun with TensorDict!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}