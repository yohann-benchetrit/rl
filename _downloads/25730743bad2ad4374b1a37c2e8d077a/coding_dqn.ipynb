{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Coding a pixel-based DQN using TorchRL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial will guide you through the steps to code DQN to solve the\nCartPole task from scratch. DQN\n([Deep Q-Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) was\nthe founding work in deep reinforcement learning. On a high level, the\nalgorithm is quite simple: Q-learning consists in learning a table of\nstate-action values in such a way that, when facing any particular state,\nwe know which action to pick just by searching for the action with the\nhighest value. This simple setting requires the actions and states to be\ndiscretizable. DQN uses a neural network that maps state-actions pairs to\na certain value, which amortizes the cost of storing and exploring all the\npossible states: if a state has not been seen in the past, we can still pass\nit through our neural network and get an interpolated value for each of the\nactions available.\n\nIn this tutorial, you will learn:\n\n- how to build an environment in TorchRL, including transforms (e.g. data\n  normalization, frame concatenation, resizing and turning to grayscale)\n  and parallel execution;\n- how to design a QValue actor, i.e. an actor that esitmates the action\n  values and picks up the action with the highest estimated return;\n- how to collect data from your environment efficiently and store them\n  in a replay buffer;\n- how to store trajectories (and not transitions) in your replay buffer),\n  and how to estimate returns using TD(lambda);\n- how to make a module functional and use ;\n- and finally how to evaluate your model.\n\nThis tutorial assumes the reader is familiar with some of TorchRL\nprimitives, such as ``TensorDict`` and ``TensorDictModules``, although it\nshould be sufficiently transparent to be understood without a deep\nunderstanding of these classes.\n\nWe do not aim at giving a SOTA implementation of the algorithm, but rather\nto provide a high-level illustration of TorchRL features in the context\nof this algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport tqdm\nfrom functorch import vmap\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nfrom tensordict import TensorDict\nfrom tensordict.nn import get_functional\nfrom torch import nn\nfrom torchrl.collectors import MultiaSyncDataCollector\nfrom torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer\nfrom torchrl.envs import EnvCreator, ParallelEnv\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import (\n    CatFrames,\n    CatTensors,\n    Compose,\n    GrayScale,\n    ObservationNorm,\n    Resize,\n    ToTensorImage,\n    TransformedEnv,\n)\nfrom torchrl.envs.utils import set_exploration_mode, step_mdp\nfrom torchrl.modules import DuelingCnnDQNet, EGreedyWrapper, QValueActor\n\n\ndef is_notebook() -> bool:\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n        elif shell == \"TerminalInteractiveShell\":\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False  # Probably standard Python interpreter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\nLet's start with our hyperparameters. This is a totally arbitrary list of\nhyperparams that we found to work well in practice. Hopefully the performance\nof the algorithm should not be too sentitive to slight variations of these.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# hyperparams\n\n# the learning rate of the optimizer\nlr = 2e-3\n# the beta parameters of Adam\nbetas = (0.9, 0.999)\n# gamma decay factor\ngamma = 0.99\n# lambda decay factor (see second the part with TD(lambda)\nlmbda = 0.95\n# total frames collected in the environment. In other implementations, the user defines a maximum number of episodes.\n# This is harder to do with our data collectors since they return batches of N collected frames, where N is a constant.\n# However, one can easily get the same restriction on number of episodes by breaking the training loop when a certain number\n# episodes has been collected.\ntotal_frames = 500\n# Random frames used to initialize the replay buffer.\ninit_random_frames = 100\n# Frames in each batch collected.\nframes_per_batch = 32\n# Optimization steps per batch collected\nn_optim = 4\n# Frames sampled from the replay buffer at each optimization step\nbatch_size = 32\n# Size of the replay buffer in terms of frames\nbuffer_size = min(total_frames, 100000)\n# Number of environments run in parallel in each data collector\nn_workers = 1\n\ndevice = \"cuda:0\" if torch.cuda.device_count() > 0 else \"cpu\"\n\n# Smooth target network update decay parameter. This loosely corresponds to a 1/(1-tau) interval with hard target network update\ntau = 0.005\n\n# Initial and final value of the epsilon factor in Epsilon-greedy exploration (notice that since our policy is deterministic exploration is crucial)\neps_greedy_val = 0.1\neps_greedy_val_env = 0.05\n\n# To speed up learning, we set the bias of the last layer of our value network to a predefined value\ninit_bias = 20.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: for fast rendering of the tutorial ``total_frames`` hyperparameter\nwas set to a very low number. To get a reasonable performance, use a greater\nvalue e.g. 500000\n\n## Building the environment\nOur environment builder has three arguments:\n\n- parallel: determines whether multiple environments have to be run in\n  parallel. We stack the transforms after the ParallelEnv to take advantage\n  of vectorization of the operations on device, although this would\n  techinally work with every single environment attached to its own set of\n  transforms.\n- mean and standard deviation: we normalize the observations (images)\n  with two parameters computed from a random rollout in the environment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_env(parallel=False, m=0, s=1):\n\n    if parallel:\n        base_env = ParallelEnv(\n            n_workers,\n            EnvCreator(\n                lambda: GymEnv(\n                    \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n                )\n            ),\n        )\n    else:\n        base_env = GymEnv(\n            \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n        )\n\n    env = TransformedEnv(\n        base_env,\n        Compose(\n            ToTensorImage(),\n            GrayScale(),\n            Resize(64, 64),\n            ObservationNorm(in_keys=[\"pixels\"], loc=m, scale=s, standard_normal=True),\n            CatFrames(4, in_keys=[\"pixels\"]),\n        ),\n    )\n    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute normalizing constants:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dummy_env = make_env()\nv = dummy_env.transform[3].parent.reset()[\"pixels\"]\nm, s = v.mean().item(), v.std().item()\nm, s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The problem\nWe can have a look at the problem by generating a video with a random\npolicy. From gym:\n\n*A pole is attached by an un-actuated joint to a cart, which moves along a*\n*frictionless track. The pendulum is placed upright on the cart and the*\n*goal is to balance the pole by applying forces in the left and right*\n*direction on the cart.*\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# we add a CatTensors transform to copy the \"pixels\" before it's being replaced by its grayscale, resized version\ndummy_env.transform.insert(0, CatTensors([\"pixels\"], \"pixels_save\", del_keys=False))\n# we omit the policy from the rollout call: this will generate random actions from the env.action_spec attribute\neval_rollout = dummy_env.rollout(max_steps=10000).cpu()\n\n# imageio.mimwrite('cartpole_random.mp4', eval_rollout[\"pixels_save\"].numpy(), fps=30)\n# Video('cartpole_random.mp4', width=480, height=360)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the model (Deep Q-network)\nThe following function builds a ``DuelingCnnDQNet`` object which is a\nsimple CNN followed by a two-layer MLP. The only trick used here is that\nthe action values (i.e. left and right action value) are computed using\n\n\\begin{align}values = baseline(observation) + values(observation) - values(observation).mean()\\end{align}\n\nwhere ``baseline`` is a ``num_obs -> 1`` function and ``values`` is a\n``num_obs -> num_actions`` function.\n\nOur network is wrapped in a ``QValueActor``, which will read the state-action\nvalues, pick up the one with the maximum value and write all those results\nin the input ``TensorDict``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_model():\n    cnn_kwargs = {\n        \"num_cells\": [32, 64, 64],\n        \"kernel_sizes\": [6, 4, 3],\n        \"strides\": [2, 2, 1],\n        \"activation_class\": nn.ELU,\n        \"squeeze_output\": True,\n        \"aggregator_class\": nn.AdaptiveAvgPool2d,\n        \"aggregator_kwargs\": {\"output_size\": (1, 1)},\n    }\n    mlp_kwargs = {\n        \"depth\": 2,\n        \"num_cells\": [\n            64,\n            64,\n        ],\n        # \"out_features\": dummy_env.action_spec.shape[-1],\n        \"activation_class\": nn.ELU,\n    }\n    net = DuelingCnnDQNet(\n        dummy_env.action_spec.shape[-1], 1, cnn_kwargs, mlp_kwargs\n    ).to(device)\n    net.value[-1].bias.data.fill_(init_bias)\n\n    actor = QValueActor(net, in_keys=[\"pixels\"], spec=dummy_env.action_spec).to(device)\n    # init actor\n    tensordict = dummy_env.reset()\n    print(\"reset results:\", tensordict)\n    actor(tensordict)\n    print(\"Q-value network results:\", tensordict)\n\n    # make functional\n    # here's an explicit way of creating the parameters and buffer tensordict.\n    # Alternatively, we could have used `params = make_functional(actor)` from\n    # tensordict.nn\n    params = TensorDict({k: v for k, v in actor.named_parameters()}, [])\n    buffers = TensorDict({k: v for k, v in actor.named_buffers()}, [])\n    params = params.update(buffers).unflatten_keys(\".\")  # creates a nested TensorDict\n    factor = get_functional(actor)\n\n    # creating the target parameters is fairly easy with tensordict:\n    (params_target,) = (params.to_tensordict().detach(),)\n\n    # we wrap our actor in an EGreedyWrapper for data collection\n    actor_explore = EGreedyWrapper(\n        actor,\n        annealing_num_steps=total_frames,\n        eps_init=eps_greedy_val,\n        eps_end=eps_greedy_val_env,\n    )\n\n    return factor, actor, actor_explore, params, params_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When creating the model, we initialize the network with an environment reset.\nWe print the resulting tensordict instance to get an idea of what\n``QValueActor`` (pay attention to the keys ``action``, ``action_value`` and\n``chosen_action_value`` after calling the policy).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(\n    factor,\n    actor,\n    actor_explore,\n    params,\n    params_target,\n) = make_model()\nparams_flat = params.flatten_keys(\".\")\nparams_target_flat = params_target.flatten_keys(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regular DQN\nWe'll start with a simple implementation of DQN where the returns are\ncomputed without bootstrapping, i.e.\n\n  return = reward + gamma * value_next_step * not_terminated\n\nWe start with the *replay buffer*. We'll use a regular replay buffer,\nalthough a prioritized RB could improve the performance significantly.\nWe place the storage on disk using ``LazyMemmapStorage``. The only requirement\nof this storage is that the data given to it must always have the same\nshape. This storage will be instantiated later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "replay_buffer = TensorDictReplayBuffer(\n    storage=LazyMemmapStorage(buffer_size),\n    prefetch=n_optim,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our *data collector* will run two parallel environments in parallel, and\ndeliver the collected tensordicts once at a time to the main process. We'll\nuse the ``MultiaSyncDataCollector`` collector, which will collect data while\nthe optimization is taking place.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_collector = MultiaSyncDataCollector(\n    [\n        make_env(True, m=m, s=s),\n        make_env(True, m=m, s=s),\n    ],  # 2 collectors, each with an set of `num_workers` environments being run in parallel\n    policy=actor_explore,\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    exploration_mode=\"random\",  # this is the default behaviour: the collector runs in `\"random\"` (or explorative) mode\n    devices=[device, device],  # each collector can sit on a different device\n    passing_devices=[device, device],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our *optimizer* and the env used for evaluation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(list(params_flat.values()), lr)\ndummy_env = make_env(parallel=False, m=m, s=s)\nprint(actor_explore(dummy_env.reset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Various lists that will contain the values recorded for evaluation:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evals = []\ntraj_lengths_eval = []\nlosses = []\nframes = []\nvalues = []\ngrad_vals = []\ntraj_lengths = []\nmavgs = []\ntraj_count = []\nprev_traj_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pbar = tqdm.tqdm(total=total_frames)\nfor j, data in enumerate(data_collector):\n    # trajectories are padded to be stored in the same tensordict: since we do not care about consecutive step, we'll just mask the tensordict and get the flattened representation instead.\n    mask = data[\"mask\"]\n    current_frames = mask.sum().cpu().item()\n    pbar.update(current_frames)\n\n    # We store the values on the replay buffer, after placing them on CPU. When called for the first time, this will instantiate our storage object which will print its content.\n    replay_buffer.extend(data[mask].cpu())\n\n    # some logging\n    if len(frames):\n        frames.append(current_frames + frames[-1])\n    else:\n        frames.append(current_frames)\n\n    if data[\"done\"].any():\n        done = data[\"done\"].squeeze(-1)\n        traj_lengths.append(data[\"step_count\"][done].float().mean().item())\n\n    # check that we have enough data to start training\n    if sum(frames) > init_random_frames:\n        for _ in range(n_optim):\n            # sample from the RB and send to device\n            sampled_data = replay_buffer.sample(batch_size)\n            sampled_data = sampled_data.to(device, non_blocking=True)\n\n            # collect data from RB\n            reward = sampled_data[\"reward\"].squeeze(-1)\n            done = sampled_data[\"done\"].squeeze(-1).to(reward.dtype)\n            action = sampled_data[\"action\"].clone()\n\n            # Compute action value (of the action actually taken) at time t\n            sampled_data_out = sampled_data.select(*actor.in_keys)\n            sampled_data_out = factor(sampled_data_out, params=params)\n            action_value = sampled_data_out[\"action_value\"]\n            action_value = (action_value * action.to(action_value.dtype)).sum(-1)\n            with torch.no_grad():\n                # compute best action value for the next step, using target parameters\n                tdstep = step_mdp(sampled_data)\n                next_value = factor(\n                    tdstep.select(*actor.in_keys),\n                    params=params_target,\n                )[\"chosen_action_value\"].squeeze(-1)\n                exp_value = reward + gamma * next_value * (1 - done)\n            assert exp_value.shape == action_value.shape\n            # we use MSE loss but L1 or smooth L1 should also work\n            error = nn.functional.mse_loss(exp_value, action_value).mean()\n            error.backward()\n\n            gv = sum([p.grad.pow(2).sum() for p in params_flat.values()]).sqrt()\n            nn.utils.clip_grad_value_(list(params_flat.values()), 1)\n\n            optim.step()\n            optim.zero_grad()\n\n            # update of the target parameters\n            for (key, p1) in params_flat.items():\n                p2 = params_target_flat[key]\n                params_target_flat.set_(key, tau * p1.data + (1 - tau) * p2.data)\n\n        pbar.set_description(\n            f\"error: {error: 4.4f}, value: {action_value.mean(): 4.4f}\"\n        )\n        actor_explore.step(current_frames)\n\n        # logs\n        with set_exploration_mode(\"mode\"), torch.no_grad():\n            # execute a rollout. The `set_exploration_mode(\"mode\")` has no effect here since the policy is deterministic, but we add it for completeness\n            eval_rollout = dummy_env.rollout(max_steps=10000, policy=actor).cpu()\n        grad_vals.append(float(gv))\n        traj_lengths_eval.append(eval_rollout.shape[-1])\n        evals.append(eval_rollout[\"reward\"].squeeze(-1).sum(-1).item())\n        if len(mavgs):\n            mavgs.append(evals[-1] * 0.05 + mavgs[-1] * 0.95)\n        else:\n            mavgs.append(evals[-1])\n        losses.append(error.item())\n        values.append(action_value.mean().item())\n        traj_count.append(prev_traj_count + data[\"done\"].sum().item())\n        prev_traj_count = traj_count[-1]\n        # plots\n        if j % 10 == 0:\n            if is_notebook():\n                display.clear_output(wait=True)\n                display.display(plt.gcf())\n            else:\n                plt.clf()\n            plt.figure(figsize=(15, 15))\n            plt.subplot(3, 2, 1)\n            plt.plot(frames[-len(evals) :], evals, label=\"return\")\n            plt.plot(frames[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"frames collected\")\n            plt.ylabel(\"trajectory length (= return)\")\n            plt.subplot(3, 2, 2)\n            plt.plot(traj_count[-len(evals) :], evals, label=\"return\")\n            plt.plot(traj_count[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"trajectories collected\")\n            plt.legend()\n            plt.subplot(3, 2, 3)\n            plt.plot(frames[-len(losses) :], losses)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"loss\")\n            plt.subplot(3, 2, 4)\n            plt.plot(frames[-len(values) :], values)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"value\")\n            plt.subplot(3, 2, 5)\n            plt.plot(frames[-len(grad_vals) :], grad_vals)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"grad norm\")\n            if len(traj_lengths):\n                plt.subplot(3, 2, 6)\n                plt.plot(traj_lengths)\n                plt.xlabel(\"batches\")\n                plt.title(\"traj length (training)\")\n        plt.savefig(\"dqn_td0.png\")\n        if is_notebook():\n            plt.show()\n\n    # update policy weights\n    data_collector.update_policy_weights_()\n\nif is_notebook():\n    display.clear_output(wait=True)\n    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 500000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\nplt.imshow(plt.imread(\"dqn_td0.png\"))\nplt.tight_layout()\nplt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# save results\ntorch.save(\n    {\n        \"frames\": frames,\n        \"evals\": evals,\n        \"mavgs\": mavgs,\n        \"losses\": losses,\n        \"values\": values,\n        \"grad_vals\": grad_vals,\n        \"traj_lengths_training\": traj_lengths,\n        \"traj_count\": traj_count,\n        \"weights\": (params,),\n    },\n    \"saved_results_td0.pt\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TD-lambda\nWe can improve the above algorithm by getting a better estimate of the\nreturn, using not only the next state value but the whole sequence of rewards\nand values that follow a particular step.\n\nTorchRL provides a vectorized version of TD(lambda) named\n``vec_td_lambda_advantage_estimate``. We'll use this to obtain a target\nvalue that the value network will be trained to match.\n\nThe big difference in this implementation is that we'll store entire\ntrajectories and not single steps in the replay buffer. This will be done\nautomatically as long as we're not \"flattening\" the tensordict collected\nusing its mask: by keeping a shape ``[Batch x timesteps]`` and giving this\nto the RB, we'll be creating a replay buffer of size\n``[Capacity x timesteps]``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.tensordict import pad\nfrom torchrl.objectives.value.functional import vec_td_lambda_advantage_estimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We reset the actor, the RB and the collector\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(\n    factor,\n    actor,\n    actor_explore,\n    params,\n    params_target,\n) = make_model()\nparams_flat = params.flatten_keys(\".\")\nparams_target_flat = params_target.flatten_keys(\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_size = frames_per_batch // n_workers\n\nreplay_buffer = TensorDictReplayBuffer(\n    storage=LazyMemmapStorage(-(-buffer_size // max_size)),\n    prefetch=n_optim,\n)\n\ndata_collector = MultiaSyncDataCollector(\n    [make_env(True, m=m, s=s), make_env(True, m=m, s=s)],\n    policy=actor_explore,\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    exploration_mode=\"random\",\n    devices=[device, device],\n    passing_devices=[device, device],\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(list(params_flat.values()), lr)\ndummy_env = make_env(parallel=False, m=m, s=s)\nprint(actor_explore(dummy_env.reset()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evals = []\ntraj_lengths_eval = []\nlosses = []\nframes = []\nvalues = []\ngrad_vals = []\ntraj_lengths = []\nmavgs = []\ntraj_count = []\nprev_traj_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop\nThere are very few differences with the training loop above:\n\n- The tensordict received by the collector is not masked but padded to the\n  desired shape (such that all tensordicts have the same shape of\n  ``[Batch x max_size]``), and sent directly to the RB.\n- We use ``vec_td_lambda_advantage_estimate`` to compute the target value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pbar = tqdm.tqdm(total=total_frames)\nfor j, data in enumerate(data_collector):\n    mask = data[\"mask\"]\n    data = pad(data, [0, 0, 0, max_size - data.shape[1]])\n    current_frames = mask.sum().cpu().item()\n    pbar.update(current_frames)\n\n    replay_buffer.extend(data.cpu())\n    if len(frames):\n        frames.append(current_frames + frames[-1])\n    else:\n        frames.append(current_frames)\n\n    if data[\"done\"].any():\n        done = data[\"done\"].squeeze(-1)\n        traj_lengths.append(data[\"step_count\"][done].float().mean().item())\n\n    if sum(frames) > init_random_frames:\n        for _ in range(n_optim):\n            sampled_data = replay_buffer.sample(batch_size // max_size)\n            sampled_data = sampled_data.clone().to(device, non_blocking=True)\n\n            reward = sampled_data[\"reward\"]\n            done = sampled_data[\"done\"].to(reward.dtype)\n            action = sampled_data[\"action\"].clone()\n\n            sampled_data_out = sampled_data.select(*actor.in_keys)\n            sampled_data_out = vmap(factor, (0, None))(sampled_data_out, params)\n            action_value = sampled_data_out[\"action_value\"]\n            action_value = (action_value * action.to(action_value.dtype)).sum(-1, True)\n            with torch.no_grad():\n                tdstep = step_mdp(sampled_data)\n                next_value = vmap(factor, (0, None))(\n                    tdstep.select(*actor.in_keys), params\n                )\n                next_value = next_value[\"chosen_action_value\"]\n            error = vec_td_lambda_advantage_estimate(\n                gamma,\n                lmbda,\n                action_value,\n                next_value,\n                reward,\n                done,\n            ).pow(2)\n            # reward + gamma * next_value * (1 - done)\n            mask = sampled_data[\"mask\"]\n            error = error[mask].mean()\n            # assert exp_value.shape == action_value.shape\n            # error = nn.functional.smooth_l1_loss(exp_value, action_value).mean()\n            # error = nn.functional.mse_loss(exp_value, action_value)[mask].mean()\n            error.backward()\n\n            # gv = sum([p.grad.pow(2).sum() for p in params_flat.values()]).sqrt()\n            # nn.utils.clip_grad_value_(list(params_flat.values()), 1)\n            gv = nn.utils.clip_grad_norm_(list(params_flat.values()), 100)\n\n            optim.step()\n            optim.zero_grad()\n\n            for (key, p1) in params_flat.items():\n                p2 = params_target_flat[key]\n                params_target_flat.set_(key, tau * p1.data + (1 - tau) * p2.data)\n\n        pbar.set_description(\n            f\"error: {error: 4.4f}, value: {action_value.mean(): 4.4f}\"\n        )\n        actor_explore.step(current_frames)\n\n        # logs\n        with set_exploration_mode(\"random\"), torch.no_grad():\n            #         eval_rollout = dummy_env.rollout(max_steps=1000, policy=actor_explore, auto_reset=True).cpu()\n            eval_rollout = dummy_env.rollout(\n                max_steps=10000, policy=actor, auto_reset=True\n            ).cpu()\n        grad_vals.append(float(gv))\n        traj_lengths_eval.append(eval_rollout.shape[-1])\n        evals.append(eval_rollout[\"reward\"].squeeze(-1).sum(-1).item())\n        if len(mavgs):\n            mavgs.append(evals[-1] * 0.05 + mavgs[-1] * 0.95)\n        else:\n            mavgs.append(evals[-1])\n        losses.append(error.item())\n        values.append(action_value[mask].mean().item())\n        traj_count.append(prev_traj_count + data[\"done\"].sum().item())\n        prev_traj_count = traj_count[-1]\n        # plots\n        if j % 10 == 0:\n            if is_notebook():\n                display.clear_output(wait=True)\n                display.display(plt.gcf())\n            else:\n                plt.clf()\n            plt.figure(figsize=(15, 15))\n            plt.subplot(3, 2, 1)\n            plt.plot(frames[-len(evals) :], evals, label=\"return\")\n            plt.plot(frames[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"frames collected\")\n            plt.ylabel(\"trajectory length (= return)\")\n            plt.subplot(3, 2, 2)\n            plt.plot(traj_count[-len(evals) :], evals, label=\"return\")\n            plt.plot(traj_count[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"trajectories collected\")\n            plt.legend()\n            plt.subplot(3, 2, 3)\n            plt.plot(frames[-len(losses) :], losses)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"loss\")\n            plt.subplot(3, 2, 4)\n            plt.plot(frames[-len(values) :], values)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"value\")\n            plt.subplot(3, 2, 5)\n            plt.plot(frames[-len(grad_vals) :], grad_vals)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"grad norm\")\n            if len(traj_lengths):\n                plt.subplot(3, 2, 6)\n                plt.plot(traj_lengths)\n                plt.xlabel(\"batches\")\n                plt.title(\"traj length (training)\")\n        plt.savefig(\"dqn_tdlambda.png\")\n        if is_notebook():\n            plt.show()\n\n    # update policy weights\n    data_collector.update_policy_weights_()\n\nif is_notebook():\n    display.clear_output(wait=True)\n    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 500000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\nplt.imshow(plt.imread(\"dqn_tdlambda.png\"))\nplt.tight_layout()\nplt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# save results\ntorch.save(\n    {\n        \"frames\": frames,\n        \"evals\": evals,\n        \"mavgs\": mavgs,\n        \"losses\": losses,\n        \"values\": values,\n        \"grad_vals\": grad_vals,\n        \"traj_lengths_training\": traj_lengths,\n        \"traj_count\": traj_count,\n        \"weights\": (params,),\n    },\n    \"saved_results_tdlambda.pt\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare the results on a single plot. Because the TD(lambda) version\nworks better, we'll have fewer episodes collected for a given number of\nframes (as there are more frames per episode).\n\n**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 500000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "load_td0 = torch.load(\"saved_results_td0.pt\")\nload_tdlambda = torch.load(\"saved_results_tdlambda.pt\")\nframes_td0 = load_td0[\"frames\"]\nframes_tdlambda = load_tdlambda[\"frames\"]\nevals_td0 = load_td0[\"evals\"]\nevals_tdlambda = load_tdlambda[\"evals\"]\nmavgs_td0 = load_td0[\"mavgs\"]\nmavgs_tdlambda = load_tdlambda[\"mavgs\"]\nlosses_td0 = load_td0[\"losses\"]\nlosses_tdlambda = load_tdlambda[\"losses\"]\nvalues_td0 = load_td0[\"values\"]\nvalues_tdlambda = load_tdlambda[\"values\"]\ngrad_vals_td0 = load_td0[\"grad_vals\"]\ngrad_vals_tdlambda = load_tdlambda[\"grad_vals\"]\ntraj_lengths_td0 = load_td0[\"traj_lengths_training\"]\ntraj_lengths_tdlambda = load_tdlambda[\"traj_lengths_training\"]\ntraj_count_td0 = load_td0[\"traj_count\"]\ntraj_count_tdlambda = load_tdlambda[\"traj_count\"]\n\nplt.figure(figsize=(15, 15))\nplt.subplot(3, 2, 1)\nplt.plot(frames[-len(evals_td0) :], evals_td0, label=\"return (td0)\", alpha=0.5)\nplt.plot(\n    frames[-len(evals_tdlambda) :],\n    evals_tdlambda,\n    label=\"return (td(lambda))\",\n    alpha=0.5,\n)\nplt.plot(frames[-len(mavgs_td0) :], mavgs_td0, label=\"mavg (td0)\")\nplt.plot(frames[-len(mavgs_tdlambda) :], mavgs_tdlambda, label=\"mavg (td(lambda))\")\nplt.xlabel(\"frames collected\")\nplt.ylabel(\"trajectory length (= return)\")\nplt.subplot(3, 2, 2)\nplt.plot(traj_count_td0[-len(evals_td0) :], evals_td0, label=\"return (td0)\", alpha=0.5)\nplt.plot(\n    traj_count_tdlambda[-len(evals_tdlambda) :],\n    evals_tdlambda,\n    label=\"return (td(lambda))\",\n    alpha=0.5,\n)\nplt.plot(traj_count_td0[-len(mavgs_td0) :], mavgs_td0, label=\"mavg (td0)\")\nplt.plot(\n    traj_count_tdlambda[-len(mavgs_tdlambda) :],\n    mavgs_tdlambda,\n    label=\"mavg (td(lambda))\",\n)\nplt.xlabel(\"trajectories collected\")\nplt.legend()\nplt.subplot(3, 2, 3)\nplt.plot(frames[-len(losses_td0) :], losses_td0, label=\"loss (td0)\")\nplt.plot(frames[-len(losses_tdlambda) :], losses_tdlambda, label=\"loss (td(lambda))\")\nplt.xlabel(\"frames collected\")\nplt.title(\"loss\")\nplt.legend()\nplt.subplot(3, 2, 4)\nplt.plot(frames[-len(values_td0) :], values_td0, label=\"values (td0)\")\nplt.plot(frames[-len(values_tdlambda) :], values_tdlambda, label=\"values (td(lambda))\")\nplt.xlabel(\"frames collected\")\nplt.title(\"value\")\nplt.legend()\nplt.subplot(3, 2, 5)\nplt.plot(frames[-len(grad_vals_td0) :], grad_vals_td0, label=\"gradient norm (td0)\")\nplt.plot(\n    frames[-len(grad_vals_tdlambda) :],\n    grad_vals_tdlambda,\n    label=\"gradient norm (td(lambda))\",\n)\nplt.xlabel(\"frames collected\")\nplt.title(\"grad norm\")\nplt.legend()\nif len(traj_lengths):\n    plt.subplot(3, 2, 6)\n    plt.plot(traj_lengths_td0, label=\"episode length (td0)\")\n    plt.plot(traj_lengths_tdlambda, label=\"episode length (td(lambda))\")\n    plt.xlabel(\"batches\")\n    plt.legend()\n    plt.title(\"episode length (training)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we generate a new video to check what the algorithm has learnt.\nIf all goes well, the duration should be significantly longer than with the\ninitial, random rollout.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dummy_env.transform.insert(0, CatTensors([\"pixels\"], \"pixels_save\", del_keys=False))\neval_rollout = dummy_env.rollout(max_steps=10000, policy=actor, auto_reset=True).cpu()\neval_rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# imageio.mimwrite('cartpole.mp4', eval_rollout[\"pixels_save\"].numpy(), fps=30);\n# Video('cartpole.mp4', width=480, height=360) #the width and height option as additional thing new in Ipython 7.6.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion and possible improvements\nWe have seen that using TD(lambda) greatly improved the performance of our\nalgorithm. Other possible improvements could include:\n\n- Using the Multi-Step post-processing. Multi-step will project an action\n  to the nth following step, and create a discounted sum of the rewards in\n  between. This trick can make the algorithm noticebly less myopic. To use\n  this, simply create the collector with\n\n      from torchrl.data.postprocs.postprocs import MultiStep\n      collector = CollectorClass(..., postproc=MultiStep(gamma, n))\n\n  where ``n`` is the number of looking-forward steps. Pay attention to the\n  fact that the ``gamma`` factor has to be corrected by the number of\n  steps till the next observation when being passed to\n  ``vec_td_lambda_advantage_estimate``:\n\n      gamma = gamma ** tensordict[\"steps_to_next_obs\"]\n- A prioritized replay buffer could also be used. This will give a\n  higher priority to samples that have the worst value accuracy.\n- A distributional loss (see ``torchrl.objectives.DistributionalDQNLoss``\n  for more information).\n- More fancy exploration techniques, such as NoisyLinear layers and such\n  (check ``torchrl.modules.NoisyLinear``, which is fully compatible with the\n  ``MLP`` class used in our Dueling DQN).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}