{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Batched data loading with tensorclasses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial we demonstrate how tensorclasses and memory-mapped\ntensors can be used together to efficiently and transparently load data\nfrom disk inside a model training pipeline.\n\nThe basic idea is that we pre-load the entire dataset into a\nmemory-mapped tensors, applying any non-random transformations before\nsaving to disk. This means that not only do we avoid performing repeated\ncomputation each time we iterate through the data, we also are able to\nefficiently load data from the memory-mapped tensor in batches, rather\nthan sequentially from the raw image files.\n\nWe\u2019ll use the same subset of imagenet used in [this transfer learning\ntutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)_.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Download the data from\n  [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip)_\n  and extract it. We assume in this tutorial that the extracted data is\n  saved in the subdirectory ``data/``.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport torch\nfrom kornia import augmentation\nfrom tensordict import MemmapTensor\nfrom tensordict.prototype import tensorclass\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First define train and val transforms that will be applied to train and\nval examples respectively. Note that there are random components in the\ntrain transform to prevent overfitting to training data over multiple\nepochs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose(\n    [\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ]\n)\n\nval_transform = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use ``torchvision.datasets.ImageFolder`` to conveniently load and\ntransform the data from disk.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dir = Path(\"data/hymenoptera_data/\")\n\n\ntrain_data = datasets.ImageFolder(root=data_dir / \"train\", transform=train_transform)\nval_data = datasets.ImageFolder(root=data_dir / \"val\", transform=val_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We\u2019ll also create a dataset of the raw training data that simply resizes\nthe image to a common size and converts to tensor. We\u2019ll use this to\nload the data into memory-mapped tensors. The random transformations\nneed to be different each time we iterate through the data, so they\ncannot be pre-computed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_data_raw = datasets.ImageFolder(\n    root=data_dir / \"train\",\n    transform=transforms.Compose(\n        [transforms.Resize((256, 256)), transforms.ToTensor()]\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use augmentations from the\n[Kornia](https://github.com/kornia/kornia)_ library to apply the\nrandom transformations to batched tensors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_train_transform = augmentation.AugmentationSequential(\n    augmentation.RandomResizedCrop((224, 224)),\n    augmentation.RandomHorizontalFlip(),\n    augmentation.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensorclasses are a good choice when the structure of your data is known\napriori. They are dataclasses that expose dedicated tensor methods over\ntheir contents much like a ``TensorDict``.\n\nAs well as specifying the contents (in this case ``images`` and\n``targets``) we can also encapsulate related logic as custom methods\nwhen defining the class. Here we add a classmethod that takes a dataset\nand creates a tensorclass containing the data by iterating over the\ndataset. We create memory-mapped tensors to hold the data so that they\ncan be efficiently loaded in batches later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@tensorclass\nclass ImageNetData:\n    images: torch.Tensor\n    targets: torch.Tensor\n\n    @classmethod\n    def from_dataset(cls, dataset, device=None):\n        data = cls(\n            images=MemmapTensor(\n                len(dataset), *dataset[0][0].squeeze().shape, dtype=torch.float32\n            ),\n            targets=MemmapTensor(len(dataset), dtype=torch.int64),\n            batch_size=[len(dataset)],\n            device=device,\n        )\n        for i, (image, target) in enumerate(dataset):\n            data[i] = cls(images=image, targets=torch.tensor(target), batch_size=[])\n        return data\n\n    def __len__(self):\n        return self.batch_size[0] if self.batch_dims else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create two tensorclasses, one for the training and on for the\nvalidation data. Note that while this step can be slightly expensive, it\nallows us to save repeated computation later during training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_data_tc = ImageNetData.from_dataset(train_data_raw, device=device)\nval_data_tc = ImageNetData.from_dataset(val_data, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoaders\n\nWe can create dataloaders both from the ``torchvision``-provided\nDatasets, as well as from our memory-mapped tensorclasses.\n\nSince tensorclasses implement ``__len__`` and ``__getitem__`` (and also\n``__getitems__``) we can use them like a map-style Dataset and create a\n``DataLoader`` directly from them.\n\nNote that because the tensorclass can handle batched indices, there is\nno need for additional collation, so we pass the identity function as\nthe ``collate_fn``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 64\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\nval_dataloader = DataLoader(val_data, batch_size=batch_size)\n\ntrain_dataloader_tc = DataLoader(\n    train_data_tc, batch_size=batch_size, collate_fn=lambda x: x\n)\nval_dataloader_tc = DataLoader(\n    val_data_tc, batch_size=batch_size, collate_fn=lambda x: x\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now compare how long it takes to iterate once over the data in\neach case. The regular dataloader loads images one by one from disk,\napplies the transform sequentially and then stacks the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\nt0 = time.time()\nfor image, target in train_dataloader:\n    image, target = image.to(device), target.to(device)\nprint(f\"One iteration over dataloader done! Time: {time.time() - t0:4.4f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our tensorclass-based dataloader instead loads data from the\nmemory-mapped tensor in batches. We then apply the batched random\ntransformations to the batched images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\nfor batch in train_dataloader_tc:\n    image, target = (\n        batch_train_transform(batch.images.contiguous()),\n        batch.targets.contiguous(),\n    )\nprint(f\"One iteration over tensorclass dataloader done! Time: {time.time() - t0:4.4f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the case of the validation set, we see an even bigger performance\nimprovement, because there are no random transformations, so we can save\nthe fully transformed data in the memory-mapped tensor, eliminating the\nneed for additional transformations as we load from disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\nfor image, target in val_dataloader:\n    image, target = image.to(device), target.to(device)\nprint(f\"One iteration over val data. Time: {time.time() - t0:4.4f}s\")\n\nt0 = time.time()\nfor batch in val_dataloader_tc:\n    image, target = batch.images.contiguous(), batch.targets.contiguous()\nprint(f\"One iteration over tensorclass val data. Time: {time.time() - t0:4.4f}s\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}