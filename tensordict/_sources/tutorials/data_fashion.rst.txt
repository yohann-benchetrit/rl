
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/data_fashion.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_data_fashion.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_data_fashion.py:


Using TensorDict for datasets
=============================

.. GENERATED FROM PYTHON SOURCE LINES 9-14

In this tutorial we demonstrate how ``TensorDict`` can be used to
efficiently and transparently load and manage data inside a training
pipeline. The tutorial is based heavily on the `PyTorch Quickstart
Tutorial <https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html>`__,
but modified to demonstrate use of ``TensorDict``.

.. GENERATED FROM PYTHON SOURCE LINES 14-27

.. code-block:: default



    import torch
    import torch.nn as nn
    from tensordict import MemmapTensor, TensorDict
    from torch.utils.data import DataLoader
    from torchvision import datasets
    from torchvision.transforms import ToTensor

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Using device: cpu




.. GENERATED FROM PYTHON SOURCE LINES 28-32

The ``torchvision.datasets`` module contains a number of convenient pre-prepared
datasets. In this tutorial we'll use the relatively simple FashionMNIST dataset. Each
image is an item of clothing, the objective is to classify the type of clothing in
the image (e.g. "Bag", "Sneaker" etc.).

.. GENERATED FROM PYTHON SOURCE LINES 32-46

.. code-block:: default


    training_data = datasets.FashionMNIST(
        root="data",
        train=True,
        download=True,
        transform=ToTensor(),
    )
    test_data = datasets.FashionMNIST(
        root="data",
        train=False,
        download=True,
        transform=ToTensor(),
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz
    0.1%    0.2%    0.4%    0.5%    0.6%    0.7%    0.9%    1.0%    1.1%    1.2%    1.4%    1.5%    1.6%    1.7%    1.9%    2.0%    2.1%    2.2%    2.4%    2.5%    2.6%    2.7%    2.9%    3.0%    3.1%    3.2%    3.3%    3.5%    3.6%    3.7%    3.8%    4.0%    4.1%    4.2%    4.3%    4.5%    4.6%    4.7%    4.8%    5.0%    5.1%    5.2%    5.3%    5.5%    5.6%    5.7%    5.8%    6.0%    6.1%    6.2%    6.3%    6.4%    6.6%    6.7%    6.8%    6.9%    7.1%    7.2%    7.3%    7.4%    7.6%    7.7%    7.8%    7.9%    8.1%    8.2%    8.3%    8.4%    8.6%    8.7%    8.8%    8.9%    9.1%    9.2%    9.3%    9.4%    9.5%    9.7%    9.8%    9.9%    10.0%    10.2%    10.3%    10.4%    10.5%    10.7%    10.8%    10.9%    11.0%    11.2%    11.3%    11.4%    11.5%    11.7%    11.8%    11.9%    12.0%    12.2%    12.3%    12.4%    12.5%    12.6%    12.8%    12.9%    13.0%    13.1%    13.3%    13.4%    13.5%    13.6%    13.8%    13.9%    14.0%    14.1%    14.3%    14.4%    14.5%    14.6%    14.8%    14.9%    15.0%    15.1%    15.3%    15.4%    15.5%    15.6%    15.8%    15.9%    16.0%    16.1%    16.2%    16.4%    16.5%    16.6%    16.7%    16.9%    17.0%    17.1%    17.2%    17.4%    17.5%    17.6%    17.7%    17.9%    18.0%    18.1%    18.2%    18.4%    18.5%    18.6%    18.7%    18.9%    19.0%    19.1%    19.2%    19.3%    19.5%    19.6%    19.7%    19.8%    20.0%    20.1%    20.2%    20.3%    20.5%    20.6%    20.7%    20.8%    21.0%    21.1%    21.2%    21.3%    21.5%    21.6%    21.7%    21.8%    22.0%    22.1%    22.2%    22.3%    22.4%    22.6%    22.7%    22.8%    22.9%    23.1%    23.2%    23.3%    23.4%    23.6%    23.7%    23.8%    23.9%    24.1%    24.2%    24.3%    24.4%    24.6%    24.7%    24.8%    24.9%    25.1%    25.2%    25.3%    25.4%    25.5%    25.7%    25.8%    25.9%    26.0%    26.2%    26.3%    26.4%    26.5%    26.7%    26.8%    26.9%    27.0%    27.2%    27.3%    27.4%    27.5%    27.7%    27.8%    27.9%    28.0%    28.2%    28.3%    28.4%    28.5%    28.6%    28.8%    28.9%    29.0%    29.1%    29.3%    29.4%    29.5%    29.6%    29.8%    29.9%    30.0%    30.1%    30.3%    30.4%    30.5%    30.6%    30.8%    30.9%    31.0%    31.1%    31.3%    31.4%    31.5%    31.6%    31.7%    31.9%    32.0%    32.1%    32.2%    32.4%    32.5%    32.6%    32.7%    32.9%    33.0%    33.1%    33.2%    33.4%    33.5%    33.6%    33.7%    33.9%    34.0%    34.1%    34.2%    34.4%    34.5%    34.6%    34.7%    34.8%    35.0%    35.1%    35.2%    35.3%    35.5%    35.6%    35.7%    35.8%    36.0%    36.1%    36.2%    36.3%    36.5%    36.6%    36.7%    36.8%    37.0%    37.1%    37.2%    37.3%    37.5%    37.6%    37.7%    37.8%    37.9%    38.1%    38.2%    38.3%    38.4%    38.6%    38.7%    38.8%    38.9%    39.1%    39.2%    39.3%    39.4%    39.6%    39.7%    39.8%    39.9%    40.1%    40.2%    40.3%    40.4%    40.6%    40.7%    40.8%    40.9%    41.1%    41.2%    41.3%    41.4%    41.5%    41.7%    41.8%    41.9%    42.0%    42.2%    42.3%    42.4%    42.5%    42.7%    42.8%    42.9%    43.0%    43.2%    43.3%    43.4%    43.5%    43.7%    43.8%    43.9%    44.0%    44.2%    44.3%    44.4%    44.5%    44.6%    44.8%    44.9%    45.0%    45.1%    45.3%    45.4%    45.5%    45.6%    45.8%    45.9%    46.0%    46.1%    46.3%    46.4%    46.5%    46.6%    46.8%    46.9%    47.0%    47.1%    47.3%    47.4%    47.5%    47.6%    47.7%    47.9%    48.0%    48.1%    48.2%    48.4%    48.5%    48.6%    48.7%    48.9%    49.0%    49.1%    49.2%    49.4%    49.5%    49.6%    49.7%    49.9%    50.0%    50.1%    50.2%    50.4%    50.5%    50.6%    50.7%    50.8%    51.0%    51.1%    51.2%    51.3%    51.5%    51.6%    51.7%    51.8%    52.0%    52.1%    52.2%    52.3%    52.5%    52.6%    52.7%    52.8%    53.0%    53.1%    53.2%    53.3%    53.5%    53.6%    53.7%    53.8%    53.9%    54.1%    54.2%    54.3%    54.4%    54.6%    54.7%    54.8%    54.9%    55.1%    55.2%    55.3%    55.4%    55.6%    55.7%    55.8%    55.9%    56.1%    56.2%    56.3%    56.4%    56.6%    56.7%    56.8%    56.9%    57.0%    57.2%    57.3%    57.4%    57.5%    57.7%    57.8%    57.9%    58.0%    58.2%    58.3%    58.4%    58.5%    58.7%    58.8%    58.9%    59.0%    59.2%    59.3%    59.4%    59.5%    59.7%    59.8%    59.9%    60.0%    60.1%    60.3%    60.4%    60.5%    60.6%    60.8%    60.9%    61.0%    61.1%    61.3%    61.4%    61.5%    61.6%    61.8%    61.9%    62.0%    62.1%    62.3%    62.4%    62.5%    62.6%    62.8%    62.9%    63.0%    63.1%    63.2%    63.4%    63.5%    63.6%    63.7%    63.9%    64.0%    64.1%    64.2%    64.4%    64.5%    64.6%    64.7%    64.9%    65.0%    65.1%    65.2%    65.4%    65.5%    65.6%    65.7%    65.9%    66.0%    66.1%    66.2%    66.3%    66.5%    66.6%    66.7%    66.8%    67.0%    67.1%    67.2%    67.3%    67.5%    67.6%    67.7%    67.8%    68.0%    68.1%    68.2%    68.3%    68.5%    68.6%    68.7%    68.8%    69.0%    69.1%    69.2%    69.3%    69.5%    69.6%    69.7%    69.8%    69.9%    70.1%    70.2%    70.3%    70.4%    70.6%    70.7%    70.8%    70.9%    71.1%    71.2%    71.3%    71.4%    71.6%    71.7%    71.8%    71.9%    72.1%    72.2%    72.3%    72.4%    72.6%    72.7%    72.8%    72.9%    73.0%    73.2%    73.3%    73.4%    73.5%    73.7%    73.8%    73.9%    74.0%    74.2%    74.3%    74.4%    74.5%    74.7%    74.8%    74.9%    75.0%    75.2%    75.3%    75.4%    75.5%    75.7%    75.8%    75.9%    76.0%    76.1%    76.3%    76.4%    76.5%    76.6%    76.8%    76.9%    77.0%    77.1%    77.3%    77.4%    77.5%    77.6%    77.8%    77.9%    78.0%    78.1%    78.3%    78.4%    78.5%    78.6%    78.8%    78.9%    79.0%    79.1%    79.2%    79.4%    79.5%    79.6%    79.7%    79.9%    80.0%    80.1%    80.2%    80.4%    80.5%    80.6%    80.7%    80.9%    81.0%    81.1%    81.2%    81.4%    81.5%    81.6%    81.7%    81.9%    82.0%    82.1%    82.2%    82.3%    82.5%    82.6%    82.7%    82.8%    83.0%    83.1%    83.2%    83.3%    83.5%    83.6%    83.7%    83.8%    84.0%    84.1%    84.2%    84.3%    84.5%    84.6%    84.7%    84.8%    85.0%    85.1%    85.2%    85.3%    85.4%    85.6%    85.7%    85.8%    85.9%    86.1%    86.2%    86.3%    86.4%    86.6%    86.7%    86.8%    86.9%    87.1%    87.2%    87.3%    87.4%    87.6%    87.7%    87.8%    87.9%    88.1%    88.2%    88.3%    88.4%    88.5%    88.7%    88.8%    88.9%    89.0%    89.2%    89.3%    89.4%    89.5%    89.7%    89.8%    89.9%    90.0%    90.2%    90.3%    90.4%    90.5%    90.7%    90.8%    90.9%    91.0%    91.2%    91.3%    91.4%    91.5%    91.6%    91.8%    91.9%    92.0%    92.1%    92.3%    92.4%    92.5%    92.6%    92.8%    92.9%    93.0%    93.1%    93.3%    93.4%    93.5%    93.6%    93.8%    93.9%    94.0%    94.1%    94.3%    94.4%    94.5%    94.6%    94.8%    94.9%    95.0%    95.1%    95.2%    95.4%    95.5%    95.6%    95.7%    95.9%    96.0%    96.1%    96.2%    96.4%    96.5%    96.6%    96.7%    96.9%    97.0%    97.1%    97.2%    97.4%    97.5%    97.6%    97.7%    97.9%    98.0%    98.1%    98.2%    98.3%    98.5%    98.6%    98.7%    98.8%    99.0%    99.1%    99.2%    99.3%    99.5%    99.6%    99.7%    99.8%    100.0%    100.0%
    Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz
    100.0%
    Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz
    0.7%    1.5%    2.2%    3.0%    3.7%    4.4%    5.2%    5.9%    6.7%    7.4%    8.2%    8.9%    9.6%    10.4%    11.1%    11.9%    12.6%    13.3%    14.1%    14.8%    15.6%    16.3%    17.0%    17.8%    18.5%    19.3%    20.0%    20.7%    21.5%    22.2%    23.0%    23.7%    24.5%    25.2%    25.9%    26.7%    27.4%    28.2%    28.9%    29.6%    30.4%    31.1%    31.9%    32.6%    33.3%    34.1%    34.8%    35.6%    36.3%    37.1%    37.8%    38.5%    39.3%    40.0%    40.8%    41.5%    42.2%    43.0%    43.7%    44.5%    45.2%    45.9%    46.7%    47.4%    48.2%    48.9%    49.6%    50.4%    51.1%    51.9%    52.6%    53.4%    54.1%    54.8%    55.6%    56.3%    57.1%    57.8%    58.5%    59.3%    60.0%    60.8%    61.5%    62.2%    63.0%    63.7%    64.5%    65.2%    65.9%    66.7%    67.4%    68.2%    68.9%    69.7%    70.4%    71.1%    71.9%    72.6%    73.4%    74.1%    74.8%    75.6%    76.3%    77.1%    77.8%    78.5%    79.3%    80.0%    80.8%    81.5%    82.3%    83.0%    83.7%    84.5%    85.2%    86.0%    86.7%    87.4%    88.2%    88.9%    89.7%    90.4%    91.1%    91.9%    92.6%    93.4%    94.1%    94.8%    95.6%    96.3%    97.1%    97.8%    98.6%    99.3%    100.0%
    Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
    100.0%
    Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw





.. GENERATED FROM PYTHON SOURCE LINES 47-53

We will create two tensordicts, one each for the training and test data. We create
memory-mapped tensors to hold the data. This will allow us to efficiently load
batches of transformed data from disk rather than repeatedly load and transform
individual images.

First we create the ``MemmapTensor`` containers.

.. GENERATED FROM PYTHON SOURCE LINES 53-78

.. code-block:: default



    training_data_td = TensorDict(
        {
            "images": MemmapTensor(
                len(training_data),
                *training_data[0][0].squeeze().shape,
                dtype=torch.float32,
            ),
            "targets": MemmapTensor(len(training_data), dtype=torch.int64),
        },
        batch_size=[len(training_data)],
        device=device,
    )
    test_data_td = TensorDict(
        {
            "images": MemmapTensor(
                len(test_data), *test_data[0][0].squeeze().shape, dtype=torch.float32
            ),
            "targets": MemmapTensor(len(test_data), dtype=torch.int64),
        },
        batch_size=[len(test_data)],
        device=device,
    )








.. GENERATED FROM PYTHON SOURCE LINES 79-82

Then we can iterate over the data to populate the memory-mapped tensors. This takes a
bit of time, but performing the transforms up-front will save repeated effort during
training later.

.. GENERATED FROM PYTHON SOURCE LINES 82-89

.. code-block:: default


    for i, (img, label) in enumerate(training_data):
        training_data_td[i] = TensorDict({"images": img, "targets": label}, [])

    for i, (img, label) in enumerate(test_data):
        test_data_td[i] = TensorDict({"images": img, "targets": label}, [])








.. GENERATED FROM PYTHON SOURCE LINES 90-100

DataLoaders
----------------

We'll create DataLoaders from the ``torchvision``-provided Datasets, as well as from
our memory-mapped TensorDicts.

Since ``TensorDict`` implements ``__len__`` and ``__getitem__`` (and also
``__getitems__``) we can use it like a map-style Dataset and create a ``DataLoader``
directly from it. Note that because ``TensorDict`` can already handle batched indices,
there is no need for collation, so we pass the identity function as ``collate_fn``.

.. GENERATED FROM PYTHON SOURCE LINES 100-113

.. code-block:: default


    batch_size = 64

    train_dataloader = DataLoader(training_data, batch_size=batch_size)
    test_dataloader = DataLoader(test_data, batch_size=batch_size)

    train_dataloader_td = DataLoader(
        training_data_td, batch_size=batch_size, collate_fn=lambda x: x
    )
    test_dataloader_td = DataLoader(
        test_data_td, batch_size=batch_size, collate_fn=lambda x: x
    )








.. GENERATED FROM PYTHON SOURCE LINES 114-120

Model
-------

We use the same model from the
`Quickstart Tutorial <https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html>`__.


.. GENERATED FROM PYTHON SOURCE LINES 120-144

.. code-block:: default



    class Net(nn.Module):
        def __init__(self):
            super().__init__()
            self.flatten = nn.Flatten()
            self.linear_relu_stack = nn.Sequential(
                nn.Linear(28 * 28, 512),
                nn.ReLU(),
                nn.Linear(512, 512),
                nn.ReLU(),
                nn.Linear(512, 10),
            )

        def forward(self, x):
            x = self.flatten(x)
            logits = self.linear_relu_stack(x)
            return logits


    model = Net().to(device)
    model_td = Net().to(device)
    model, model_td





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    (Net(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (linear_relu_stack): Sequential(
        (0): Linear(in_features=784, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=10, bias=True)
      )
    ), Net(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (linear_relu_stack): Sequential(
        (0): Linear(in_features=784, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=10, bias=True)
      )
    ))



.. GENERATED FROM PYTHON SOURCE LINES 145-151

Optimizing the parameters
---------------------------------

We'll optimise the parameters of the model using stochastic gradient descent and
cross-entropy loss.


.. GENERATED FROM PYTHON SOURCE LINES 151-176

.. code-block:: default


    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
    optimizer_td = torch.optim.SGD(model_td.parameters(), lr=1e-3)


    def train(dataloader, model, loss_fn, optimizer):
        size = len(dataloader.dataset)
        model.train()

        for batch, (X, y) in enumerate(dataloader):
            X, y = X.to(device), y.to(device)

            pred = model(X)
            loss = loss_fn(pred, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if batch % 100 == 0:
                loss, current = loss.item(), batch * len(X)
                print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]")









.. GENERATED FROM PYTHON SOURCE LINES 177-180

The training loop for our ``TensorDict``-based DataLoader is very similar, we just
adjust how we unpack the data to the more explicit key-based retrieval offered by
``TensorDict``. The ``.contiguous()`` method loads the data stored in the memmap tensor.

.. GENERATED FROM PYTHON SOURCE LINES 180-266

.. code-block:: default



    def train_td(dataloader, model, loss_fn, optimizer):
        size = len(dataloader.dataset)
        model.train()

        for batch, data in enumerate(dataloader):
            X, y = data["images"].contiguous(), data["targets"].contiguous()

            pred = model(X)
            loss = loss_fn(pred, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if batch % 100 == 0:
                loss, current = loss.item(), batch * len(X)
                print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]")


    def test(dataloader, model, loss_fn):
        size = len(dataloader.dataset)
        num_batches = len(dataloader)
        model.eval()
        test_loss, correct = 0, 0
        with torch.no_grad():
            for X, y in dataloader:
                X, y = X.to(device), y.to(device)

                pred = model(X)

                test_loss += loss_fn(pred, y).item()
                correct += (pred.argmax(1) == y).type(torch.float).sum().item()

        test_loss /= num_batches
        correct /= size

        print(
            f"Test Error: \n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \n"
        )


    def test_td(dataloader, model, loss_fn):
        size = len(dataloader.dataset)
        num_batches = len(dataloader)
        model.eval()
        test_loss, correct = 0, 0
        with torch.no_grad():
            for batch in dataloader:
                X, y = batch["images"].contiguous(), batch["targets"].contiguous()

                pred = model(X)

                test_loss += loss_fn(pred, y).item()
                correct += (pred.argmax(1) == y).type(torch.float).sum().item()

        test_loss /= num_batches
        correct /= size

        print(
            f"Test Error: \n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \n"
        )


    for d in train_dataloader_td:
        print(d)
        break

    import time

    t0 = time.time()
    epochs = 5
    for t in range(epochs):
        print(f"Epoch {t + 1}\n-------------------------")
        train_td(train_dataloader_td, model_td, loss_fn, optimizer_td)
        test_td(test_dataloader_td, model_td, loss_fn)
    print(f"TensorDict training done! time: {time.time() - t0: 4.4f} s")

    t0 = time.time()
    epochs = 5
    for t in range(epochs):
        print(f"Epoch {t + 1}\n-------------------------")
        train(train_dataloader, model, loss_fn, optimizer)
        test(test_dataloader, model, loss_fn)
    print(f"Training done! time: {time.time() - t0: 4.4f} s")




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            images: MemmapTensor(shape=torch.Size([64, 28, 28]), device=cpu, dtype=torch.float32, is_shared=False),
            targets: MemmapTensor(shape=torch.Size([64]), device=cpu, dtype=torch.int64, is_shared=False)},
        batch_size=torch.Size([64]),
        device=cpu,
        is_shared=False)
    Epoch 1
    -------------------------
    loss: 2.297705 [    0/60000]
    loss: 2.291298 [ 6400/60000]
    loss: 2.266612 [12800/60000]
    loss: 2.256810 [19200/60000]
    loss: 2.262443 [25600/60000]
    loss: 2.208214 [32000/60000]
    loss: 2.232882 [38400/60000]
    loss: 2.195832 [44800/60000]
    loss: 2.190250 [51200/60000]
    loss: 2.158256 [57600/60000]
    Test Error: 
     Accuracy: 34.4%, Avg loss: 2.154627 

    Epoch 2
    -------------------------
    loss: 2.157342 [    0/60000]
    loss: 2.154477 [ 6400/60000]
    loss: 2.097176 [12800/60000]
    loss: 2.118717 [19200/60000]
    loss: 2.076585 [25600/60000]
    loss: 1.995821 [32000/60000]
    loss: 2.041064 [38400/60000]
    loss: 1.960695 [44800/60000]
    loss: 1.963487 [51200/60000]
    loss: 1.895609 [57600/60000]
    Test Error: 
     Accuracy: 55.3%, Avg loss: 1.893494 

    Epoch 3
    -------------------------
    loss: 1.910257 [    0/60000]
    loss: 1.888803 [ 6400/60000]
    loss: 1.775547 [12800/60000]
    loss: 1.828395 [19200/60000]
    loss: 1.720869 [25600/60000]
    loss: 1.650842 [32000/60000]
    loss: 1.687431 [38400/60000]
    loss: 1.584762 [44800/60000]
    loss: 1.607041 [51200/60000]
    loss: 1.503690 [57600/60000]
    Test Error: 
     Accuracy: 63.8%, Avg loss: 1.519463 

    Epoch 4
    -------------------------
    loss: 1.572754 [    0/60000]
    loss: 1.541595 [ 6400/60000]
    loss: 1.391343 [12800/60000]
    loss: 1.475045 [19200/60000]
    loss: 1.362031 [25600/60000]
    loss: 1.334748 [32000/60000]
    loss: 1.361515 [38400/60000]
    loss: 1.282478 [44800/60000]
    loss: 1.315961 [51200/60000]
    loss: 1.219422 [57600/60000]
    Test Error: 
     Accuracy: 65.0%, Avg loss: 1.243010 

    Epoch 5
    -------------------------
    loss: 1.313287 [    0/60000]
    loss: 1.295996 [ 6400/60000]
    loss: 1.128731 [12800/60000]
    loss: 1.242475 [19200/60000]
    loss: 1.131389 [25600/60000]
    loss: 1.131659 [32000/60000]
    loss: 1.164709 [38400/60000]
    loss: 1.099253 [44800/60000]
    loss: 1.135807 [51200/60000]
    loss: 1.057812 [57600/60000]
    Test Error: 
     Accuracy: 65.6%, Avg loss: 1.075205 

    TensorDict training done! time:  12.9581 s
    Epoch 1
    -------------------------
    loss: 2.299827 [    0/60000]
    loss: 2.289411 [ 6400/60000]
    loss: 2.266534 [12800/60000]
    loss: 2.272936 [19200/60000]
    loss: 2.235183 [25600/60000]
    loss: 2.216762 [32000/60000]
    loss: 2.242464 [38400/60000]
    loss: 2.192392 [44800/60000]
    loss: 2.196963 [51200/60000]
    loss: 2.165815 [57600/60000]
    Test Error: 
     Accuracy: 26.6%, Avg loss: 2.157415 

    Epoch 2
    -------------------------
    loss: 2.165035 [    0/60000]
    loss: 2.160033 [ 6400/60000]
    loss: 2.096011 [12800/60000]
    loss: 2.129886 [19200/60000]
    loss: 2.052278 [25600/60000]
    loss: 1.999595 [32000/60000]
    loss: 2.057226 [38400/60000]
    loss: 1.951510 [44800/60000]
    loss: 1.983561 [51200/60000]
    loss: 1.909088 [57600/60000]
    Test Error: 
     Accuracy: 47.8%, Avg loss: 1.902260 

    Epoch 3
    -------------------------
    loss: 1.932409 [    0/60000]
    loss: 1.906975 [ 6400/60000]
    loss: 1.782688 [12800/60000]
    loss: 1.845799 [19200/60000]
    loss: 1.706896 [25600/60000]
    loss: 1.663180 [32000/60000]
    loss: 1.713725 [38400/60000]
    loss: 1.581007 [44800/60000]
    loss: 1.635852 [51200/60000]
    loss: 1.530957 [57600/60000]
    Test Error: 
     Accuracy: 59.2%, Avg loss: 1.542289 

    Epoch 4
    -------------------------
    loss: 1.604445 [    0/60000]
    loss: 1.572633 [ 6400/60000]
    loss: 1.413604 [12800/60000]
    loss: 1.496351 [19200/60000]
    loss: 1.361649 [25600/60000]
    loss: 1.362530 [32000/60000]
    loss: 1.383049 [38400/60000]
    loss: 1.280936 [44800/60000]
    loss: 1.339802 [51200/60000]
    loss: 1.237336 [57600/60000]
    Test Error: 
     Accuracy: 63.2%, Avg loss: 1.264853 

    Epoch 5
    -------------------------
    loss: 1.338911 [    0/60000]
    loss: 1.325101 [ 6400/60000]
    loss: 1.148927 [12800/60000]
    loss: 1.258517 [19200/60000]
    loss: 1.131014 [25600/60000]
    loss: 1.161820 [32000/60000]
    loss: 1.177489 [38400/60000]
    loss: 1.092707 [44800/60000]
    loss: 1.156803 [51200/60000]
    loss: 1.069178 [57600/60000]
    Test Error: 
     Accuracy: 64.5%, Avg loss: 1.092868 

    Training done! time:  36.9972 s





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  5.694 seconds)


.. _sphx_glr_download_tutorials_data_fashion.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: data_fashion.py <data_fashion.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: data_fashion.ipynb <data_fashion.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
